[2022-07-08 16:27:17,043][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-08 16:27:17,044][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-08 16:27:17,199][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 00:47:28,201][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:47:28,201][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 00:48:50,741][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:48:50,742][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 00:50:52,902][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 00:50:52,913][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 10:20:34,133][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 10:20:34,143][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:22:15,438][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:22:15,440][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:23:23,378][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:23:23,379][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:23:23,520][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:26:09,031][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:26:09,033][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:26:09,154][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:35:18,283][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:35:18,295][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:35:18,506][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:35:35,115][train][INFO] - Instantiating model <models.fast_detr>
[2022-07-11 11:37:25,532][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:37:25,533][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:37:25,604][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:37:30,897][train][INFO] - Instantiating model <models.FastDETR>
[2022-07-11 11:43:07,256][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:43:07,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:43:07,327][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:43:11,154][train][INFO] - Instantiating model <models.FastDETR>
[2022-07-11 11:44:01,861][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:44:01,861][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:44:01,933][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:44:03,826][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:53:14,574][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:53:14,574][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:53:14,644][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:53:16,424][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:54:05,217][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:54:05,217][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:54:05,288][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:54:06,976][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:54:08,329][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:54:08,337][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:54:08,338][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:09,932][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:57:09,932][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:57:10,002][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:57:12,409][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:57:14,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:57:14,133][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:57:14,134][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:56,559][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:57:56,560][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:57:56,628][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:57:58,317][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:57:59,681][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:57:59,683][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:57:59,684][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:57:59,685][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 11:57:59,689][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: False
[2022-07-11 11:57:59,689][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 11:58:27,853][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 11:58:27,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 11:58:27,922][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 11:58:29,606][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 11:58:30,974][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 11:58:30,976][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 11:58:30,977][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 11:58:30,978][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 11:58:30,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 11:58:30,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 11:58:30,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 11:58:30,984][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 11:58:30,984][train][INFO] - Logging hyperparameters!
[2022-07-11 11:58:30,989][train][INFO] - Starting training!
[2022-07-11 11:58:49,611][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 11:58:51,993][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 11:59:46,195][train][INFO] - Starting testing!
[2022-07-11 11:59:46,688][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 11:59:47,011][train][INFO] - Finalizing!
[2022-07-11 12:00:28,722][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:00:28,722][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:00:28,790][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:00:31,170][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:00:32,723][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:00:32,725][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:00:32,726][train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
[2022-07-11 12:00:32,727][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:00:32,731][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:00:32,731][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:00:32,731][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:00:32,732][train][INFO] - Logging hyperparameters!
[2022-07-11 12:02:02,128][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:02:02,129][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:02:02,197][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:02:03,927][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:02:05,332][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:02:05,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:02:05,335][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:02:05,336][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:02:05,340][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:02:05,340][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:02:05,341][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:02:05,341][train][INFO] - Logging hyperparameters!
[2022-07-11 12:02:05,364][train][INFO] - Starting training!
[2022-07-11 12:02:23,828][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:02:25,420][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:29:40,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:29:40,902][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:29:40,968][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:29:44,602][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:29:46,276][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:29:46,278][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:29:46,279][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:29:46,281][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:29:46,284][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:29:46,285][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:29:46,285][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:29:46,286][train][INFO] - Logging hyperparameters!
[2022-07-11 12:29:46,307][train][INFO] - Starting training!
[2022-07-11 12:30:04,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:30:06,257][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:35:11,949][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:35:11,965][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:35:12,046][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:35:27,517][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:35:29,325][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:35:29,327][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:35:29,328][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:35:29,329][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:35:29,333][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:35:29,333][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:35:29,334][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:35:29,334][train][INFO] - Logging hyperparameters!
[2022-07-11 12:35:29,376][train][INFO] - Starting training!
[2022-07-11 12:35:47,800][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:35:50,165][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 12:57:58,148][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:57:58,148][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:57:58,216][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:00,986][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:58:18,789][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:58:18,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:58:18,967][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:20,501][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:58:37,177][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:58:37,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:58:37,245][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:58:38,789][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:59:17,535][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 12:59:17,535][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 12:59:17,603][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 12:59:19,150][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 12:59:21,266][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 12:59:21,267][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 12:59:21,268][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 12:59:21,270][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 12:59:21,274][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 12:59:21,274][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 12:59:21,274][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 12:59:21,275][train][INFO] - Logging hyperparameters!
[2022-07-11 12:59:21,295][train][INFO] - Starting training!
[2022-07-11 12:59:39,545][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 12:59:41,078][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:01:11,145][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:01:11,145][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:01:11,212][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:01:13,045][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:01:15,146][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:01:15,147][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:01:15,148][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:01:15,150][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:01:15,154][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:01:15,154][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:01:15,154][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:01:15,155][train][INFO] - Logging hyperparameters!
[2022-07-11 13:01:15,175][train][INFO] - Starting training!
[2022-07-11 13:01:33,407][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:01:34,940][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:03:06,999][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:03:06,999][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:03:07,066][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:03:08,724][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:10,367][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:04:10,367][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:04:10,435][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:04:12,013][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:48,987][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:04:48,987][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:04:49,054][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:04:50,577][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:04:52,700][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:04:52,703][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:04:52,703][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:04:52,705][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:04:52,709][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:04:52,709][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:04:52,709][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:04:52,710][train][INFO] - Logging hyperparameters!
[2022-07-11 13:04:52,730][train][INFO] - Starting training!
[2022-07-11 13:05:10,906][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:05:12,432][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:05:33,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:05:33,569][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:05:33,656][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:05:35,355][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:07:03,596][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:07:03,597][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:07:03,662][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:07:05,217][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:14:06,257][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:14:06,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:14:06,324][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:14:07,929][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:15:34,079][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:15:34,079][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:15:34,146][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:15:35,667][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:15:37,782][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:15:37,783][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:15:37,784][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:15:37,786][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:15:37,790][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:15:37,790][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:15:37,790][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:15:37,791][train][INFO] - Logging hyperparameters!
[2022-07-11 13:15:37,811][train][INFO] - Starting training!
[2022-07-11 13:15:55,903][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:15:57,435][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:19:01,580][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:19:01,580][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:19:01,647][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:19:03,235][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:19:05,408][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:19:05,410][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:19:05,410][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:19:05,412][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:19:05,416][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:19:05,416][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:19:05,417][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:19:05,417][train][INFO] - Logging hyperparameters!
[2022-07-11 13:19:05,438][train][INFO] - Starting training!
[2022-07-11 13:19:23,626][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:19:25,161][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:21:43,873][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:21:43,886][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:21:43,953][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:21:45,845][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:21:47,985][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:21:47,987][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:21:47,988][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:21:47,989][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:21:47,993][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:21:47,994][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:21:47,994][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:21:47,994][train][INFO] - Logging hyperparameters!
[2022-07-11 13:21:48,015][train][INFO] - Starting training!
[2022-07-11 13:22:06,227][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:22:07,748][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:23:28,280][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:23:28,280][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:23:28,348][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:23:29,993][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:23:32,343][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:23:32,345][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:23:32,346][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:23:32,347][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:23:32,351][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:23:32,352][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:23:32,352][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:23:32,352][train][INFO] - Logging hyperparameters!
[2022-07-11 13:23:32,373][train][INFO] - Starting training!
[2022-07-11 13:23:50,886][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:23:52,426][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:25:11,911][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:25:11,911][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:25:11,978][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:25:13,499][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:25:15,598][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:25:15,600][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:25:15,601][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:25:15,602][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:25:15,606][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:25:15,606][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:25:15,606][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:25:15,607][train][INFO] - Logging hyperparameters!
[2022-07-11 13:25:15,627][train][INFO] - Starting training!
[2022-07-11 13:25:33,837][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:25:35,346][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 13:33:42,858][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 13:33:42,859][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 13:33:42,926][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 13:33:45,702][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 13:33:47,875][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 13:33:47,877][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 13:33:47,878][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 13:33:47,879][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 13:33:47,883][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 13:33:47,883][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 13:33:47,884][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 13:33:47,884][train][INFO] - Logging hyperparameters!
[2022-07-11 13:33:47,904][train][INFO] - Starting training!
[2022-07-11 13:34:06,060][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 13:34:07,577][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 14:57:17,719][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 14:57:17,727][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 14:57:17,819][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 14:57:33,381][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 14:57:35,937][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 14:57:35,939][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 14:57:35,940][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 14:57:35,941][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 14:57:35,946][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 14:57:35,946][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 14:57:35,947][train][INFO] - Logging hyperparameters!
[2022-07-11 14:57:35,951][train][INFO] - Starting training!
[2022-07-11 14:57:54,489][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 14:57:56,183][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 14:58:54,311][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 14:58:54,311][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 14:58:54,379][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 14:58:56,423][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 14:58:58,475][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 14:58:58,477][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 14:58:58,478][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 14:58:58,479][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 14:58:58,483][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 14:58:58,483][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 14:58:58,484][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 14:58:58,484][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 14:58:58,484][train][INFO] - Logging hyperparameters!
[2022-07-11 14:58:58,489][train][INFO] - Starting training!
[2022-07-11 14:59:17,149][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 14:59:18,702][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:01:02,634][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:01:02,635][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:01:02,702][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:01:06,136][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:01:08,472][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:01:08,474][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:01:08,475][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:01:08,476][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:01:08,480][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:01:08,481][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:01:08,481][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:01:08,481][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:01:08,482][train][INFO] - Logging hyperparameters!
[2022-07-11 15:01:08,486][train][INFO] - Starting training!
[2022-07-11 15:01:27,835][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:01:29,387][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:02:50,543][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:02:50,543][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:02:50,612][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:02:52,251][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:02:54,316][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:02:54,318][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:02:54,319][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:02:54,320][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:02:54,324][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:02:54,325][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:02:54,325][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:02:54,325][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:02:54,326][train][INFO] - Logging hyperparameters!
[2022-07-11 15:02:54,330][train][INFO] - Starting training!
[2022-07-11 15:03:12,728][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:03:14,268][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:04:25,653][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:04:25,654][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:04:25,721][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:04:27,268][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:04:29,304][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:04:29,306][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:04:29,307][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:04:29,309][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:04:29,313][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:04:29,313][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:04:29,313][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:04:29,314][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:04:29,314][train][INFO] - Logging hyperparameters!
[2022-07-11 15:04:29,319][train][INFO] - Starting training!
[2022-07-11 15:04:47,797][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:04:49,347][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:05:32,897][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:05:32,898][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:05:32,963][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:05:34,520][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:05:36,567][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:05:36,569][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:05:36,569][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:05:36,575][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:05:36,579][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:05:36,580][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:05:36,580][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:05:36,580][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:05:36,581][train][INFO] - Logging hyperparameters!
[2022-07-11 15:05:36,585][train][INFO] - Starting training!
[2022-07-11 15:05:54,862][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:05:56,405][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:06:01,471][train][INFO] - Starting testing!
[2022-07-11 15:06:01,978][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:06:02,302][train][INFO] - Finalizing!
[2022-07-11 15:06:49,978][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:06:49,978][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:06:50,047][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:06:52,680][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:06:55,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:06:55,084][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:06:55,085][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:06:55,087][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:06:55,091][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:06:55,091][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:06:55,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:06:55,092][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:06:55,092][train][INFO] - Logging hyperparameters!
[2022-07-11 15:06:55,097][train][INFO] - Starting training!
[2022-07-11 15:07:13,745][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:07:15,294][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:07:20,076][train][INFO] - Starting testing!
[2022-07-11 15:07:20,592][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:07:20,917][train][INFO] - Finalizing!
[2022-07-11 15:39:28,687][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:39:28,688][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:39:28,755][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:39:31,904][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:39:34,699][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:39:34,700][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:39:34,701][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:39:34,703][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:39:34,707][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:39:34,707][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:39:34,707][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:39:34,708][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:39:34,708][train][INFO] - Logging hyperparameters!
[2022-07-11 15:39:34,713][train][INFO] - Starting training!
[2022-07-11 15:39:53,903][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:39:55,442][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:40:00,016][train][INFO] - Starting testing!
[2022-07-11 15:40:00,525][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:40:00,849][train][INFO] - Finalizing!
[2022-07-11 15:53:18,051][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:53:18,052][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:53:18,122][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:53:21,951][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:53:24,472][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:53:24,473][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:53:24,474][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:53:24,476][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:53:24,480][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:53:24,480][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:53:24,480][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:53:24,481][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:53:24,481][train][INFO] - Logging hyperparameters!
[2022-07-11 15:53:24,486][train][INFO] - Starting training!
[2022-07-11 15:53:43,073][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:53:44,616][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:54:49,441][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:54:49,441][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:54:49,510][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:54:51,102][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:54:53,141][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:54:53,143][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:54:53,144][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:54:53,146][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:54:53,150][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:54:53,150][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:54:53,150][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:54:53,151][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:54:53,151][train][INFO] - Logging hyperparameters!
[2022-07-11 15:54:53,156][train][INFO] - Starting training!
[2022-07-11 15:55:11,893][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:55:13,469][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:55:18,316][train][INFO] - Starting testing!
[2022-07-11 15:55:18,833][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:55:19,161][train][INFO] - Finalizing!
[2022-07-11 15:58:56,833][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 15:58:56,834][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 15:58:56,903][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 15:59:00,478][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 15:59:02,915][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 15:59:02,916][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 15:59:02,917][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 15:59:02,919][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 15:59:02,923][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 15:59:02,923][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 15:59:02,923][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 15:59:02,924][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 15:59:02,924][train][INFO] - Logging hyperparameters!
[2022-07-11 15:59:02,929][train][INFO] - Starting training!
[2022-07-11 15:59:21,516][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:59:23,067][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 15:59:27,920][train][INFO] - Starting testing!
[2022-07-11 15:59:28,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 15:59:28,759][train][INFO] - Finalizing!
[2022-07-11 16:00:07,619][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:00:07,619][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:00:07,691][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:00:09,452][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:00:11,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:00:11,506][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:00:11,506][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:00:11,508][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:00:11,512][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:00:11,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:00:11,512][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:00:11,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:00:11,513][train][INFO] - Logging hyperparameters!
[2022-07-11 16:00:11,518][train][INFO] - Starting training!
[2022-07-11 16:00:30,184][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:00:31,751][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:00:36,428][train][INFO] - Starting testing!
[2022-07-11 16:00:36,937][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:00:37,263][train][INFO] - Finalizing!
[2022-07-11 16:04:17,896][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:04:17,897][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:04:17,971][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:04:19,640][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:05:50,118][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:05:50,118][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:05:50,188][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:05:51,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:07:09,560][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:07:09,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:07:09,640][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:07:11,243][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:09:05,427][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:09:05,427][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:09:05,502][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:09:07,107][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:09:56,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:09:56,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:09:56,253][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:09:57,844][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:07,987][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:11:07,988][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:11:08,057][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:11:09,659][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:31,651][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:11:31,652][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:11:31,721][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:11:33,320][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:11:35,452][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:11:35,454][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:11:35,455][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:11:35,456][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:11:35,460][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:11:35,460][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:11:35,461][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:11:35,461][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:11:35,461][train][INFO] - Logging hyperparameters!
[2022-07-11 16:11:35,466][train][INFO] - Starting training!
[2022-07-11 16:11:54,179][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:11:55,766][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:12:00,515][train][INFO] - Starting testing!
[2022-07-11 16:12:01,025][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:12:01,351][train][INFO] - Finalizing!
[2022-07-11 16:17:13,888][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:17:13,888][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:17:13,960][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:17:15,558][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:17:17,585][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:17:17,587][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:17:17,588][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:17:17,590][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:17:17,594][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:17:17,594][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:17:17,594][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:17:17,595][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:17:17,595][train][INFO] - Logging hyperparameters!
[2022-07-11 16:17:17,600][train][INFO] - Starting training!
[2022-07-11 16:17:36,287][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:17:37,858][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:17:42,501][train][INFO] - Starting testing!
[2022-07-11 16:17:43,014][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:17:43,340][train][INFO] - Finalizing!
[2022-07-11 16:21:25,432][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 16:21:25,433][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 16:21:25,508][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 16:21:27,113][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 16:21:29,180][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 16:21:29,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 16:21:29,182][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 16:21:29,184][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 16:21:29,188][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 16:21:29,188][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 16:21:29,188][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 16:21:29,189][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 16:21:29,189][train][INFO] - Logging hyperparameters!
[2022-07-11 16:21:29,194][train][INFO] - Starting training!
[2022-07-11 16:21:47,757][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:21:49,330][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 16:21:53,907][train][INFO] - Starting testing!
[2022-07-11 16:21:54,417][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 16:21:54,736][train][INFO] - Finalizing!
[2022-07-11 17:14:24,336][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:14:24,350][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:14:24,445][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:14:39,911][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:14:42,551][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:14:42,552][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:14:42,553][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:14:42,555][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:14:42,559][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:14:42,559][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:14:42,559][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:14:42,560][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:14:42,560][train][INFO] - Logging hyperparameters!
[2022-07-11 17:14:42,565][train][INFO] - Starting training!
[2022-07-11 17:15:01,102][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:03,342][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:15:21,350][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:15:21,350][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:15:21,418][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:15:23,352][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:15:25,501][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:15:25,502][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:15:25,503][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:15:25,506][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:15:25,510][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:15:25,510][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:15:25,510][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:15:25,511][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:15:25,511][train][INFO] - Logging hyperparameters!
[2022-07-11 17:15:25,516][train][INFO] - Starting training!
[2022-07-11 17:15:44,466][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:45,997][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:15:50,998][train][INFO] - Starting testing!
[2022-07-11 17:15:51,509][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:15:51,830][train][INFO] - Finalizing!
[2022-07-11 17:24:19,602][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 17:24:19,603][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 17:24:19,669][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 17:24:21,950][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 17:24:24,332][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 17:24:24,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 17:24:24,335][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 17:24:24,336][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 17:24:24,340][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 17:24:24,340][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 17:24:24,341][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 17:24:24,341][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 17:24:24,341][train][INFO] - Logging hyperparameters!
[2022-07-11 17:24:24,346][train][INFO] - Starting training!
[2022-07-11 17:24:42,774][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:24:44,291][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 17:24:48,859][train][INFO] - Starting testing!
[2022-07-11 17:24:49,359][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 17:24:49,765][train][INFO] - Finalizing!
[2022-07-11 22:41:23,502][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 22:41:23,517][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 22:41:23,618][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 22:41:41,997][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 22:41:44,745][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 22:41:44,746][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 22:41:44,747][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 22:41:44,749][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 22:41:44,753][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 22:41:44,753][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 22:41:44,753][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 22:41:44,754][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-11 22:41:44,754][train][INFO] - Logging hyperparameters!
[2022-07-11 22:41:44,759][train][INFO] - Starting training!
[2022-07-11 22:42:03,262][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 22:42:05,448][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 22:42:11,012][train][INFO] - Starting testing!
[2022-07-11 22:42:11,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 22:42:11,852][train][INFO] - Finalizing!
[2022-07-11 23:12:04,540][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 23:12:04,540][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 23:12:04,609][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 23:12:14,882][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 23:12:17,539][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 23:12:17,541][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 23:12:17,542][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 23:12:17,544][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 23:12:17,548][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 23:12:17,548][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 23:12:17,548][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 23:12:17,549][train][INFO] - Logging hyperparameters!
[2022-07-11 23:12:17,610][train][INFO] - Starting training!
[2022-07-11 23:12:35,973][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 23:12:37,514][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-11 23:13:23,875][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-11 23:13:23,876][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-11 23:13:23,944][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-11 23:13:25,666][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-11 23:13:27,827][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-11 23:13:27,828][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-11 23:13:27,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-11 23:13:27,831][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-11 23:13:27,835][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-11 23:13:27,835][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-11 23:13:27,835][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-11 23:13:27,836][train][INFO] - Logging hyperparameters!
[2022-07-11 23:13:27,856][train][INFO] - Starting training!
[2022-07-11 23:13:46,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-11 23:13:48,052][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 00:23:33,965][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 00:23:33,984][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 00:23:34,103][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 00:23:56,099][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 00:23:59,031][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 00:23:59,032][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 00:23:59,033][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 00:23:59,034][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 00:27:36,331][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 00:27:36,332][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 00:27:36,419][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 00:27:53,064][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 00:27:55,638][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 00:27:55,640][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 00:27:55,640][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 00:27:55,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,659][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,660][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,654][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,660][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,648][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,650][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 00:27:55,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 00:27:55,663][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 00:27:55,666][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 00:27:55,666][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 00:27:55,667][train][INFO] - Logging hyperparameters!
[2022-07-12 00:27:55,833][train][INFO] - Starting training!
[2022-07-12 00:27:55,834][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 00:27:56,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 00:27:56,672][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 00:27:56,672][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,673][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 00:27:56,674][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,217][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,371][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 00:28:16,382][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 00:46:52,627][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,628][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,628][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,629][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,630][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,632][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,647][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 00:46:52,644][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 09:31:51,722][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 09:31:51,747][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 09:31:51,848][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 09:32:09,737][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 09:32:12,532][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 09:32:12,534][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 09:32:12,534][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 09:32:12,536][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 09:32:12,538][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 09:32:12,539][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 09:32:12,539][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,539][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 09:32:12,539][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,540][train][INFO] - Logging hyperparameters!
[2022-07-12 09:32:12,540][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 09:32:12,544][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,545][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 09:32:12,549][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,550][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 09:32:12,553][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,554][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 09:32:12,554][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,555][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 09:32:12,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 09:32:12,564][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 09:32:12,651][train][INFO] - Starting training!
[2022-07-12 09:32:12,652][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 09:32:13,540][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 09:32:13,541][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 09:32:13,546][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 09:32:13,551][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 09:32:13,555][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 09:32:13,556][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 09:32:13,564][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,571][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,572][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 09:32:13,575][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,575][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,576][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,577][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:13,581][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,266][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 09:32:33,276][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 11:13:22,416][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 11:13:22,451][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 11:13:22,553][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 11:13:40,121][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 11:13:42,798][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 11:13:42,800][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 11:13:42,801][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 11:13:42,802][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 11:13:42,804][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 11:13:42,805][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 11:13:42,805][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 11:13:42,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,806][train][INFO] - Logging hyperparameters!
[2022-07-12 11:13:42,806][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 11:13:42,807][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,807][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,808][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 11:13:42,808][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,808][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 11:13:42,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 11:13:42,815][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,815][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 11:13:42,816][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 11:13:42,816][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 11:13:42,972][train][INFO] - Starting training!
[2022-07-12 11:13:42,973][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 11:13:43,806][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 11:13:43,807][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 11:13:43,809][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 11:13:43,809][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 11:13:43,810][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 11:13:43,816][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 11:13:43,817][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 11:13:43,821][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 11:13:43,821][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,821][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 11:13:43,827][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,828][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,829][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,829][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 11:13:43,830][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:02,961][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,127][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 11:14:03,137][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:11:37,224][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:11:37,245][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:11:37,439][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:11:53,591][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:12:52,187][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:12:52,188][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:12:52,258][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:12:54,478][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:12:57,326][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:12:57,380][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:12:57,381][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:12:57,383][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:12:57,387][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:12:57,387][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:12:57,387][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:12:57,388][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:12:57,388][train][INFO] - Logging hyperparameters!
[2022-07-12 12:12:57,393][train][INFO] - Starting training!
[2022-07-12 12:12:57,393][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:12:57,401][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:12:57,401][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:12:57,401][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:13:17,750][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:13:17,922][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:13:23,238][train][INFO] - Starting testing!
[2022-07-12 12:13:23,759][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:13:24,112][train][INFO] - Finalizing!
[2022-07-12 12:14:07,939][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:14:07,940][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:14:08,032][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:14:10,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:14:13,334][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:14:13,337][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:14:13,337][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:14:13,339][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:14:13,343][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:14:13,344][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:14:13,344][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:14:13,344][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:14:13,345][train][INFO] - Logging hyperparameters!
[2022-07-12 12:14:13,349][train][INFO] - Starting training!
[2022-07-12 12:14:13,350][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:14:13,351][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:14:13,351][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:14:13,352][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:14:33,629][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:14:33,773][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:14:38,228][train][INFO] - Starting testing!
[2022-07-12 12:14:38,733][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:14:39,089][train][INFO] - Finalizing!
[2022-07-12 12:17:05,144][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:17:05,144][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:17:05,212][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:17:08,358][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:17:10,847][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:17:10,848][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:17:10,849][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:17:10,851][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:17:10,855][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:17:10,855][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:17:10,855][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:17:10,856][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:17:10,869][train][INFO] - Logging hyperparameters!
[2022-07-12 12:17:10,874][train][INFO] - Starting training!
[2022-07-12 12:17:10,875][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:17:10,875][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:17:10,876][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:17:10,876][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:17:31,248][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:17:31,390][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:17:36,099][train][INFO] - Starting testing!
[2022-07-12 12:17:36,607][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:17:36,963][train][INFO] - Finalizing!
[2022-07-12 12:18:41,419][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:18:41,419][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:18:41,486][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:18:43,763][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:18:46,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:18:46,029][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:18:46,030][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:18:46,031][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:18:46,035][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:18:46,036][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:18:46,036][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:18:46,036][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:18:46,037][train][INFO] - Logging hyperparameters!
[2022-07-12 12:18:46,041][train][INFO] - Starting training!
[2022-07-12 12:18:46,042][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:18:46,043][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:18:46,043][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:18:46,044][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:19:06,341][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:19:06,506][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:19:11,010][train][INFO] - Starting testing!
[2022-07-12 12:19:11,522][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:19:11,878][train][INFO] - Finalizing!
[2022-07-12 12:28:02,480][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:28:02,480][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:28:02,549][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:28:06,015][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:28:08,636][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:28:08,638][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:28:08,639][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:28:08,640][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:28:08,645][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:28:08,645][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:28:08,645][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:28:08,646][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:28:08,646][train][INFO] - Logging hyperparameters!
[2022-07-12 12:28:08,651][train][INFO] - Starting training!
[2022-07-12 12:28:08,651][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:28:08,652][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:28:08,653][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:28:08,653][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:28:29,310][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:28:29,454][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:28:34,366][train][INFO] - Starting testing!
[2022-07-12 12:28:34,879][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:28:35,236][train][INFO] - Finalizing!
[2022-07-12 12:29:20,173][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:29:20,173][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:29:20,243][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:29:22,925][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:29:25,197][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:29:25,199][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:29:25,200][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:29:25,202][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:29:25,206][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:29:25,206][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:29:25,206][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:29:25,207][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:29:25,207][train][INFO] - Logging hyperparameters!
[2022-07-12 12:29:25,212][train][INFO] - Starting training!
[2022-07-12 12:29:25,213][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:29:25,214][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:29:25,214][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:29:25,214][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:29:45,551][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:29:45,695][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:29:50,129][train][INFO] - Starting testing!
[2022-07-12 12:29:50,640][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:29:51,003][train][INFO] - Finalizing!
[2022-07-12 12:32:27,956][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:32:27,957][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:32:28,026][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:32:30,823][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:32:33,392][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:32:33,394][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:32:33,395][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:32:33,397][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:32:33,401][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:32:33,421][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:32:33,421][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:32:33,422][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:32:33,422][train][INFO] - Logging hyperparameters!
[2022-07-12 12:32:33,426][train][INFO] - Starting training!
[2022-07-12 12:32:33,427][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:32:33,428][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:32:33,428][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:32:33,429][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:32:53,830][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:32:53,973][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:34:01,888][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:34:01,904][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:34:01,976][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:34:04,373][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:35:59,457][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 12:35:59,459][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 12:35:59,550][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 12:36:01,712][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 12:36:04,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 12:36:04,261][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 12:36:04,262][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 12:36:04,264][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 12:36:04,268][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 12:36:04,268][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 12:36:04,268][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 12:36:04,269][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 12:36:04,269][train][INFO] - Logging hyperparameters!
[2022-07-12 12:36:04,274][train][INFO] - Starting training!
[2022-07-12 12:36:04,275][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 12:36:04,276][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 12:36:04,305][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 12:36:04,306][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 12:36:24,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:36:24,711][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 12:36:29,471][train][INFO] - Starting testing!
[2022-07-12 12:36:29,981][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 12:36:30,337][train][INFO] - Finalizing!
[2022-07-12 16:02:00,060][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:02:00,090][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:02:00,182][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:02:16,471][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:02:19,269][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:02:19,272][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:02:19,273][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:02:19,274][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:02:19,278][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:02:19,279][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:02:19,279][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:02:19,279][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:02:19,280][train][INFO] - Logging hyperparameters!
[2022-07-12 16:02:19,284][train][INFO] - Starting training!
[2022-07-12 16:02:19,285][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:02:19,286][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:02:19,287][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:02:19,287][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:02:40,551][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:02:40,694][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 16:02:46,849][train][INFO] - Starting testing!
[2022-07-12 16:02:47,365][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:02:47,754][train][INFO] - Finalizing!
[2022-07-12 16:07:33,219][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:07:33,219][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:07:33,287][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:07:34,955][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:07:37,177][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:07:37,179][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:07:37,180][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:07:37,182][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:07:37,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:07:37,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:07:37,187][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:07:37,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:07:37,187][train][INFO] - Logging hyperparameters!
[2022-07-12 16:07:37,192][train][INFO] - Starting training!
[2022-07-12 16:07:37,193][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:07:37,194][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:07:37,194][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:07:37,194][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:07:57,363][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:07:57,517][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-12 16:08:02,107][train][INFO] - Starting testing!
[2022-07-12 16:08:02,617][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:08:03,005][train][INFO] - Finalizing!
[2022-07-12 16:32:05,705][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:32:05,705][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:32:05,773][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:32:07,311][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:32:10,086][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:32:10,087][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:32:10,088][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:32:10,090][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:32:10,094][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:32:10,094][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:32:10,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:32:10,095][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:32:10,095][train][INFO] - Logging hyperparameters!
[2022-07-12 16:32:10,100][train][INFO] - Starting training!
[2022-07-12 16:32:10,101][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:32:10,102][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:32:10,102][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:32:10,102][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:32:30,012][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:32:30,221][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:33:51,558][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:33:51,559][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:33:51,627][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:33:53,148][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:33:55,901][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:33:55,903][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:33:55,904][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:33:55,906][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:33:55,910][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:33:55,910][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:33:55,910][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:33:55,911][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:33:55,911][train][INFO] - Logging hyperparameters!
[2022-07-12 16:33:55,916][train][INFO] - Starting training!
[2022-07-12 16:33:55,941][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:33:55,942][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:33:55,942][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:33:55,942][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:34:15,955][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:34:16,163][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:34:41,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:34:41,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:34:41,250][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:34:42,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:34:45,569][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:34:45,571][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:34:45,572][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:34:45,574][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:34:45,578][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:34:45,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:34:45,578][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:34:45,579][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:34:45,579][train][INFO] - Logging hyperparameters!
[2022-07-12 16:34:45,584][train][INFO] - Starting training!
[2022-07-12 16:34:45,585][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:34:45,586][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:34:45,586][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:34:45,586][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:35:05,438][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:35:05,647][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:35:25,906][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:35:25,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:35:25,976][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:35:27,499][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:35:30,248][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:35:30,250][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:35:30,251][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:35:30,253][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:35:30,257][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:35:30,257][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:35:30,257][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:35:30,258][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:35:30,258][train][INFO] - Logging hyperparameters!
[2022-07-12 16:35:30,263][train][INFO] - Starting training!
[2022-07-12 16:35:30,264][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:35:30,265][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:35:30,265][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:35:30,265][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:35:50,322][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:35:50,531][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:36:19,032][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:36:19,032][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:36:19,100][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:36:20,612][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:36:23,308][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:36:23,310][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:36:23,310][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:36:23,326][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:36:23,330][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:36:23,330][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:36:23,331][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:36:23,331][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:36:23,331][train][INFO] - Logging hyperparameters!
[2022-07-12 16:36:23,336][train][INFO] - Starting training!
[2022-07-12 16:36:23,337][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:36:23,338][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:36:23,338][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:36:23,338][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:36:43,276][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:36:43,482][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:37:04,634][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:37:04,634][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:37:04,703][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:37:06,225][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:37:08,958][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:37:08,960][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:37:08,960][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:37:08,962][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:37:08,966][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:37:08,967][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:37:08,967][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:37:08,967][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:37:08,968][train][INFO] - Logging hyperparameters!
[2022-07-12 16:37:08,972][train][INFO] - Starting training!
[2022-07-12 16:37:08,973][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:37:08,974][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:37:08,974][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:37:08,976][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:37:29,354][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:37:29,560][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:37:35,798][train][INFO] - Starting testing!
[2022-07-12 16:37:36,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:37:36,839][train][INFO] - Finalizing!
[2022-07-12 16:38:19,845][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:38:19,845][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:38:19,913][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:38:21,431][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:38:24,111][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:38:24,113][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:38:24,113][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:38:24,115][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:38:24,119][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:38:24,119][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:38:24,120][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:38:24,120][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:38:24,120][train][INFO] - Logging hyperparameters!
[2022-07-12 16:38:24,125][train][INFO] - Starting training!
[2022-07-12 16:38:24,126][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:38:24,127][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:38:24,127][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:38:24,127][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:38:44,122][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:38:44,329][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:38:50,414][train][INFO] - Starting testing!
[2022-07-12 16:38:50,923][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:38:51,444][train][INFO] - Finalizing!
[2022-07-12 16:45:11,292][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:45:11,294][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:45:11,382][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:45:12,910][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:45:15,587][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:45:15,589][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:45:15,589][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:45:15,591][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:45:15,595][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:45:15,596][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:45:15,596][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:45:15,596][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:45:15,597][train][INFO] - Logging hyperparameters!
[2022-07-12 16:45:15,601][train][INFO] - Starting training!
[2022-07-12 16:45:15,602][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:45:15,603][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:45:15,603][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:45:15,604][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:45:35,449][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:45:35,657][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:45:41,057][train][INFO] - Starting testing!
[2022-07-12 16:45:41,572][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:45:42,255][train][INFO] - Finalizing!
[2022-07-12 16:46:16,493][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:46:16,493][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:46:16,561][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:46:18,092][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:46:20,746][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:46:20,747][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:46:20,748][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:46:20,750][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:46:20,754][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:46:20,754][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:46:20,755][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:46:20,755][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:46:20,755][train][INFO] - Logging hyperparameters!
[2022-07-12 16:46:20,760][train][INFO] - Starting training!
[2022-07-12 16:46:20,761][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:46:20,762][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:46:20,762][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:46:20,762][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:46:40,570][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:46:40,777][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:46:46,093][train][INFO] - Starting testing!
[2022-07-12 16:46:46,599][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:46:47,288][train][INFO] - Finalizing!
[2022-07-12 16:47:35,853][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:47:35,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:47:35,920][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:47:37,454][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:47:40,159][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:47:40,161][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:47:40,162][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:47:40,164][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:47:40,168][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:47:40,168][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:47:40,168][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:47:40,196][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-12 16:47:40,196][train][INFO] - Logging hyperparameters!
[2022-07-12 16:47:40,201][train][INFO] - Starting training!
[2022-07-12 16:47:40,202][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:47:40,203][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:47:40,203][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:47:40,203][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:48:00,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:48:00,626][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:48:06,003][train][INFO] - Starting testing!
[2022-07-12 16:48:06,516][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:48:07,243][train][INFO] - Finalizing!
[2022-07-12 16:50:00,732][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:50:00,732][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:50:00,800][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:50:02,340][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:50:05,011][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:50:05,013][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:50:05,014][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:50:05,015][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:50:05,019][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:50:05,020][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:50:05,020][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:50:05,020][train][INFO] - Logging hyperparameters!
[2022-07-12 16:50:05,065][train][INFO] - Starting training!
[2022-07-12 16:50:05,067][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-12 16:50:05,068][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:50:05,068][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-12 16:50:05,068][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:50:25,025][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-12 16:50:25,228][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 16:56:48,203][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 16:56:48,225][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 16:56:48,344][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 16:57:08,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 16:57:12,025][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 16:57:12,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 16:57:12,028][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 16:57:12,029][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 16:57:12,032][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 16:57:12,032][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,033][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,033][train][INFO] - Logging hyperparameters!
[2022-07-12 16:57:12,034][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 16:57:12,035][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,036][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 16:57:12,037][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,037][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,038][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 16:57:12,038][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 16:57:12,041][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,042][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 16:57:12,044][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,045][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 16:57:12,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 16:57:12,046][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 16:57:12,158][train][INFO] - Starting training!
[2022-07-12 16:57:12,159][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 16:57:13,035][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 16:57:13,037][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 16:57:13,039][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 16:57:13,039][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 16:57:13,043][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 16:57:13,046][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 16:57:13,047][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,049][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,049][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 16:57:13,050][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,053][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,056][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,056][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,057][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:13,058][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,680][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 16:57:32,691][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 21:04:22,850][train][INFO] - Starting testing!
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:23,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 21:04:24,204][train][INFO] - Finalizing!
[2022-07-12 21:04:24,205][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_005.ckpt
[2022-07-12 23:03:12,531][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:03:12,532][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:03:12,629][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:03:32,701][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:03:35,631][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:03:35,632][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:03:35,633][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:03:35,634][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:03:35,636][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,637][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:03:35,637][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:03:35,637][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:03:35,638][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:03:35,638][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,638][train][INFO] - Logging hyperparameters!
[2022-07-12 23:03:35,642][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,643][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:03:35,645][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,646][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:03:35,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,647][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,656][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,651][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:03:35,677][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:03:35,769][train][INFO] - Starting training!
[2022-07-12 23:03:35,770][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:03:36,638][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:03:36,643][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:03:36,677][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:03:36,678][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,679][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,679][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:03:36,684][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,688][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:36,689][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,042][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,260][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:03:56,270][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 23:23:11,887][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:23:11,899][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:23:12,024][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:23:32,480][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:23:35,549][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:23:35,551][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:23:35,551][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:23:35,552][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:23:35,556][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:23:35,556][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,557][train][INFO] - Logging hyperparameters!
[2022-07-12 23:23:35,561][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,562][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:23:35,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,564][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:23:35,565][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,567][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:23:35,570][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,571][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:23:35,572][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,573][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:23:35,574][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,575][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:23:35,576][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:23:35,577][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:23:35,685][train][INFO] - Starting training!
[2022-07-12 23:23:35,686][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:23:36,563][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:23:36,565][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:23:36,568][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:23:36,572][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:23:36,574][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:23:36,576][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:23:36,578][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,585][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,585][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:23:36,586][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,588][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,588][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,592][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,594][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:36,594][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,027][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,242][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:23:56,252][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-12 23:43:18,434][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:43:18,435][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-12 23:44:26,401][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-12 23:44:26,403][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-12 23:44:26,544][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-12 23:44:47,950][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-12 23:44:51,037][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-12 23:44:51,039][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-12 23:44:51,039][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-12 23:44:51,041][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-12 23:44:51,044][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-12 23:44:51,044][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-12 23:44:51,045][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-12 23:44:51,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,045][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,046][train][INFO] - Logging hyperparameters!
[2022-07-12 23:44:51,046][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-12 23:44:51,048][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,049][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,049][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-12 23:44:51,050][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-12 23:44:51,052][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,052][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,053][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-12 23:44:51,053][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-12 23:44:51,057][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,058][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-12 23:44:51,060][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-12 23:44:51,061][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-12 23:44:51,185][train][INFO] - Starting training!
[2022-07-12 23:44:51,185][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-12 23:44:52,047][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-12 23:44:52,050][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-12 23:44:52,051][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-12 23:44:52,054][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-12 23:44:52,054][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-12 23:44:52,059][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-12 23:44:52,062][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-12 23:44:52,064][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-12 23:44:52,064][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,065][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-12 23:44:52,065][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,065][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,068][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,069][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,071][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,071][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-12 23:44:52,072][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,702][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,919][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-12 23:45:11,929][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 00:34:17,379][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 00:34:17,404][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 00:34:17,520][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 00:34:35,081][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 00:34:38,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 00:34:38,084][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 00:34:38,085][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 00:34:38,086][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 00:34:38,089][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 00:34:38,089][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 00:34:38,090][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 00:34:38,090][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,091][train][INFO] - Logging hyperparameters!
[2022-07-13 00:34:38,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,093][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 00:34:38,093][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,094][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 00:34:38,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,096][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 00:34:38,097][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,098][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 00:34:38,098][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,099][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 00:34:38,100][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,101][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 00:34:38,112][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 00:34:38,113][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 00:34:38,217][train][INFO] - Starting training!
[2022-07-13 00:34:38,218][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 00:34:39,094][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 00:34:39,095][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 00:34:39,097][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 00:34:39,099][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 00:34:39,100][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 00:34:39,102][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 00:34:39,114][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,117][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,117][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 00:34:39,119][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,121][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,123][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,124][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,125][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:39,126][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,361][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,578][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 00:34:58,588][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 00:43:43,599][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,677][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,690][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,692][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,701][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,695][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,683][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,682][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:33:45,691][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 10:49:27,499][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 10:49:27,545][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 10:49:27,695][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 10:49:50,847][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 10:49:54,595][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 10:49:54,597][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 10:49:54,598][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 10:49:54,600][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 10:49:54,604][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 10:49:54,604][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 10:49:54,604][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 10:49:54,605][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 10:49:54,605][train][INFO] - Logging hyperparameters!
[2022-07-13 10:49:54,610][train][INFO] - Starting training!
[2022-07-13 10:49:54,611][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 10:49:54,612][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 10:49:54,612][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 10:49:54,613][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 10:50:16,358][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 10:50:16,679][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 10:50:25,466][train][INFO] - Starting testing!
[2022-07-13 10:50:25,979][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 10:50:26,688][train][INFO] - Finalizing!
[2022-07-13 10:58:13,413][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 10:58:13,413][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 10:58:13,491][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 10:58:22,403][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:00:03,570][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:00:03,570][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:00:03,651][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:00:06,369][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:00:09,410][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:00:09,412][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:00:09,413][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:00:09,415][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:00:09,419][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:00:09,419][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:00:09,419][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:00:09,420][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:00:09,420][train][INFO] - Logging hyperparameters!
[2022-07-13 11:00:09,425][train][INFO] - Starting training!
[2022-07-13 11:00:09,426][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:00:09,427][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:00:09,427][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:00:09,427][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:00:29,930][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:00:30,139][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 11:02:18,219][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:02:18,219][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:02:18,290][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:02:19,835][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:02:22,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:02:22,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:02:22,451][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:02:22,452][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:02:22,457][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:02:22,477][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:02:22,478][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:02:22,478][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:02:22,478][train][INFO] - Logging hyperparameters!
[2022-07-13 11:02:22,483][train][INFO] - Starting training!
[2022-07-13 11:02:22,484][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:02:22,485][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:02:22,485][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:02:22,485][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:02:42,877][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:02:43,088][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 75.5 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
133 M     Trainable params
222 K     Non-trainable params
133 M     Total params
535.197   Total estimated model params size (MB)
[2022-07-13 11:03:32,621][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:03:32,621][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:03:32,735][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:03:35,392][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:03:37,903][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:03:37,905][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:03:37,906][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:03:37,908][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:03:37,913][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:03:37,913][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:03:37,913][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:03:37,914][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:03:37,914][train][INFO] - Logging hyperparameters!
[2022-07-13 11:03:37,920][train][INFO] - Starting training!
[2022-07-13 11:03:37,921][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:03:37,922][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:03:37,922][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:03:37,922][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:03:58,146][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:03:58,290][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:04:04,186][train][INFO] - Starting testing!
[2022-07-13 11:04:04,691][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:04:05,075][train][INFO] - Finalizing!
[2022-07-13 11:05:11,757][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:05:11,757][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:05:11,825][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:05:13,429][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:05:15,457][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:05:15,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:05:15,460][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:05:15,462][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:05:15,466][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:05:15,466][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:05:15,466][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:05:15,467][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:05:15,467][train][INFO] - Logging hyperparameters!
[2022-07-13 11:05:15,472][train][INFO] - Starting training!
[2022-07-13 11:05:15,473][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:05:15,474][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:05:15,474][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:05:15,474][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:05:35,704][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:05:35,849][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:05:41,892][train][INFO] - Starting testing!
[2022-07-13 11:05:42,398][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:05:42,764][train][INFO] - Finalizing!
[2022-07-13 11:06:59,173][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:06:59,183][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:06:59,253][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:07:01,677][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:07:04,124][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:07:04,126][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:07:04,127][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:07:04,128][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:07:04,133][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:07:04,133][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:07:04,133][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:07:04,134][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:07:04,134][train][INFO] - Logging hyperparameters!
[2022-07-13 11:07:04,139][train][INFO] - Starting training!
[2022-07-13 11:07:04,140][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:07:04,141][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:07:04,141][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:07:04,141][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:07:24,484][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:07:24,628][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 177 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.3 M    Trainable params
222 K     Non-trainable params
58.5 M    Total params
233.895   Total estimated model params size (MB)
[2022-07-13 11:09:37,864][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:09:37,865][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:09:37,932][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:09:39,641][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:10:10,965][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:10:10,965][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:10:11,034][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:10:13,716][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:10:16,861][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:10:16,863][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:10:16,864][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:10:16,865][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:10:16,870][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:10:16,870][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:10:16,870][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:10:16,871][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:10:16,871][train][INFO] - Logging hyperparameters!
[2022-07-13 11:10:16,877][train][INFO] - Starting training!
[2022-07-13 11:10:16,878][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:10:16,879][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:10:16,879][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:10:16,879][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:10:37,171][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:10:37,360][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:11:04,874][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:11:04,874][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:11:04,943][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:11:06,925][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:11:09,838][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:11:09,840][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:11:09,841][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:11:09,842][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:11:09,846][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:11:09,847][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:11:09,847][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:11:09,847][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:11:09,848][train][INFO] - Logging hyperparameters!
[2022-07-13 11:11:09,854][train][INFO] - Starting training!
[2022-07-13 11:11:09,881][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:11:09,882][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:11:09,882][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:11:09,883][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:11:29,978][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:11:30,167][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:12:55,279][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:12:55,280][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:12:55,349][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:12:56,921][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:12:59,894][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:12:59,896][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:12:59,897][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:12:59,898][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:12:59,902][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:12:59,903][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:12:59,903][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:12:59,903][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:12:59,904][train][INFO] - Logging hyperparameters!
[2022-07-13 11:12:59,910][train][INFO] - Starting training!
[2022-07-13 11:12:59,911][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:12:59,912][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:12:59,912][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:12:59,912][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:13:20,419][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:13:20,607][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:13:40,926][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:13:40,927][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:13:40,998][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:13:44,037][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:13:47,097][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:13:47,099][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:13:47,100][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:13:47,101][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:13:47,106][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:13:47,106][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:13:47,106][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:13:47,107][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:13:47,107][train][INFO] - Logging hyperparameters!
[2022-07-13 11:13:47,113][train][INFO] - Starting training!
[2022-07-13 11:13:47,114][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:13:47,115][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:13:47,115][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:13:47,115][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:14:07,301][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:14:07,488][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:14:50,260][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:14:50,261][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:14:50,329][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:14:51,862][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:14:54,826][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:14:54,828][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:14:54,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:14:54,845][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:14:54,849][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:14:54,849][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:14:54,849][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:14:54,850][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:14:54,850][train][INFO] - Logging hyperparameters!
[2022-07-13 11:14:54,856][train][INFO] - Starting training!
[2022-07-13 11:14:54,857][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:14:54,858][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:14:54,858][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:14:54,859][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:15:15,151][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:15:15,341][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:15:34,984][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:15:34,985][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:15:35,053][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:15:36,591][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:15:39,529][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:15:39,531][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:15:39,532][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:15:39,533][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:15:39,538][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:15:39,538][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:15:39,538][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:15:39,539][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:15:39,539][train][INFO] - Logging hyperparameters!
[2022-07-13 11:15:39,545][train][INFO] - Starting training!
[2022-07-13 11:15:39,546][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:15:39,547][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:15:39,547][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:15:39,548][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:15:59,561][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:15:59,749][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 23.7 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
81.8 M    Trainable params
222 K     Non-trainable params
82.0 M    Total params
327.907   Total estimated model params size (MB)
[2022-07-13 11:16:18,708][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:16:18,708][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:16:18,775][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:16:20,337][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:16:23,309][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:16:23,311][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:16:23,311][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:16:23,313][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:16:23,317][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:16:23,318][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:16:23,318][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:16:23,318][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:16:23,319][train][INFO] - Logging hyperparameters!
[2022-07-13 11:16:23,324][train][INFO] - Starting training!
[2022-07-13 11:16:23,325][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:16:23,326][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:16:23,327][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:16:23,327][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:16:43,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:16:43,699][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 8.7 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
66.8 M    Trainable params
222 K     Non-trainable params
67.0 M    Total params
268.048   Total estimated model params size (MB)
[2022-07-13 11:17:02,144][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:17:02,144][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:17:02,213][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:17:04,740][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:17:07,720][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:17:07,722][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:17:07,723][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:17:07,724][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:17:07,728][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:17:07,729][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:17:07,729][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:17:07,729][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:17:07,730][train][INFO] - Logging hyperparameters!
[2022-07-13 11:17:07,735][train][INFO] - Starting training!
[2022-07-13 11:17:07,736][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:17:07,737][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:17:07,737][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:17:07,737][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:17:27,849][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:17:28,000][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 1.6 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.7 M    Trainable params
222 K     Non-trainable params
59.9 M    Total params
239.655   Total estimated model params size (MB)
[2022-07-13 11:17:55,264][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:17:55,265][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:17:55,334][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:17:56,868][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:17:59,857][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:17:59,859][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:17:59,859][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:17:59,861][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:17:59,865][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:17:59,866][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:17:59,866][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:17:59,867][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:17:59,867][train][INFO] - Logging hyperparameters!
[2022-07-13 11:17:59,873][train][INFO] - Starting training!
[2022-07-13 11:17:59,873][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:17:59,874][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:17:59,875][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:17:59,875][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:18:19,923][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:18:20,074][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:18:42,532][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:18:42,532][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:18:42,600][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:18:44,143][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:18:47,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:18:47,085][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:18:47,086][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:18:47,088][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:18:47,092][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:18:47,092][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:18:47,092][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:18:47,093][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:18:47,093][train][INFO] - Logging hyperparameters!
[2022-07-13 11:18:47,098][train][INFO] - Starting training!
[2022-07-13 11:18:47,099][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:18:47,100][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:18:47,100][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:18:47,101][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:19:07,329][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:07,480][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:19:19,016][train][INFO] - Starting testing!
[2022-07-13 11:19:19,539][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:19,921][train][INFO] - Finalizing!
[2022-07-13 11:19:29,486][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:19:29,486][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:19:29,554][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:19:32,252][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:19:35,510][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:19:35,512][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:19:35,513][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:19:35,515][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:19:35,519][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:19:35,519][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:19:35,519][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:19:35,520][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:19:35,520][train][INFO] - Logging hyperparameters!
[2022-07-13 11:19:35,525][train][INFO] - Starting training!
[2022-07-13 11:19:35,526][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:19:35,527][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:19:35,527][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:19:35,528][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:19:55,778][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:19:55,928][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:20:07,185][train][INFO] - Starting testing!
[2022-07-13 11:20:07,708][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:20:08,108][train][INFO] - Finalizing!
[2022-07-13 11:23:45,891][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:23:45,903][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:24:29,361][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:24:29,361][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:24:29,432][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:24:32,327][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:24:35,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:24:35,616][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:24:35,617][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:24:35,618][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:24:35,624][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:24:35,625][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:24:35,625][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:24:35,625][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:24:35,626][train][INFO] - Logging hyperparameters!
[2022-07-13 11:24:35,631][train][INFO] - Starting training!
[2022-07-13 11:24:35,632][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:24:35,633][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:24:35,633][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:24:35,633][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:24:55,750][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:24:55,900][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:25:07,160][train][INFO] - Starting testing!
[2022-07-13 11:25:07,678][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:25:08,252][train][INFO] - Finalizing!
[2022-07-13 11:25:38,489][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:25:38,489][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:25:38,559][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:25:40,438][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:25:43,368][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:25:43,370][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:25:43,371][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:25:43,372][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:25:43,378][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:25:43,378][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:25:43,378][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:25:43,379][train][INFO] - Logging hyperparameters!
[2022-07-13 11:25:43,731][train][INFO] - Starting training!
[2022-07-13 11:25:43,733][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:25:43,734][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:25:43,734][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:25:43,735][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:26:03,928][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:26:04,094][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:28:30,046][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:28:30,058][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:28:30,127][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:28:32,927][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:28:36,166][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:28:36,168][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:28:36,169][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:28:36,170][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:28:36,176][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:28:36,176][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:28:36,176][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:28:36,177][train][INFO] - Logging hyperparameters!
[2022-07-13 11:28:36,199][train][INFO] - Starting training!
[2022-07-13 11:28:36,201][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:28:36,202][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:28:36,202][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:28:36,207][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:28:56,243][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:28:56,430][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:34:00,167][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:34:00,191][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:34:00,261][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:34:03,766][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:34:07,152][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:34:07,154][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:34:07,155][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:34:07,156][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:34:07,162][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:34:07,162][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:34:07,163][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:34:07,163][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:34:07,163][train][INFO] - Logging hyperparameters!
[2022-07-13 11:34:07,169][train][INFO] - Starting training!
[2022-07-13 11:34:07,170][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:34:07,171][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:34:07,171][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:34:07,171][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:34:27,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:34:27,510][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:40:36,127][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:40:36,127][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:40:36,196][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:40:39,746][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:40:43,068][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:40:43,070][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:40:43,071][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:40:43,072][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:40:43,078][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:40:43,078][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:40:43,079][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:40:43,079][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:40:43,079][train][INFO] - Logging hyperparameters!
[2022-07-13 11:40:43,085][train][INFO] - Starting training!
[2022-07-13 11:40:43,086][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:40:43,087][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:40:43,087][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:40:43,087][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:41:03,468][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:41:03,619][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:41:50,436][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:41:50,436][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:41:50,506][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:41:52,159][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:41:55,091][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:41:55,093][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:41:55,093][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:41:55,095][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:41:55,101][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:41:55,101][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:41:55,102][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:41:55,103][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:41:55,103][train][INFO] - Logging hyperparameters!
[2022-07-13 11:41:55,108][train][INFO] - Starting training!
[2022-07-13 11:41:55,109][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:41:55,110][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:41:55,110][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:41:55,111][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:42:15,251][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:42:15,402][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:42:54,686][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:42:54,686][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:42:54,756][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:42:56,296][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:42:59,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:42:59,261][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:42:59,262][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:42:59,264][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:42:59,269][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:42:59,270][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:42:59,270][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:42:59,270][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:42:59,271][train][INFO] - Logging hyperparameters!
[2022-07-13 11:42:59,276][train][INFO] - Starting training!
[2022-07-13 11:42:59,277][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:42:59,278][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:42:59,278][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:42:59,279][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:43:19,326][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:43:19,478][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:43:36,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:43:36,570][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:43:36,640][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:43:39,454][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:43:42,564][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:43:42,565][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:43:42,566][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:43:42,568][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:43:42,574][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:43:42,574][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:43:42,574][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:43:42,575][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:43:42,575][train][INFO] - Logging hyperparameters!
[2022-07-13 11:43:42,580][train][INFO] - Starting training!
[2022-07-13 11:43:42,581][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:43:42,582][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:43:42,582][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:43:42,583][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:44:02,791][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:44:02,941][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:44:36,789][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:44:36,789][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:44:36,860][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:44:38,801][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:44:41,934][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:44:41,936][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:44:41,937][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:44:41,938][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:44:41,944][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:44:41,944][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:44:41,944][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:44:41,945][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:44:41,945][train][INFO] - Logging hyperparameters!
[2022-07-13 11:44:41,951][train][INFO] - Starting training!
[2022-07-13 11:44:41,952][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:44:41,953][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:44:41,953][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:44:41,953][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:45:02,033][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:45:02,185][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:46:55,451][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:46:55,452][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:46:55,522][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:46:57,143][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:47:17,596][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:47:17,596][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:47:17,669][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:47:19,744][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:51:38,591][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:51:38,613][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:51:38,722][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:51:57,916][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:52:16,227][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:52:16,227][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:52:16,303][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:52:18,938][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:52:22,978][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:52:22,980][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:52:22,981][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:52:22,982][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:52:22,989][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:52:22,989][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:52:22,989][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:52:22,990][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:52:22,990][train][INFO] - Logging hyperparameters!
[2022-07-13 11:52:22,997][train][INFO] - Starting training!
[2022-07-13 11:52:22,998][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:52:23,009][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:52:23,009][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:52:23,010][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:52:45,049][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:52:45,284][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:53:32,468][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:53:32,469][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:53:32,558][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:53:34,494][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:53:37,850][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:53:37,852][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:53:37,853][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:53:37,855][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:53:37,861][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:53:37,861][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:53:37,861][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:53:37,862][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:53:37,880][train][INFO] - Logging hyperparameters!
[2022-07-13 11:53:37,886][train][INFO] - Starting training!
[2022-07-13 11:53:37,887][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:53:37,889][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:53:37,889][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:53:37,889][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:53:58,518][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:53:58,674][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:54:52,338][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:54:52,338][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:54:52,411][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:54:54,064][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:54:57,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:54:57,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:54:57,451][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:54:57,453][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:54:57,459][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:54:57,460][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:54:57,460][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:54:57,460][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:54:57,461][train][INFO] - Logging hyperparameters!
[2022-07-13 11:54:57,467][train][INFO] - Starting training!
[2022-07-13 11:54:57,468][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:54:57,469][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:54:57,470][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:54:57,470][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:55:18,046][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:55:18,203][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:55:23,351][train][INFO] - Starting testing!
[2022-07-13 11:55:23,873][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:55:24,258][train][INFO] - Finalizing!
[2022-07-13 11:56:09,154][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:56:09,155][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:56:09,228][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:56:10,886][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:56:14,264][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:56:14,266][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:56:14,267][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:56:14,269][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:56:14,275][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:56:14,275][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:56:14,275][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:56:14,276][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 11:56:14,276][train][INFO] - Logging hyperparameters!
[2022-07-13 11:56:14,281][train][INFO] - Starting training!
[2022-07-13 11:56:14,282][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:56:14,283][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:56:14,284][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:56:14,284][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:56:34,964][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:56:35,120][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 11:56:39,880][train][INFO] - Starting testing!
[2022-07-13 11:56:40,411][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:56:40,796][train][INFO] - Finalizing!
[2022-07-13 11:57:04,915][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 11:57:04,917][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 11:57:04,990][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 11:57:06,653][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 11:57:10,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 11:57:10,029][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 11:57:10,029][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 11:57:10,031][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 11:57:10,037][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 11:57:10,037][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 11:57:10,038][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 11:57:10,038][train][INFO] - Logging hyperparameters!
[2022-07-13 11:57:10,105][train][INFO] - Starting training!
[2022-07-13 11:57:10,107][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 11:57:10,108][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 11:57:10,108][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 11:57:10,108][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 11:57:30,689][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 11:57:30,867][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:03:04,830][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:03:04,847][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:03:04,917][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:03:08,628][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:03:11,833][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:03:11,835][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:03:11,836][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:03:11,838][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:03:11,843][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:03:11,843][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:03:11,844][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:03:11,844][train][INFO] - Logging hyperparameters!
[2022-07-13 12:03:11,868][train][INFO] - Starting training!
[2022-07-13 12:03:11,870][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 12:03:11,871][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:03:11,871][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 12:03:11,871][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:03:31,623][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 12:03:31,773][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:04:13,861][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:04:13,862][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:04:13,931][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:04:15,447][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:04:18,325][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:04:18,327][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:04:18,328][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:04:18,330][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:04:18,335][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:04:18,335][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:04:18,336][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:04:18,336][train][INFO] - Logging hyperparameters!
[2022-07-13 12:04:18,358][train][INFO] - Starting training!
[2022-07-13 12:04:18,361][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 12:04:18,361][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:04:18,362][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 12:04:18,362][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:04:38,119][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 12:04:38,270][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 12:06:48,898][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 12:06:48,909][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 12:06:49,018][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 12:07:09,129][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 12:07:27,511][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 12:07:27,607][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,608][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 12:07:27,690][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,691][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 12:07:27,757][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,758][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 12:07:27,809][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 12:07:27,832][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,833][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 12:07:27,871][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,872][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 12:07:27,874][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 12:07:27,875][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 12:07:27,876][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 12:07:27,877][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 12:07:27,881][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 12:07:27,882][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 12:07:27,882][train][INFO] - Logging hyperparameters!
[2022-07-13 12:07:28,135][train][INFO] - Starting training!
[2022-07-13 12:07:28,136][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 12:07:28,517][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 12:07:28,634][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 12:07:28,725][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 12:07:28,792][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 12:07:28,818][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 12:07:28,834][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 12:07:28,873][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 12:07:28,873][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,874][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 12:07:28,874][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,876][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,877][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,880][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,882][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:28,883][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:48,845][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,018][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 12:07:49,029][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 15:25:01,341][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 15:25:01,359][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 15:25:01,498][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 15:25:17,995][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 15:25:20,831][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 15:25:20,833][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 15:25:20,834][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 15:25:20,836][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 15:25:20,841][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 15:25:20,842][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 15:25:20,842][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 15:25:20,842][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 15:25:20,843][train][INFO] - Logging hyperparameters!
[2022-07-13 15:25:20,848][train][INFO] - Starting training!
[2022-07-13 15:25:20,849][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 15:25:20,850][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 15:25:20,850][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 15:25:20,850][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 15:25:41,453][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:25:41,601][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 24.1 M
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 15:32:26,126][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 15:32:26,126][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 15:32:26,196][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 15:32:27,787][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 15:32:30,954][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 15:32:30,956][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 15:32:30,957][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 15:32:30,959][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 15:32:30,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 15:32:30,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 15:32:30,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 15:32:30,983][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 15:32:30,983][train][INFO] - Logging hyperparameters!
[2022-07-13 15:32:30,988][train][INFO] - Starting training!
[2022-07-13 15:32:30,989][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 15:32:30,990][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 15:32:30,990][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 15:32:30,990][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 15:32:50,922][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:32:51,072][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 15:32:55,842][train][INFO] - Starting testing!
[2022-07-13 15:32:56,353][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 15:32:56,736][train][INFO] - Finalizing!
[2022-07-13 22:05:53,164][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,186][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,205][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,401][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,885][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,885][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,887][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 22:05:53,911][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:12:53,145][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:12:53,165][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:12:53,319][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:13:08,227][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:13:11,928][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:13:11,973][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:13:11,974][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:13:11,976][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:13:11,982][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:13:11,982][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:13:11,982][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:13:11,983][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:13:11,983][train][INFO] - Logging hyperparameters!
[2022-07-13 23:13:11,988][train][INFO] - Starting training!
[2022-07-13 23:13:11,989][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:13:11,990][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:13:11,990][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:13:11,991][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:13:33,164][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:13:33,323][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 2.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
60.2 M    Trainable params
222 K     Non-trainable params
60.4 M    Total params
241.588   Total estimated model params size (MB)
[2022-07-13 23:13:56,993][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:13:56,993][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:13:57,063][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:13:59,374][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:14:01,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:14:01,461][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:14:01,462][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:14:01,464][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:14:01,470][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:14:01,470][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:14:01,470][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:14:01,471][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:14:01,471][train][INFO] - Logging hyperparameters!
[2022-07-13 23:14:01,476][train][INFO] - Starting training!
[2022-07-13 23:14:01,477][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:14:01,478][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:14:01,478][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:14:01,478][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:14:21,914][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:14:22,060][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 655 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 23:14:48,161][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:14:48,162][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:14:48,233][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:14:50,225][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:14:52,299][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:14:52,301][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:14:52,302][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:14:52,304][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:14:52,310][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:14:52,310][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:14:52,310][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:14:52,311][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:14:52,311][train][INFO] - Logging hyperparameters!
[2022-07-13 23:14:52,316][train][INFO] - Starting training!
[2022-07-13 23:14:52,317][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:14:52,318][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:14:52,318][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:14:52,319][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:15:12,981][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:15:13,126][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 655 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.7 M    Trainable params
222 K     Non-trainable params
59.0 M    Total params
235.808   Total estimated model params size (MB)
[2022-07-13 23:16:33,973][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:16:33,973][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:16:34,043][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:16:35,958][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:16:37,969][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:16:37,970][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:16:37,971][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:16:37,973][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:16:37,979][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:16:37,979][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:16:37,979][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:16:37,980][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:16:37,980][train][INFO] - Logging hyperparameters!
[2022-07-13 23:16:37,985][train][INFO] - Starting training!
[2022-07-13 23:16:37,986][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:16:37,987][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:16:37,987][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:16:37,987][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:16:58,126][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:16:58,269][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 4.1 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.202   Total estimated model params size (MB)
[2022-07-13 23:17:05,087][train][INFO] - Starting testing!
[2022-07-13 23:17:05,596][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:17:05,959][train][INFO] - Finalizing!
[2022-07-13 23:17:59,699][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:17:59,699][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:17:59,770][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:18:01,416][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:18:03,478][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:18:03,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:18:03,481][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:18:03,483][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:18:03,488][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:18:03,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:18:03,512][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:18:03,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:18:03,513][train][INFO] - Logging hyperparameters!
[2022-07-13 23:18:03,518][train][INFO] - Starting training!
[2022-07-13 23:18:03,519][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:18:03,520][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:18:03,520][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:18:03,520][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:18:23,868][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:18:24,011][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 4.1 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.202   Total estimated model params size (MB)
[2022-07-13 23:18:48,980][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:18:48,981][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:18:49,051][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:18:50,647][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:18:52,654][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:18:52,656][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:18:52,657][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:18:52,659][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:18:52,664][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:18:52,665][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:18:52,665][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:18:52,666][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:18:52,666][train][INFO] - Logging hyperparameters!
[2022-07-13 23:18:52,671][train][INFO] - Starting training!
[2022-07-13 23:18:52,671][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:18:52,672][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:18:52,673][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:18:52,673][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:19:12,910][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:19:13,052][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:19:17,491][train][INFO] - Starting testing!
[2022-07-13 23:19:18,001][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:19:18,349][train][INFO] - Finalizing!
[2022-07-13 23:20:08,039][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:20:08,040][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:20:08,109][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:20:09,646][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:20:11,650][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:20:11,652][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:20:11,653][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:20:11,655][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:20:11,661][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:20:11,661][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:20:11,661][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:20:11,662][train][INFO] - Logging hyperparameters!
[2022-07-13 23:20:11,698][train][INFO] - Starting training!
[2022-07-13 23:20:11,700][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:20:11,701][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:20:11,701][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:20:11,702][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:20:31,847][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:20:32,002][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:21:15,496][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:21:15,497][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:21:15,567][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:21:17,123][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:21:19,119][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:21:19,121][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:21:19,122][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:21:19,123][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:21:19,129][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:21:19,129][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:21:19,129][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:21:19,130][train][INFO] - Logging hyperparameters!
[2022-07-13 23:21:19,152][train][INFO] - Starting training!
[2022-07-13 23:21:19,154][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:21:19,155][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:21:19,155][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:21:19,155][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:21:39,268][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:21:39,403][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:22:08,465][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:22:08,466][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:22:08,535][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:22:10,092][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:22:12,087][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:22:12,089][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:22:12,089][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:22:12,091][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:22:12,097][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:22:12,097][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:22:12,097][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:22:12,098][train][INFO] - Logging hyperparameters!
[2022-07-13 23:22:12,120][train][INFO] - Starting training!
[2022-07-13 23:22:12,122][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:22:12,123][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:22:12,123][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:22:12,124][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:22:32,370][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:22:32,507][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:23:46,047][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:23:46,047][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:23:46,117][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:23:47,650][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:23:49,695][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:23:49,697][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:23:49,698][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:23:49,699][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:23:49,705][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:23:49,705][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:23:49,705][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:23:49,706][train][INFO] - Logging hyperparameters!
[2022-07-13 23:23:49,728][train][INFO] - Starting training!
[2022-07-13 23:23:49,730][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:23:49,731][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:23:49,731][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:23:49,731][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:24:09,829][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:24:09,967][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:24:38,515][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:24:38,516][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:24:38,591][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:24:40,119][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:24:42,106][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:24:42,108][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:24:42,109][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:24:42,111][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:24:42,116][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:24:42,117][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:24:42,117][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:24:42,118][train][INFO] - Logging hyperparameters!
[2022-07-13 23:24:42,139][train][INFO] - Starting training!
[2022-07-13 23:24:42,141][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:24:42,142][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:24:42,142][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:24:42,143][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:25:02,226][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:25:02,362][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:28:06,286][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:28:06,311][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:28:06,415][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:28:24,973][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:28:39,698][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:28:39,737][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,738][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-13 23:28:39,746][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,747][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-13 23:28:39,750][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:28:39,750][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:28:39,752][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:28:39,756][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-13 23:28:39,756][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,757][train][INFO] - Logging hyperparameters!
[2022-07-13 23:28:39,772][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,773][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-13 23:28:39,783][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,784][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-13 23:28:39,790][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,791][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-13 23:28:39,791][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,792][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-13 23:28:39,797][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:28:39,798][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-13 23:28:39,902][train][INFO] - Starting training!
[2022-07-13 23:28:39,903][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-13 23:28:40,743][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-13 23:28:40,748][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-13 23:28:40,781][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-13 23:28:40,791][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-13 23:28:40,791][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-13 23:28:40,793][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-13 23:28:40,799][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-13 23:28:40,802][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:28:40,802][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,802][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:28:40,803][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,804][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,809][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,810][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,811][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,811][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-13 23:28:40,812][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,431][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-13 23:29:00,441][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:44:34,687][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,687][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,705][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:34,978][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:44:35,545][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-13 23:51:18,883][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:51:18,899][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:51:19,044][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:51:23,269][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:51:25,749][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:51:25,751][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:51:25,751][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:51:25,753][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:51:25,759][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:51:25,759][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:51:25,759][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:51:25,760][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:51:25,760][train][INFO] - Logging hyperparameters!
[2022-07-13 23:51:25,765][train][INFO] - Starting training!
[2022-07-13 23:51:25,766][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:51:25,767][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:51:25,767][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:51:25,767][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:51:45,984][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:51:46,127][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:52:12,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:52:12,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:52:12,971][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:52:14,498][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:52:16,489][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:52:16,491][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:52:16,492][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:52:16,493][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:52:16,499][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:52:16,499][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:52:16,500][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:52:16,500][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:52:16,533][train][INFO] - Logging hyperparameters!
[2022-07-13 23:52:16,538][train][INFO] - Starting training!
[2022-07-13 23:52:16,539][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:52:16,540][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:52:16,540][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:52:16,540][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:52:36,578][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:52:36,724][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-13 23:54:01,409][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-13 23:54:01,409][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-13 23:54:01,479][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-13 23:54:03,001][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-13 23:54:05,003][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-13 23:54:05,005][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-13 23:54:05,006][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-13 23:54:05,008][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-13 23:54:05,014][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-13 23:54:05,014][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-13 23:54:05,014][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-13 23:54:05,015][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-13 23:54:05,015][train][INFO] - Logging hyperparameters!
[2022-07-13 23:54:05,020][train][INFO] - Starting training!
[2022-07-13 23:54:05,021][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-13 23:54:05,022][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-13 23:54:05,022][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-13 23:54:05,022][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-13 23:54:25,095][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-13 23:54:25,256][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:04:46,473][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:04:46,491][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:04:46,625][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:05:05,882][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:05:08,678][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:05:08,680][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:05:08,681][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:05:08,682][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:05:08,688][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:05:08,688][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:05:08,689][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:05:08,689][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:05:08,689][train][INFO] - Logging hyperparameters!
[2022-07-14 00:05:08,694][train][INFO] - Starting training!
[2022-07-14 00:05:08,695][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:05:08,696][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:05:08,696][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:05:08,697][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:05:29,880][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:05:30,040][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:12:18,527][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:12:18,528][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:12:18,598][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:12:22,684][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:12:25,190][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:12:25,206][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:12:25,207][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:12:25,209][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:12:25,214][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:12:25,215][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:12:25,215][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:12:25,215][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:12:25,216][train][INFO] - Logging hyperparameters!
[2022-07-14 00:12:25,220][train][INFO] - Starting training!
[2022-07-14 00:12:25,221][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:12:25,222][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:12:25,222][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:12:25,223][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:12:45,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:12:45,644][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:13:20,400][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:13:20,400][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:13:20,471][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:13:23,429][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:13:25,468][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:13:25,470][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:13:25,471][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:13:25,473][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:13:25,479][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:13:25,479][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:13:25,479][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:13:25,480][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:13:25,480][train][INFO] - Logging hyperparameters!
[2022-07-14 00:13:25,485][train][INFO] - Starting training!
[2022-07-14 00:13:25,486][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:13:25,505][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:13:25,505][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:13:25,506][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:13:45,928][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:13:46,073][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:17:25,542][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:17:25,563][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:17:25,633][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:17:28,111][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:17:30,280][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:17:30,282][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:17:30,283][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:17:30,284][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:17:30,290][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:17:30,291][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:17:30,291][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:17:30,291][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:17:30,292][train][INFO] - Logging hyperparameters!
[2022-07-14 00:17:30,297][train][INFO] - Starting training!
[2022-07-14 00:17:30,298][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:17:30,299][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:17:30,299][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:17:30,299][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:17:50,611][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:17:50,756][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:21:25,991][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:21:26,013][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:21:26,082][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:21:28,805][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:21:31,166][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:21:31,168][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:21:31,169][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:21:31,171][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:21:31,177][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:21:31,177][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:21:31,177][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:21:31,178][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:21:31,178][train][INFO] - Logging hyperparameters!
[2022-07-14 00:21:31,183][train][INFO] - Starting training!
[2022-07-14 00:21:31,184][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:21:31,185][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:21:31,185][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:21:31,185][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:21:51,469][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:21:51,612][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:23:07,237][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:23:07,237][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:23:07,308][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:23:09,739][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:23:11,956][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:23:11,958][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:23:11,958][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:23:11,985][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:23:11,990][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:23:11,991][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:23:11,991][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:23:11,991][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:23:11,992][train][INFO] - Logging hyperparameters!
[2022-07-14 00:23:11,996][train][INFO] - Starting training!
[2022-07-14 00:23:11,997][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:23:11,998][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:23:11,998][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:23:11,999][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:23:32,298][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:23:32,440][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:25:59,829][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:25:59,853][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:25:59,925][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:26:02,594][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:26:04,766][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:26:04,768][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:26:04,769][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:26:04,770][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:26:04,776][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:26:04,776][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:26:04,776][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:26:04,777][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:26:04,777][train][INFO] - Logging hyperparameters!
[2022-07-14 00:26:04,782][train][INFO] - Starting training!
[2022-07-14 00:26:04,783][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:26:04,784][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:26:04,784][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:26:04,785][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:26:25,100][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:26:25,242][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:28:19,881][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:28:19,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:28:19,975][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:28:22,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:28:24,962][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:28:24,964][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:28:24,965][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:28:24,966][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:28:24,972][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:28:24,972][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:28:24,972][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:28:24,973][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:28:24,973][train][INFO] - Logging hyperparameters!
[2022-07-14 00:28:24,978][train][INFO] - Starting training!
[2022-07-14 00:28:24,979][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:28:24,980][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:28:24,980][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:28:24,980][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:28:45,223][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:28:45,367][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:29:43,647][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:29:43,647][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:29:43,718][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:29:46,342][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:29:48,474][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:29:48,476][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:29:48,477][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:29:48,479][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:29:48,484][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:29:48,485][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:29:48,485][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:29:48,485][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:29:48,486][train][INFO] - Logging hyperparameters!
[2022-07-14 00:29:48,490][train][INFO] - Starting training!
[2022-07-14 00:29:48,491][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:29:48,492][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:29:48,492][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:29:48,493][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:30:08,771][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:30:08,915][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:30:16,325][train][INFO] - Starting testing!
[2022-07-14 00:30:16,831][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:30:17,184][train][INFO] - Finalizing!
[2022-07-14 00:30:57,006][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:30:57,006][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:30:57,114][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:30:59,350][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:31:01,634][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:31:01,635][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:31:01,636][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:31:01,638][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:31:01,658][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:31:01,659][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:31:01,659][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:31:01,659][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:31:01,660][train][INFO] - Logging hyperparameters!
[2022-07-14 00:31:01,664][train][INFO] - Starting training!
[2022-07-14 00:31:01,665][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:31:01,666][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:31:01,667][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:31:01,667][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:31:21,791][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:31:21,936][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:31:27,227][train][INFO] - Starting testing!
[2022-07-14 00:31:27,740][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:31:28,267][train][INFO] - Finalizing!
[2022-07-14 00:31:53,736][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:31:53,736][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:31:53,805][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:31:56,021][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:31:58,049][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:31:58,051][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:31:58,052][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:31:58,054][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:31:58,059][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:31:58,060][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:31:58,060][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:31:58,060][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-14 00:31:58,061][train][INFO] - Logging hyperparameters!
[2022-07-14 00:31:58,065][train][INFO] - Starting training!
[2022-07-14 00:31:58,066][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:31:58,067][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:31:58,068][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:31:58,068][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:32:18,304][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:32:18,448][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:32:23,737][train][INFO] - Starting testing!
[2022-07-14 00:32:24,242][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:32:24,592][train][INFO] - Finalizing!
[2022-07-14 00:33:08,272][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:33:08,273][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:33:08,343][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:33:10,750][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:33:13,169][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:33:13,171][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:33:13,172][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:33:13,173][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:33:13,179][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:33:13,179][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:33:13,180][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:33:13,180][train][INFO] - Logging hyperparameters!
[2022-07-14 00:33:13,227][train][INFO] - Starting training!
[2022-07-14 00:33:13,229][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:33:13,230][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:33:13,230][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:33:13,231][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:33:33,553][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:33:33,691][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:34:57,264][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:34:57,279][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:34:57,348][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:35:00,083][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:35:02,448][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:35:02,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:35:02,450][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:35:02,452][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:35:02,458][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:35:02,458][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:35:02,458][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:35:02,459][train][INFO] - Logging hyperparameters!
[2022-07-14 00:35:02,481][train][INFO] - Starting training!
[2022-07-14 00:35:02,483][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:35:02,484][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:35:02,484][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:35:02,484][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:35:22,597][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:35:22,736][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:44:10,018][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:44:10,041][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:44:10,164][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:44:28,323][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:44:42,466][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:44:42,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:44:42,481][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:44:42,482][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:44:42,486][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:44:42,486][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:44:42,494][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 00:44:42,494][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,495][train][INFO] - Logging hyperparameters!
[2022-07-14 00:44:42,557][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,558][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 00:44:42,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 00:44:42,730][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,731][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 00:44:42,733][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,734][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 00:44:42,754][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,755][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 00:44:42,818][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,819][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 00:44:42,834][train][INFO] - Starting training!
[2022-07-14 00:44:42,835][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 00:44:42,866][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:44:42,867][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 00:44:42,868][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 00:44:43,564][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 00:44:43,585][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 00:44:43,739][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 00:44:43,743][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 00:44:43,760][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 00:44:43,819][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,820][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:44:43,820][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,821][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,824][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,826][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,826][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,827][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 00:44:43,830][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,442][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,592][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 00:45:04,602][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 00:56:48,818][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 00:56:48,839][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 00:56:48,910][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 00:56:52,570][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 00:56:55,051][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 00:56:55,053][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 00:56:55,054][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 00:56:55,056][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 00:56:55,061][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 00:56:55,062][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 00:56:55,062][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 00:56:55,063][train][INFO] - Logging hyperparameters!
[2022-07-14 00:56:55,142][train][INFO] - Starting training!
[2022-07-14 00:56:55,144][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 00:56:55,145][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 00:56:55,145][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 00:56:55,145][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 00:57:15,488][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 00:57:15,623][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 400   
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.248   Total estimated model params size (MB)
[2022-07-14 01:02:07,457][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,460][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,471][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,473][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,937][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:07,974][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:02:08,061][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:06:29,765][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 01:06:29,766][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 01:06:29,866][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 01:06:49,468][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 01:06:52,007][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 01:06:52,009][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 01:06:52,009][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 01:06:52,011][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 01:06:52,015][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 01:06:52,015][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,016][train][INFO] - Logging hyperparameters!
[2022-07-14 01:06:52,019][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,020][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 01:06:52,020][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,021][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 01:06:52,023][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,023][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,024][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 01:06:52,024][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 01:06:52,025][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,026][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 01:06:52,027][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,028][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 01:06:52,031][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:06:52,032][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 01:06:52,116][train][INFO] - Starting training!
[2022-07-14 01:06:52,117][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 01:06:53,021][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 01:06:53,022][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 01:06:53,025][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 01:06:53,025][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 01:06:53,027][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 01:06:53,028][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 01:06:53,033][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 01:06:53,036][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 01:06:53,036][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,036][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 01:06:53,037][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,039][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,041][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,042][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,043][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,045][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 01:06:53,046][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:07:12,679][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 01:10:34,726][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,726][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,727][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,727][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,731][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,732][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,753][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:10:34,754][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-14 01:13:00,165][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 01:13:00,166][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 01:13:00,275][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 01:13:16,558][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 01:13:19,187][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 01:13:19,199][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,201][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,202][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,200][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,200][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 01:13:19,206][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 01:13:19,206][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 01:13:19,207][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 01:13:19,208][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 01:13:19,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 01:13:19,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 01:13:19,212][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 01:13:19,212][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 01:13:19,213][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 01:13:19,213][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 01:13:19,213][train][INFO] - Logging hyperparameters!
[2022-07-14 01:13:19,349][train][INFO] - Starting training!
[2022-07-14 01:13:19,350][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 01:13:20,207][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 01:13:20,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 01:13:20,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 01:13:20,218][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 01:13:20,218][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,219][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 01:13:20,221][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,221][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:20,227][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,174][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 01:13:39,184][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 11:17:09,003][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 11:17:09,024][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 11:17:09,140][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 11:17:20,363][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 11:17:23,516][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 11:17:23,520][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 11:17:23,521][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 11:17:23,522][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 11:17:23,528][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 11:17:23,528][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 11:17:23,528][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:17:23,529][train][INFO] - Logging hyperparameters!
[2022-07-14 11:17:23,731][train][INFO] - Starting training!
[2022-07-14 11:17:23,733][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 11:17:23,734][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 11:17:23,734][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 11:17:23,734][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 11:17:44,257][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 11:17:44,414][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 11:29:48,759][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 11:29:48,773][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 11:29:48,874][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 11:30:07,044][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 11:30:21,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,581][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 11:30:21,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 11:30:21,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 11:30:21,613][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 11:30:21,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 11:30:21,615][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 11:30:21,616][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 11:30:21,616][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,620][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 11:30:21,629][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 11:30:21,629][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 11:30:21,629][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,630][train][INFO] - Logging hyperparameters!
[2022-07-14 11:30:21,630][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 11:30:21,654][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,655][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 11:30:21,662][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,663][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 11:30:21,667][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 11:30:21,668][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 11:30:21,786][train][INFO] - Starting training!
[2022-07-14 11:30:21,787][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 11:30:22,583][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 11:30:22,583][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 11:30:22,591][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 11:30:22,638][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 11:30:22,659][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 11:30:22,664][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 11:30:22,669][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 11:30:22,676][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 11:30:22,676][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,676][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 11:30:22,679][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,679][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,680][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,682][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:22,684][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,429][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 11:30:42,439][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 12:13:01,833][train][INFO] - Starting testing!
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:02,444][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 12:13:03,011][train][INFO] - Finalizing!
[2022-07-14 12:13:03,012][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010.ckpt
[2022-07-14 13:34:51,128][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 13:34:51,153][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 13:34:51,292][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 13:35:10,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 13:35:13,560][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 13:35:13,565][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 13:35:13,566][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 13:35:13,568][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 13:35:13,577][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 13:35:13,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 13:35:13,578][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 13:35:13,578][train][INFO] - Logging hyperparameters!
[2022-07-14 13:35:13,670][train][INFO] - Starting training!
[2022-07-14 13:35:13,672][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 13:35:13,673][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 13:35:13,673][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 13:35:13,674][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 13:35:35,289][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 13:35:35,515][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 13:36:57,994][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 13:36:58,009][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 13:36:58,080][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 13:37:01,334][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 13:37:03,760][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 13:37:03,762][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 13:37:03,763][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 13:37:03,764][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 13:37:03,770][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 13:37:03,770][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 13:37:03,771][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 13:37:03,771][train][INFO] - Logging hyperparameters!
[2022-07-14 13:37:03,796][train][INFO] - Starting training!
[2022-07-14 13:37:03,798][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-14 13:37:03,799][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 13:37:03,799][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-14 13:37:03,799][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 13:37:24,161][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-14 13:37:24,297][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 16:45:07,304][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 16:45:07,324][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 16:45:07,445][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 16:45:25,825][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 16:45:28,317][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 16:45:28,319][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 16:45:28,320][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 16:45:28,321][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 16:45:28,324][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,325][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 16:45:28,325][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 16:45:28,325][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,326][train][INFO] - Logging hyperparameters!
[2022-07-14 16:45:28,329][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,330][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 16:45:28,330][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,331][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,331][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 16:45:28,332][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 16:45:28,334][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,335][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 16:45:28,335][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,336][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 16:45:28,348][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 16:45:28,349][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 16:45:28,452][train][INFO] - Starting training!
[2022-07-14 16:45:28,453][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 16:45:29,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 16:45:29,350][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 16:45:29,352][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 16:45:29,352][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,352][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,353][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:29,360][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,559][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,706][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 16:45:48,716][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-14 20:30:38,993][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-14 20:30:39,017][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-14 20:30:39,125][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-14 20:30:57,192][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-14 20:30:59,668][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-14 20:30:59,671][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-14 20:30:59,672][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-14 20:30:59,673][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-14 20:30:59,677][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-14 20:30:59,677][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-14 20:30:59,678][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-14 20:30:59,678][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,678][train][INFO] - Logging hyperparameters!
[2022-07-14 20:30:59,680][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,681][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,681][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-14 20:30:59,682][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-14 20:30:59,682][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,682][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,683][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-14 20:30:59,683][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,683][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-14 20:30:59,684][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-14 20:30:59,686][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,687][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-14 20:30:59,692][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-14 20:30:59,693][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-14 20:30:59,812][train][INFO] - Starting training!
[2022-07-14 20:30:59,813][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-14 20:31:00,682][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-14 20:31:00,683][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-14 20:31:00,684][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-14 20:31:00,684][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-14 20:31:00,685][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-14 20:31:00,688][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-14 20:31:00,694][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-14 20:31:00,702][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-14 20:31:00,702][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,702][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-14 20:31:00,703][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,703][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,704][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,705][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:00,709][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,179][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-14 20:31:20,190][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:23:56,930][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:23:56,968][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:23:57,117][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:24:09,301][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:24:12,175][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:24:12,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:24:12,179][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:24:12,180][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:24:12,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:24:12,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:24:12,187][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:24:12,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-15 01:24:12,187][train][INFO] - Logging hyperparameters!
[2022-07-15 01:24:12,193][train][INFO] - Starting training!
[2022-07-15 01:24:12,194][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 01:24:12,195][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:24:12,195][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 01:24:12,195][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:24:33,221][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:24:33,401][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:25:02,564][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:25:02,565][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:25:02,635][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:25:04,325][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:25:06,509][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:25:06,511][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:25:06,512][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:25:06,513][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:25:06,519][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:25:06,519][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:25:06,520][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:25:06,520][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-15 01:25:06,520][train][INFO] - Logging hyperparameters!
[2022-07-15 01:25:06,525][train][INFO] - Starting training!
[2022-07-15 01:25:06,526][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 01:25:06,527][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:25:06,527][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 01:25:06,527][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:25:26,715][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:25:26,858][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:25:32,458][train][INFO] - Starting testing!
[2022-07-15 01:25:32,965][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 01:25:33,316][train][INFO] - Finalizing!
[2022-07-15 01:27:49,080][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:27:49,095][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:27:49,210][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:28:07,017][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:28:20,690][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,700][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:28:20,787][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,791][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:28:20,808][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,809][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:28:20,854][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:20,855][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:28:21,130][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:28:21,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:28:21,132][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:28:21,134][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:28:21,138][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:28:21,138][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,139][train][INFO] - Logging hyperparameters!
[2022-07-15 01:28:21,156][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,157][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:28:21,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,210][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:28:21,222][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:28:21,223][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:28:21,627][train][INFO] - Starting training!
[2022-07-15 01:28:21,628][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:28:21,719][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:28:21,803][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:28:21,812][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:28:21,856][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:28:22,170][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:28:22,211][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:28:22,224][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,225][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,226][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:28:22,227][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,234][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,229][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,231][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,233][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:22,232][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:41,889][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,059][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:28:42,070][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 01:38:09,589][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,606][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,613][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,636][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,652][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,661][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,668][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:38:09,937][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:41:54,786][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:41:54,787][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:41:54,887][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:42:14,555][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:42:17,495][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:42:17,496][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:42:17,497][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:42:17,498][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:42:17,503][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,503][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:42:17,503][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,504][train][INFO] - Logging hyperparameters!
[2022-07-15 01:42:17,504][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:42:17,506][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,507][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:42:17,508][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,509][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:42:17,510][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,511][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:42:17,515][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,515][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:42:17,516][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:42:17,517][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:42:17,517][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:42:17,680][train][INFO] - Starting training!
[2022-07-15 01:42:17,681][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:42:18,505][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:42:18,508][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:42:18,510][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:42:18,512][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:42:18,516][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:42:18,519][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:42:18,521][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,528][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,517][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:42:18,538][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,525][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,538][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,522][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,518][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:42:18,539][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:18,539][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:42:18,539][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:37,945][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,095][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:42:38,105][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-15 01:43:24,862][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,862][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,863][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,865][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,867][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,877][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,883][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:43:24,927][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 01:45:33,594][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 01:45:33,599][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 01:45:33,723][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 01:45:48,346][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 01:45:50,557][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 01:45:50,579][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 01:45:50,579][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 01:45:50,581][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 01:45:50,585][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 01:45:50,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,585][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][train][INFO] - Logging hyperparameters!
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 01:45:50,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 01:45:50,586][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 01:45:50,587][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 01:45:50,701][train][INFO] - Starting training!
[2022-07-15 01:45:50,702][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 01:45:51,587][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 01:45:51,588][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 01:45:51,588][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 01:45:51,590][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 01:45:51,591][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 01:45:51,591][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,591][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,598][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 01:45:51,600][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,568][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,718][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 01:46:10,728][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | query_embed | Embedding        | 1.2 K 
4 | input_proj  | ModuleList       | 525 K 
5 | class_embed | ModuleList       | 140 K 
6 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.186   Total estimated model params size (MB)
[2022-07-15 02:28:57,916][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,924][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,930][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,958][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,968][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,976][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,979][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:28:57,998][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 02:53:23,541][train][INFO] - Starting testing!
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:26,927][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 02:53:27,400][train][INFO] - Finalizing!
[2022-07-15 02:53:27,401][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v1.ckpt
[2022-07-15 06:28:21,751][train][INFO] - Starting testing!
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:22,718][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 06:28:23,253][train][INFO] - Finalizing!
[2022-07-15 06:28:23,253][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v2.ckpt
[2022-07-15 11:46:03,871][train][INFO] - Starting testing!
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:07,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 11:46:08,457][train][INFO] - Finalizing!
[2022-07-15 11:46:08,457][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v3.ckpt
[2022-07-15 12:35:15,132][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:35:15,152][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:35:15,267][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:35:34,054][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:35:37,098][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:35:37,101][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:35:37,102][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:35:37,103][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:35:37,109][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:35:37,109][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:35:37,109][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:35:37,110][train][INFO] - Logging hyperparameters!
[2022-07-15 12:35:37,155][train][INFO] - Starting training!
[2022-07-15 12:35:37,157][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:35:37,158][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:35:37,159][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:35:37,159][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:35:57,857][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:35:58,082][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:38:12,006][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:38:12,028][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:38:12,097][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:38:14,816][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:38:17,145][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:38:17,147][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:38:17,148][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:38:17,149][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:38:17,155][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:38:17,155][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:38:17,156][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:38:17,156][train][INFO] - Logging hyperparameters!
[2022-07-15 12:38:17,193][train][INFO] - Starting training!
[2022-07-15 12:38:17,195][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:38:17,196][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:38:17,197][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:38:17,197][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:38:37,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:38:37,592][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:41:39,725][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:41:39,725][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:41:39,794][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:41:41,810][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:41:43,871][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:41:43,872][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:41:43,873][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:41:43,875][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:41:43,880][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:41:43,881][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:41:43,903][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:41:43,903][train][INFO] - Logging hyperparameters!
[2022-07-15 12:41:43,925][train][INFO] - Starting training!
[2022-07-15 12:41:43,927][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 12:41:43,928][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:41:43,928][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 12:41:43,928][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:42:04,185][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 12:42:04,321][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:44:16,156][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 12:44:16,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 12:44:16,287][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 12:44:35,877][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 12:44:49,096][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 12:44:49,111][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 12:44:49,112][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 12:44:49,113][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 12:44:49,118][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 12:44:49,118][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,119][train][INFO] - Logging hyperparameters!
[2022-07-15 12:44:49,374][train][INFO] - Starting training!
[2022-07-15 12:44:49,375][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 12:44:49,615][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,616][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 12:44:49,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 12:44:49,747][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,749][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 12:44:49,750][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 12:44:49,973][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:49,974][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 12:44:49,975][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 12:44:50,202][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,203][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 12:44:50,206][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 12:44:50,211][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,212][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 12:44:50,212][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 12:44:50,306][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,307][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 12:44:50,310][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 12:44:50,332][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 12:44:50,334][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 12:44:50,334][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 12:44:50,339][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 12:44:50,339][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,339][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 12:44:50,340][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,340][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,341][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,345][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 12:44:50,348][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,405][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,560][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 12:45:10,571][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 12:54:01,758][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,759][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,773][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,777][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,788][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,800][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:01,834][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 12:54:02,009][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 13:02:32,787][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:02:32,805][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:02:32,912][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:02:38,173][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:03:13,632][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:03:13,632][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:03:13,702][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:03:15,221][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:03:17,197][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 13:03:17,199][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 13:03:17,200][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 13:03:17,201][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 13:03:17,207][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 13:03:17,207][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 13:03:17,208][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:03:17,209][train][INFO] - Logging hyperparameters!
[2022-07-15 13:03:17,308][train][INFO] - Starting training!
[2022-07-15 13:03:17,310][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-15 13:03:17,311][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 13:03:17,311][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-15 13:03:17,311][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 13:03:37,325][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-15 13:03:37,463][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 13:07:07,320][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 13:07:07,345][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 13:07:07,443][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 13:07:24,680][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 13:07:27,236][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 13:07:27,240][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,240][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 13:07:27,243][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,244][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 13:07:27,244][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,244][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 13:07:27,245][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 13:07:27,245][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 13:07:27,245][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,246][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 13:07:27,246][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 13:07:27,247][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 13:07:27,251][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 13:07:27,251][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 13:07:27,252][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 13:07:27,252][train][INFO] - Logging hyperparameters!
[2022-07-15 13:07:27,253][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 13:07:27,253][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 13:07:27,374][train][INFO] - Starting training!
[2022-07-15 13:07:27,375][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 13:07:28,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 13:07:28,241][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 13:07:28,245][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 13:07:28,245][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 13:07:28,247][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 13:07:28,253][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 13:07:28,254][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,264][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 13:07:28,264][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,266][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,266][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,267][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,269][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:28,272][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,729][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 13:07:47,740][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 14:13:08,786][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-15 14:13:08,801][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-15 14:13:08,889][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-15 14:13:27,457][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-15 14:13:30,064][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-15 14:13:30,065][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-15 14:13:30,066][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-15 14:13:30,067][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-15 14:13:30,070][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,071][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8
[2022-07-15 14:13:30,071][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-15 14:13:30,071][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-15 14:13:30,072][pytorch_lightning.utilities.distributed][INFO] - Multi-processing is handled by Slurm.
[2022-07-15 14:13:30,072][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,072][train][INFO] - Logging hyperparameters!
[2022-07-15 14:13:30,073][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,074][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,080][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,081][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8
[2022-07-15 14:13:30,081][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8
[2022-07-15 14:13:30,081][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,082][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8
[2022-07-15 14:13:30,082][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8
[2022-07-15 14:13:30,083][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-15 14:13:30,084][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8
[2022-07-15 14:13:30,188][train][INFO] - Starting training!
[2022-07-15 14:13:30,189][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8
[2022-07-15 14:13:31,072][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3
[2022-07-15 14:13:31,075][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2
[2022-07-15 14:13:31,082][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6
[2022-07-15 14:13:31,082][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5
[2022-07-15 14:13:31,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7
[2022-07-15 14:13:31,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1
[2022-07-15 14:13:31,085][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4
[2022-07-15 14:13:31,088][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-15 14:13:31,088][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,088][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 8 processes
----------------------------------------------------------------------------------------------------

[2022-07-15 14:13:31,092][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,092][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,093][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,095][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:31,095][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for 8 nodes.
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,327][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,478][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - Set SLURM handle signals.
[2022-07-15 14:13:50,488][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-15 17:14:42,511][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,515][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,516][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,527][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,536][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,536][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,540][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-15 17:14:42,543][pytorch_lightning.trainer.connectors.slurm_connector][INFO] - bypassing sigterm
[2022-07-16 00:50:42,653][train][INFO] - Starting testing!
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:43,651][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
[2022-07-16 00:50:44,116][train][INFO] - Finalizing!
[2022-07-16 00:50:44,117][train][INFO] - Best model ckpt at /gpfsdswork/projects/rech/way/uex85wx/HydraRSYNC/checkpoints/epoch_010-v4.ckpt
[2022-07-16 01:46:46,124][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:46:46,132][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:46:46,282][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:47:01,455][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:47:01,456][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:47:01,526][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:47:13,107][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:47:15,944][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:47:15,946][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:47:15,947][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:47:15,949][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:47:15,955][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:47:15,955][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:47:15,955][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:47:15,956][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:47:15,956][train][INFO] - Logging hyperparameters!
[2022-07-16 01:47:15,961][train][INFO] - Starting training!
[2022-07-16 01:47:15,961][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:47:15,962][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:47:15,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:47:15,963][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:47:37,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:47:37,580][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:48:35,560][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:48:35,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:48:35,646][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:48:37,173][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:48:39,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:48:39,183][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:48:39,184][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:48:39,185][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:48:39,214][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:48:39,215][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:48:39,215][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:48:39,215][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:48:39,216][train][INFO] - Logging hyperparameters!
[2022-07-16 01:48:39,220][train][INFO] - Starting training!
[2022-07-16 01:48:39,221][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:48:39,222][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:48:39,222][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:48:39,223][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:48:59,376][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:48:59,518][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:49:19,293][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:49:19,293][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:49:19,378][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:49:20,908][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 01:49:22,923][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 01:49:22,925][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 01:49:22,926][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 01:49:22,928][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 01:49:22,933][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 01:49:22,934][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 01:49:22,934][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 01:49:22,935][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 01:49:22,935][train][INFO] - Logging hyperparameters!
[2022-07-16 01:49:22,939][train][INFO] - Starting training!
[2022-07-16 01:49:22,940][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 01:49:22,941][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 01:49:22,942][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 01:49:22,942][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 01:49:43,058][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:49:43,200][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | Cropper          | 16.4 K
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.1 M    Trainable params
222 K     Non-trainable params
58.3 M    Total params
233.251   Total estimated model params size (MB)
[2022-07-16 01:49:51,002][train][INFO] - Starting testing!
[2022-07-16 01:49:51,510][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 01:49:51,858][train][INFO] - Finalizing!
[2022-07-16 01:59:47,919][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 01:59:47,919][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 01:59:47,991][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 01:59:49,524][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:00:25,511][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:00:25,511][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:00:25,581][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:00:27,111][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:01:03,177][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:01:03,177][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:01:03,249][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:01:04,777][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:01:06,827][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:01:06,829][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:01:06,829][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:01:06,831][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:01:06,837][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:01:06,837][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:01:06,837][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:01:06,838][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:01:06,838][train][INFO] - Logging hyperparameters!
[2022-07-16 02:01:06,843][train][INFO] - Starting training!
[2022-07-16 02:01:06,844][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:01:06,845][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:01:06,845][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:01:06,845][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:01:27,013][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:01:27,156][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:02:07,174][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:02:07,175][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:02:07,248][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:02:08,772][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:02:10,795][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:02:10,797][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:02:10,797][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:02:10,799][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:02:10,805][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:02:10,805][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:02:10,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:02:10,806][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:02:10,806][train][INFO] - Logging hyperparameters!
[2022-07-16 02:02:10,811][train][INFO] - Starting training!
[2022-07-16 02:02:10,812][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:02:10,813][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:02:10,813][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:02:10,813][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:02:30,947][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:02:31,090][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:02:36,358][train][INFO] - Starting testing!
[2022-07-16 02:02:36,863][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:02:37,214][train][INFO] - Finalizing!
[2022-07-16 02:05:36,244][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:05:36,245][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:05:36,314][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:05:37,843][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:05:39,867][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:05:39,869][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:05:39,870][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:05:39,872][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:05:39,877][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:05:39,878][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:05:39,878][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:05:39,878][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:05:39,879][train][INFO] - Logging hyperparameters!
[2022-07-16 02:05:39,884][train][INFO] - Starting training!
[2022-07-16 02:05:39,885][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:05:39,886][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:05:39,886][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:05:39,886][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:06:00,113][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:06:00,257][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:09:16,933][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:09:16,933][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:09:17,003][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:09:18,522][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:09:20,531][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:09:20,533][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:09:20,533][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:09:20,535][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:09:20,541][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:09:20,541][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:09:20,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:09:20,564][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:09:20,564][train][INFO] - Logging hyperparameters!
[2022-07-16 02:09:20,569][train][INFO] - Starting training!
[2022-07-16 02:09:20,570][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:09:20,571][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:09:20,571][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:09:20,572][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:09:40,680][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:09:40,823][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:11:23,439][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:11:23,440][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:11:23,514][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:11:25,041][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:11:27,074][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:11:27,076][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:11:27,077][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:11:27,078][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:11:27,084][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:11:27,085][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:11:27,085][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:11:27,085][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:11:27,086][train][INFO] - Logging hyperparameters!
[2022-07-16 02:11:27,090][train][INFO] - Starting training!
[2022-07-16 02:11:27,091][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:11:27,092][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:11:27,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:11:27,093][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:11:47,298][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:11:47,442][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:13:32,496][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:13:32,496][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:13:32,567][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:13:34,095][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:13:36,114][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:13:36,116][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:13:36,117][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:13:36,119][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:13:36,125][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:13:36,125][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:13:36,125][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:13:36,126][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:13:36,126][train][INFO] - Logging hyperparameters!
[2022-07-16 02:13:36,131][train][INFO] - Starting training!
[2022-07-16 02:13:36,131][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:13:36,132][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:13:36,133][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:13:36,133][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:13:56,129][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:13:56,272][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:18:40,717][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:18:40,717][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:18:40,811][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:18:42,359][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:18:44,387][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:18:44,389][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:18:44,389][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:18:44,391][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:18:44,397][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:18:44,397][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:18:44,397][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:18:44,398][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:18:44,398][train][INFO] - Logging hyperparameters!
[2022-07-16 02:18:44,403][train][INFO] - Starting training!
[2022-07-16 02:18:44,404][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:18:44,405][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:18:44,405][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:18:44,405][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:19:04,576][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:19:04,720][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:20:36,322][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:20:36,323][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:20:36,394][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:20:37,923][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:20:39,948][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:20:39,950][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:20:39,951][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:20:39,953][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:20:39,959][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:20:39,959][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:20:39,959][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:20:39,960][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:20:39,982][train][INFO] - Logging hyperparameters!
[2022-07-16 02:20:39,987][train][INFO] - Starting training!
[2022-07-16 02:20:39,988][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:20:39,989][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:20:39,989][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:20:39,989][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:21:00,104][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:21:00,248][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:21:58,871][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:21:58,872][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:21:58,942][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:00,468][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:02,525][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:22:02,527][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:22:02,527][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:22:02,529][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:22:02,535][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:22:02,535][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:22:02,535][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:22:02,536][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:22:02,536][train][INFO] - Logging hyperparameters!
[2022-07-16 02:22:02,541][train][INFO] - Starting training!
[2022-07-16 02:22:02,542][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:22:02,543][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:22:02,543][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:22:02,543][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:22:10,517][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:22:10,517][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:22:10,587][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:12,125][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:48,490][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:22:48,490][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:22:48,562][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:22:50,094][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:22:52,152][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:22:52,154][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:22:52,155][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:22:52,156][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:22:52,162][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:22:52,162][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:22:52,162][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:22:52,163][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:22:52,163][train][INFO] - Logging hyperparameters!
[2022-07-16 02:22:52,168][train][INFO] - Starting training!
[2022-07-16 02:22:52,169][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:22:52,170][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:22:52,170][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:22:52,170][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:23:12,401][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:23:12,545][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 02:23:37,717][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 02:23:37,717][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 02:23:37,787][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 02:23:39,315][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 02:23:41,328][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 02:23:41,330][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 02:23:41,331][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 02:23:41,332][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 02:23:41,338][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 02:23:41,338][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 02:23:41,339][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 02:23:41,340][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 02:23:41,340][train][INFO] - Logging hyperparameters!
[2022-07-16 02:23:41,345][train][INFO] - Starting training!
[2022-07-16 02:23:41,346][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 02:23:41,347][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 02:23:41,347][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 02:23:41,348][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 02:24:01,542][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 02:24:01,686][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 12:52:49,415][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 12:52:49,437][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 12:52:49,544][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 12:52:56,765][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 12:52:59,500][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 12:52:59,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 12:52:59,505][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 12:52:59,506][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 12:52:59,512][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 12:52:59,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 12:52:59,513][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 12:52:59,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 12:52:59,513][train][INFO] - Logging hyperparameters!
[2022-07-16 12:52:59,518][train][INFO] - Starting training!
[2022-07-16 12:52:59,519][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 12:52:59,520][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 12:52:59,520][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 12:52:59,521][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 12:53:20,225][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 12:53:20,447][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 1.1 M 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
59.1 M    Trainable params
222 K     Non-trainable params
59.4 M    Total params
237.446   Total estimated model params size (MB)
[2022-07-16 13:14:09,429][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 13:14:09,448][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 13:14:09,517][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 13:14:13,200][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 13:14:16,691][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 13:14:16,693][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 13:14:16,693][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 13:14:16,695][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 13:14:16,701][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 13:14:16,701][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 13:14:16,701][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 13:14:16,709][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 13:14:16,709][train][INFO] - Logging hyperparameters!
[2022-07-16 13:14:16,714][train][INFO] - Starting training!
[2022-07-16 13:14:16,715][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 13:14:16,716][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 13:14:16,716][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 13:14:16,716][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 13:14:36,793][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 13:14:36,930][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-16 13:15:00,639][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-16 13:15:00,639][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-16 13:15:00,710][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-16 13:15:02,687][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-16 13:15:05,002][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-16 13:15:05,004][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-16 13:15:05,005][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-16 13:15:05,007][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-16 13:15:05,013][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-16 13:15:05,013][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-16 13:15:05,013][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-16 13:15:05,021][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-16 13:15:05,021][train][INFO] - Logging hyperparameters!
[2022-07-16 13:15:05,026][train][INFO] - Starting training!
[2022-07-16 13:15:05,027][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-16 13:15:05,028][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-16 13:15:05,028][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-16 13:15:05,029][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-16 13:15:24,954][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-16 13:15:25,090][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:34:13,305][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:34:13,328][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:34:13,486][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:34:29,567][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:34:34,124][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:34:34,126][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:34:34,127][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:34:34,129][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:34:34,135][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:34:34,135][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:34:34,135][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:34:34,144][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:34:34,145][train][INFO] - Logging hyperparameters!
[2022-07-17 00:34:34,150][train][INFO] - Starting training!
[2022-07-17 00:34:34,151][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:34:34,152][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:34:34,153][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:34:34,153][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:34:55,580][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:34:55,759][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:35:17,834][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:35:17,834][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:35:17,906][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:35:19,557][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:35:22,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:35:22,085][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:35:22,086][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:35:22,088][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:35:22,094][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:35:22,094][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:35:22,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:35:22,104][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:35:22,104][train][INFO] - Logging hyperparameters!
[2022-07-17 00:35:22,109][train][INFO] - Starting training!
[2022-07-17 00:35:22,110][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:35:22,111][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:35:22,112][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:35:22,112][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:35:42,425][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:35:42,564][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:39:40,582][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:39:40,583][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:39:40,654][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:39:42,298][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:39:44,815][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:39:44,817][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:39:44,818][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:39:44,819][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:39:44,825][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:39:44,825][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:39:44,826][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:39:44,835][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:39:44,835][train][INFO] - Logging hyperparameters!
[2022-07-17 00:39:44,840][train][INFO] - Starting training!
[2022-07-17 00:39:44,841][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:39:44,842][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:39:44,843][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:39:44,843][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:40:05,191][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:40:05,331][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:41:11,194][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:41:11,195][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:41:11,267][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:41:12,916][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:41:15,451][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:41:15,453][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:41:15,454][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:41:15,456][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:41:15,462][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:41:15,462][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:41:15,462][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:41:15,471][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:41:15,472][train][INFO] - Logging hyperparameters!
[2022-07-17 00:41:15,477][train][INFO] - Starting training!
[2022-07-17 00:41:15,478][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:41:15,479][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:41:15,479][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:41:15,479][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:41:36,030][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:41:36,171][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:41:51,591][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:41:51,591][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:41:51,665][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:41:53,294][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:41:55,812][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:41:55,814][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:41:55,814][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:41:55,816][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:41:55,822][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:41:55,822][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:41:55,822][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:41:55,832][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:41:55,832][train][INFO] - Logging hyperparameters!
[2022-07-17 00:41:55,837][train][INFO] - Starting training!
[2022-07-17 00:41:55,838][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:41:55,844][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:41:55,844][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:41:55,844][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:42:16,416][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:42:16,556][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:47:39,932][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:47:39,932][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:47:40,004][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:47:41,651][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:47:44,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:47:44,180][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:47:44,180][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:47:44,182][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:47:44,188][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:47:44,188][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:47:44,189][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:47:44,198][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:47:44,198][train][INFO] - Logging hyperparameters!
[2022-07-17 00:47:44,203][train][INFO] - Starting training!
[2022-07-17 00:47:44,204][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:47:44,205][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:47:44,206][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:47:44,206][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:48:04,722][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:04,863][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:48:11,328][train][INFO] - Starting testing!
[2022-07-17 00:48:14,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:14,392][train][INFO] - Finalizing!
[2022-07-17 00:48:33,539][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:48:33,540][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:48:33,611][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:48:35,251][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:48:37,868][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:48:37,870][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:48:37,871][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:48:37,873][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:48:37,879][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:48:37,879][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:48:37,879][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:48:37,889][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:48:37,889][train][INFO] - Logging hyperparameters!
[2022-07-17 00:48:37,894][train][INFO] - Starting training!
[2022-07-17 00:48:37,895][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:48:37,896][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:48:37,896][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:48:37,896][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:48:58,531][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:48:58,671][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:49:17,907][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:49:17,908][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:49:17,980][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:49:19,627][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:49:22,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:49:22,183][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:49:22,184][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:49:22,185][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:49:22,191][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:49:22,192][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:49:22,192][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:49:22,201][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-07-17 00:49:22,202][train][INFO] - Logging hyperparameters!
[2022-07-17 00:49:22,207][train][INFO] - Starting training!
[2022-07-17 00:49:22,207][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:49:22,209][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:49:22,209][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:49:22,209][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:49:42,644][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:49:42,784][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-07-17 00:49:48,250][train][INFO] - Starting testing!
[2022-07-17 00:49:50,950][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:49:51,321][train][INFO] - Finalizing!
[2022-07-17 00:50:14,917][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-07-17 00:50:14,917][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-07-17 00:50:14,989][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-07-17 00:50:16,633][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-07-17 00:50:19,170][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-07-17 00:50:19,172][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-07-17 00:50:19,173][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-07-17 00:50:19,175][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-07-17 00:50:19,180][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-07-17 00:50:19,181][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-07-17 00:50:19,181][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-07-17 00:50:19,190][train][INFO] - Logging hyperparameters!
[2022-07-17 00:50:19,235][train][INFO] - Starting training!
[2022-07-17 00:50:19,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-07-17 00:50:19,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-07-17 00:50:19,239][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-07-17 00:50:19,239][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-07-17 00:50:39,853][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-07-17 00:50:40,021][pytorch_lightning.core.lightning][INFO] - 
  | Name        | Type             | Params
-------------------------------------------------
0 | backbone    | Joiner           | 23.5 M
1 | transformer | Transformer      | 34.2 M
2 | matcher     | HungarianMatcher | 0     
3 | cropper     | DynamicCropper   | 371 K 
4 | query_embed | Embedding        | 1.2 K 
5 | input_proj  | ModuleList       | 525 K 
6 | class_embed | ModuleList       | 140 K 
7 | bbox_embed  | ModuleList       | 795 K 
-------------------------------------------------
58.4 M    Trainable params
222 K     Non-trainable params
58.7 M    Total params
234.671   Total estimated model params size (MB)
[2022-09-16 16:42:24,504][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-16 16:42:24,513][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-16 16:42:24,630][train][INFO] - Instantiating datamodule <dataloader.COCODataModule>
[2022-09-20 10:41:25,038][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 10:41:25,038][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 10:41:25,123][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 10:42:00,276][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 10:42:00,276][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 10:42:00,341][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 10:42:00,831][train][INFO] - Instantiating model <models.fast_detr.FastDETR>
[2022-09-20 17:05:50,822][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:05:50,822][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:05:50,912][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:05:57,178][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:06:40,023][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:06:40,024][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:06:40,084][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:06:42,013][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:07:07,906][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:07:07,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:07:07,959][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:07:09,896][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:11:38,783][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:11:38,783][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:11:38,837][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:11:40,727][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:12:49,491][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:12:49,492][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:12:49,549][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:12:51,474][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:36:27,520][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:36:27,521][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:36:27,602][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:36:29,560][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:40:54,363][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:40:54,363][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:40:54,424][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:40:54,961][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:42:38,259][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:42:38,259][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:42:38,334][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:42:38,915][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:57:08,154][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:57:08,155][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:57:08,223][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:57:08,728][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:58:41,975][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:58:41,975][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:58:42,037][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:58:42,578][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 17:58:57,007][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 17:58:57,007][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 17:58:57,068][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 17:58:57,513][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:02:07,306][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:02:07,306][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:02:07,371][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:02:07,871][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:02:34,315][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:02:34,316][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:02:34,375][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:02:34,831][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:03:02,132][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:03:02,132][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:03:02,187][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:03:02,625][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:03:23,988][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:03:23,989][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:03:24,043][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:03:24,482][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:03:48,766][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:03:48,767][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:03:48,819][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:03:49,263][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 18:04:39,123][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 18:04:39,123][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 18:04:39,179][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 18:04:39,649][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:50:40,592][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:50:40,593][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:50:40,683][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:50:46,164][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:51:04,459][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:51:04,460][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:51:04,514][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:51:04,931][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:51:45,201][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:51:45,201][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:51:45,254][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:51:45,679][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:51:56,727][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:51:56,727][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:51:56,778][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:51:57,182][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:52:23,608][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:52:23,609][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:52:23,664][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:52:24,070][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:52:56,288][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:52:56,288][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:52:56,341][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:52:56,751][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:53:44,441][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:53:44,441][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:53:44,505][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:53:45,017][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:54:35,712][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:54:35,712][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:54:35,770][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:54:36,286][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:57:39,098][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:57:39,099][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:57:39,154][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:57:39,619][train][INFO] - Instantiating model <models.model.Model>
[2022-09-20 23:57:48,976][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-20 23:57:48,976][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-20 23:57:49,030][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-20 23:57:49,457][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:01:39,636][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:01:39,652][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:01:39,787][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:01:45,707][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:02:02,748][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:02:02,748][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:02:02,832][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:02:05,447][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:04:17,566][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:04:17,566][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:04:17,652][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:04:21,538][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:04:50,283][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:04:50,283][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:04:50,368][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:04:53,319][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:07:32,654][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:07:32,669][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:07:32,752][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:07:36,482][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:11:41,036][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:11:41,036][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:11:41,119][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:11:43,329][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:13:33,775][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:13:33,775][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:13:33,858][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:13:35,525][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:14:50,046][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:14:50,046][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:14:50,130][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:14:51,629][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:15:45,474][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:15:45,475][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:15:45,558][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:15:47,036][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:16:05,835][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:16:05,835][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:16:05,919][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:16:07,411][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:16:08,749][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:16:08,750][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:16:08,752][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:16:08,754][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:16:08,760][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:16:08,760][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:16:08,760][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:16:08,761][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:16:08,761][train][INFO] - Logging hyperparameters!
[2022-09-21 10:16:08,764][train][INFO] - Starting training!
[2022-09-21 10:16:08,765][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:16:08,818][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:16:08,818][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:16:08,818][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:21:20,434][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:21:20,434][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:21:20,518][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:21:22,421][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:21:23,798][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:21:23,800][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:21:23,800][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:21:23,802][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:21:23,808][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:21:23,808][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:21:23,808][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:21:23,809][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:21:23,809][train][INFO] - Logging hyperparameters!
[2022-09-21 10:21:23,812][train][INFO] - Starting training!
[2022-09-21 10:21:23,813][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:21:23,814][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:21:23,815][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:21:23,815][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:37:46,728][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:37:46,729][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:37:46,828][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:37:48,405][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:37:49,738][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:37:49,739][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:37:49,740][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:37:49,742][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:37:49,748][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:37:49,748][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:37:49,748][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:37:49,749][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:37:49,749][train][INFO] - Logging hyperparameters!
[2022-09-21 10:37:49,752][train][INFO] - Starting training!
[2022-09-21 10:37:49,753][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:37:49,754][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:37:49,754][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:37:49,755][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:38:06,228][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:38:06,229][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:38:06,313][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:38:07,797][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:38:09,113][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:38:09,115][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:38:09,116][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:38:09,117][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:38:09,123][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:38:09,123][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:38:09,124][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:38:09,124][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:38:09,124][train][INFO] - Logging hyperparameters!
[2022-09-21 10:38:09,128][train][INFO] - Starting training!
[2022-09-21 10:38:09,128][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:38:09,129][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:38:09,130][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:38:09,130][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:39:08,791][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:39:08,791][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:39:08,874][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:39:10,357][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:39:11,701][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:39:11,703][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:39:11,704][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:39:11,705][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:39:11,711][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:39:11,711][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:39:11,712][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:39:11,712][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:39:11,712][train][INFO] - Logging hyperparameters!
[2022-09-21 10:39:11,716][train][INFO] - Starting training!
[2022-09-21 10:39:11,717][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:39:11,718][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:39:11,718][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:39:11,718][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:41:44,180][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:41:44,180][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:41:44,265][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:41:45,756][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:41:47,113][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:41:47,114][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:41:47,115][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:41:47,117][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:41:47,123][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:41:47,123][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:41:47,123][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:41:47,124][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:41:47,124][train][INFO] - Logging hyperparameters!
[2022-09-21 10:41:47,127][train][INFO] - Starting training!
[2022-09-21 10:41:47,128][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:41:47,129][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:41:47,129][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:41:47,129][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:43:28,954][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:43:28,955][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:43:29,038][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:43:30,547][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:43:31,879][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:43:31,881][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:43:31,881][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:43:31,883][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:43:31,889][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:43:31,889][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:43:31,889][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:43:31,890][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:43:31,890][train][INFO] - Logging hyperparameters!
[2022-09-21 10:43:31,893][train][INFO] - Starting training!
[2022-09-21 10:43:31,894][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:43:31,895][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:43:31,896][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:43:31,896][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:43:39,695][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:43:39,912][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:44:30,010][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:44:30,010][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:44:30,102][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:44:31,786][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:44:33,157][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:44:33,159][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:44:33,160][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:44:33,161][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:44:33,167][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:44:33,167][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:44:33,168][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:44:33,168][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:44:33,168][train][INFO] - Logging hyperparameters!
[2022-09-21 10:44:33,172][train][INFO] - Starting training!
[2022-09-21 10:44:33,173][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:44:33,174][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:44:33,174][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:44:33,174][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:44:40,886][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:44:41,100][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:47:55,656][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:47:55,656][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:47:55,739][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:47:57,265][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:47:58,593][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:47:58,595][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:47:58,595][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:47:58,597][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:47:58,603][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:47:58,603][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:47:58,603][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:47:58,604][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:47:58,604][train][INFO] - Logging hyperparameters!
[2022-09-21 10:47:58,607][train][INFO] - Starting training!
[2022-09-21 10:47:58,608][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:47:58,609][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:47:58,609][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:47:58,610][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:48:06,306][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:48:06,522][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:50:35,197][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:50:35,197][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:50:35,280][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:50:36,810][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:50:38,156][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:50:38,158][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:50:38,159][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:50:38,160][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:50:38,166][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:50:38,167][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:50:38,167][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:50:38,167][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:50:38,168][train][INFO] - Logging hyperparameters!
[2022-09-21 10:50:38,171][train][INFO] - Starting training!
[2022-09-21 10:50:38,172][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:50:38,173][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:50:38,173][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:50:38,173][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:50:45,924][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:50:46,143][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:51:40,910][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:51:40,910][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:51:40,993][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:51:42,475][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:51:43,860][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:51:43,862][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:51:43,863][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:51:43,864][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:51:43,870][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:51:43,871][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:51:43,871][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:51:43,871][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:51:43,872][train][INFO] - Logging hyperparameters!
[2022-09-21 10:51:43,875][train][INFO] - Starting training!
[2022-09-21 10:51:43,876][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:51:43,877][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:51:43,877][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:51:43,877][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:51:51,829][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:51:52,047][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:52:25,997][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:52:25,997][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:52:26,080][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:52:27,637][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:52:29,004][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:52:29,005][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:52:29,006][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:52:29,008][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:52:29,014][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:52:29,014][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:52:29,014][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:52:29,015][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:52:29,015][train][INFO] - Logging hyperparameters!
[2022-09-21 10:52:29,018][train][INFO] - Starting training!
[2022-09-21 10:52:29,019][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:52:29,020][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:52:29,020][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:52:29,021][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:52:36,713][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:52:36,939][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:53:57,531][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:53:57,531][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:53:57,613][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:53:59,129][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:54:00,497][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:54:00,499][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:54:00,500][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:54:00,501][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:54:00,507][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:54:00,507][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:54:00,508][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:54:00,508][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:54:00,508][train][INFO] - Logging hyperparameters!
[2022-09-21 10:54:00,512][train][INFO] - Starting training!
[2022-09-21 10:54:00,513][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:54:00,514][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:54:00,514][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:54:00,514][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:54:08,209][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:54:08,427][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 10:55:15,963][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 10:55:15,964][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 10:55:16,047][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 10:55:17,514][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 10:55:18,848][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 10:55:18,849][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 10:55:18,850][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 10:55:18,852][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 10:55:18,858][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 10:55:18,858][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 10:55:18,858][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 10:55:18,859][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 10:55:18,859][train][INFO] - Logging hyperparameters!
[2022-09-21 10:55:18,862][train][INFO] - Starting training!
[2022-09-21 10:55:18,863][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 10:55:18,864][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 10:55:18,864][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 10:55:18,864][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 10:55:26,550][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 10:55:26,767][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:07:40,106][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:07:40,106][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:07:40,188][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:07:41,707][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:07:43,094][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:07:43,096][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:07:43,097][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:07:43,098][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:07:43,104][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:07:43,104][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:07:43,105][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:07:43,105][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:07:43,105][train][INFO] - Logging hyperparameters!
[2022-09-21 11:07:43,109][train][INFO] - Starting training!
[2022-09-21 11:07:43,109][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:07:43,110][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:07:43,111][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:07:43,111][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:07:50,826][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:07:51,042][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:09:18,734][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:09:18,734][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:09:18,816][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:09:20,287][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:09:21,629][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:09:21,631][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:09:21,632][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:09:21,633][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:09:21,639][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:09:21,640][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:09:21,640][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:09:21,640][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:09:21,641][train][INFO] - Logging hyperparameters!
[2022-09-21 11:09:21,644][train][INFO] - Starting training!
[2022-09-21 11:09:21,645][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:09:21,646][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:09:21,646][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:09:21,646][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:09:29,341][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:09:29,559][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:11:10,069][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:11:10,069][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:11:10,152][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:11:11,622][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:11:12,971][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:11:12,972][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:11:12,973][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:11:12,975][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:11:12,981][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:11:12,981][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:11:12,981][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:11:12,982][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:11:12,982][train][INFO] - Logging hyperparameters!
[2022-09-21 11:11:12,985][train][INFO] - Starting training!
[2022-09-21 11:11:12,986][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:11:12,987][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:11:12,987][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:11:12,987][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:11:20,613][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:11:20,830][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:12:27,488][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:12:27,488][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:12:27,571][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:12:29,037][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:12:30,419][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:12:30,420][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:12:30,421][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:12:30,423][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:12:30,429][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:12:30,429][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:12:30,429][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:12:30,430][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:12:30,430][train][INFO] - Logging hyperparameters!
[2022-09-21 11:12:30,433][train][INFO] - Starting training!
[2022-09-21 11:12:30,434][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:12:30,435][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:12:30,435][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:12:30,436][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:12:38,050][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:12:38,265][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:15:07,446][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:15:07,446][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:15:07,528][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:15:08,996][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:15:10,326][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:15:10,328][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:15:10,329][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:15:10,330][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:15:10,336][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:15:10,337][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:15:10,337][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:15:10,337][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:15:10,338][train][INFO] - Logging hyperparameters!
[2022-09-21 11:15:10,341][train][INFO] - Starting training!
[2022-09-21 11:15:10,342][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:15:10,343][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:15:10,343][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:15:10,343][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:15:17,966][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:15:18,181][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:16:06,371][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:16:06,372][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:16:06,456][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:16:07,948][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:16:09,680][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:16:09,682][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:16:09,683][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:16:09,684][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:16:09,690][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:16:09,690][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:16:09,691][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:16:09,691][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:16:09,691][train][INFO] - Logging hyperparameters!
[2022-09-21 11:16:09,695][train][INFO] - Starting training!
[2022-09-21 11:16:09,696][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:16:09,697][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:16:09,697][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:16:09,697][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:16:17,354][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:16:17,571][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:20:47,892][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:20:47,892][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:20:47,974][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:20:49,492][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:20:50,848][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:20:50,850][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:20:50,850][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:20:50,852][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:20:50,858][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:20:50,858][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:20:50,858][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:20:50,859][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:20:50,859][train][INFO] - Logging hyperparameters!
[2022-09-21 11:20:50,862][train][INFO] - Starting training!
[2022-09-21 11:20:50,863][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:20:50,864][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:20:50,865][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:20:50,865][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:20:58,585][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:20:58,811][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:23:35,367][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:23:35,367][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:23:35,450][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:23:36,941][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:23:38,298][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:23:38,300][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:23:38,301][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:23:38,302][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:23:38,308][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:23:38,308][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:23:38,309][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:23:38,309][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:23:38,309][train][INFO] - Logging hyperparameters!
[2022-09-21 11:23:38,313][train][INFO] - Starting training!
[2022-09-21 11:23:38,314][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:23:38,315][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:23:38,315][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:23:38,315][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:23:46,054][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:23:46,280][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:33:35,899][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:33:35,900][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:33:35,982][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:33:37,452][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:33:38,838][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:33:38,840][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:33:38,841][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:33:38,842][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:33:38,848][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:33:38,848][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:33:38,848][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:33:38,849][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:33:38,849][train][INFO] - Logging hyperparameters!
[2022-09-21 11:33:38,853][train][INFO] - Starting training!
[2022-09-21 11:33:38,853][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:33:38,854][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:33:38,855][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:33:38,855][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:33:46,534][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:33:46,751][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:34:22,322][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:34:22,322][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:34:22,405][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:34:23,871][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:34:25,207][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:34:25,209][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:34:25,209][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:34:25,211][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:34:25,217][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:34:25,217][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:34:25,217][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:34:25,218][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:34:25,218][train][INFO] - Logging hyperparameters!
[2022-09-21 11:34:25,221][train][INFO] - Starting training!
[2022-09-21 11:34:25,222][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:34:25,223][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:34:25,224][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:34:25,224][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:34:32,891][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:34:33,108][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:38:12,216][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:38:12,216][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:38:12,299][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:38:13,784][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:38:15,121][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:38:15,123][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:38:15,124][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:38:15,125][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:38:15,131][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:38:15,132][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:38:15,132][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:38:15,132][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:38:15,133][train][INFO] - Logging hyperparameters!
[2022-09-21 11:38:15,136][train][INFO] - Starting training!
[2022-09-21 11:38:15,137][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:38:15,138][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:38:15,138][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:38:15,138][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:38:22,767][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:38:22,983][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:38:42,824][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:38:42,824][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:38:42,907][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:38:44,379][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:38:45,704][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:38:45,706][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:38:45,707][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:38:45,708][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:38:45,714][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:38:45,714][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:38:45,715][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:38:45,715][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:38:45,715][train][INFO] - Logging hyperparameters!
[2022-09-21 11:38:45,719][train][INFO] - Starting training!
[2022-09-21 11:38:45,720][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:38:45,720][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:38:45,721][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:38:45,721][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:38:53,480][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:38:53,698][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:39:24,721][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:39:24,721][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:39:24,803][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:39:26,270][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:39:27,607][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:39:27,608][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:39:27,609][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:39:27,611][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:39:27,617][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:39:27,617][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:39:27,617][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:39:27,618][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:39:27,618][train][INFO] - Logging hyperparameters!
[2022-09-21 11:39:27,621][train][INFO] - Starting training!
[2022-09-21 11:39:27,622][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:39:27,623][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:39:27,623][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:39:27,623][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:39:35,276][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:39:35,493][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:40:38,465][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:40:38,465][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:40:38,548][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:40:40,025][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:41:03,674][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:41:03,675][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:41:03,758][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:41:05,218][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:41:06,556][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:41:06,558][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:41:06,558][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:41:06,560][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:41:06,566][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:41:06,566][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:41:06,566][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:41:06,567][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:41:06,567][train][INFO] - Logging hyperparameters!
[2022-09-21 11:41:06,570][train][INFO] - Starting training!
[2022-09-21 11:41:06,571][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:41:06,572][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:41:06,572][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:41:06,573][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:41:14,293][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:41:14,511][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:41:38,788][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:41:38,789][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:41:38,871][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:41:40,335][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:41:41,669][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:41:41,671][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:41:41,671][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:41:41,673][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:41:41,679][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:41:41,679][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:41:41,679][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:41:41,680][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:41:41,680][train][INFO] - Logging hyperparameters!
[2022-09-21 11:41:41,683][train][INFO] - Starting training!
[2022-09-21 11:41:41,684][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:41:41,685][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:41:41,685][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:41:41,686][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:41:49,368][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:41:49,584][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:42:48,712][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:42:48,712][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:42:48,795][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:42:50,266][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:42:51,608][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:42:51,610][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:42:51,611][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:42:51,612][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:42:51,618][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:42:51,619][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:42:51,619][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:42:51,619][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:42:51,620][train][INFO] - Logging hyperparameters!
[2022-09-21 11:42:51,623][train][INFO] - Starting training!
[2022-09-21 11:42:51,624][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:42:51,625][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:42:51,625][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:42:51,625][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:42:59,331][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:42:59,549][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:49:35,212][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:49:35,212][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:49:35,296][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:49:36,764][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:49:38,102][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:49:38,103][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:49:38,104][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:49:38,106][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:49:38,112][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:49:38,112][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:49:38,112][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:49:38,113][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:49:38,113][train][INFO] - Logging hyperparameters!
[2022-09-21 11:49:38,116][train][INFO] - Starting training!
[2022-09-21 11:49:38,117][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:49:38,118][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:49:38,118][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:49:38,118][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:49:45,761][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:49:45,977][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 11:54:31,841][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 11:54:31,841][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 11:54:31,925][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 11:54:33,397][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 11:54:34,739][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 11:54:34,741][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 11:54:34,741][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 11:54:34,743][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 11:54:34,749][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 11:54:34,749][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 11:54:34,749][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 11:54:34,750][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 11:54:34,750][train][INFO] - Logging hyperparameters!
[2022-09-21 11:54:34,753][train][INFO] - Starting training!
[2022-09-21 11:54:34,754][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 11:54:34,755][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 11:54:34,755][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 11:54:34,756][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 11:54:42,400][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 11:54:42,616][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:09:44,143][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:09:44,159][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:09:44,270][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:10:00,178][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:10:02,315][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:10:02,317][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:10:02,318][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:10:02,320][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:10:02,326][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:10:02,326][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:10:02,326][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:10:02,327][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:10:02,327][train][INFO] - Logging hyperparameters!
[2022-09-21 12:10:02,330][train][INFO] - Starting training!
[2022-09-21 12:10:02,331][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:10:02,332][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:10:02,333][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:10:02,333][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:10:10,739][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:10:10,954][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:10:27,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:10:27,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:10:27,265][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:10:28,729][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:10:30,111][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:10:30,112][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:10:30,113][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:10:30,115][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:10:30,121][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:10:30,121][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:10:30,121][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:10:30,122][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:10:30,122][train][INFO] - Logging hyperparameters!
[2022-09-21 12:10:30,125][train][INFO] - Starting training!
[2022-09-21 12:10:30,126][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:10:30,127][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:10:30,127][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:10:30,127][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:10:37,723][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:10:37,935][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:11:21,193][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:11:21,193][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:11:21,278][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:11:22,756][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:11:24,089][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:11:24,091][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:11:24,092][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:11:24,093][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:11:24,099][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:11:24,099][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:11:24,100][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:11:24,100][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:11:24,100][train][INFO] - Logging hyperparameters!
[2022-09-21 12:11:24,104][train][INFO] - Starting training!
[2022-09-21 12:11:24,105][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:11:24,106][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:11:24,106][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:11:24,106][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:11:31,824][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:11:32,039][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:12:07,560][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:12:07,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:12:07,644][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:12:09,127][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:12:10,451][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:12:10,453][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:12:10,454][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:12:10,455][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:12:10,461][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:12:10,461][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:12:10,462][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:12:10,462][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:12:10,462][train][INFO] - Logging hyperparameters!
[2022-09-21 12:12:10,466][train][INFO] - Starting training!
[2022-09-21 12:12:10,467][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:12:10,468][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:12:10,468][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:12:10,468][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:12:18,180][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:12:18,399][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:13:49,600][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:13:49,600][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:13:49,685][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:13:51,176][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:13:52,502][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:13:52,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:13:52,505][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:13:52,506][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:13:52,512][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:13:52,512][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:13:52,513][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:13:52,513][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:13:52,513][train][INFO] - Logging hyperparameters!
[2022-09-21 12:13:52,517][train][INFO] - Starting training!
[2022-09-21 12:13:52,518][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:13:52,519][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:13:52,519][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:13:52,519][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:14:00,214][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:14:00,433][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:14:26,794][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:14:26,794][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:14:26,877][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:14:28,336][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:14:29,719][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:14:29,721][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:14:29,722][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:14:29,723][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:14:29,729][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:14:29,730][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:14:29,730][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:14:29,730][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:14:29,731][train][INFO] - Logging hyperparameters!
[2022-09-21 12:14:29,734][train][INFO] - Starting training!
[2022-09-21 12:14:29,735][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:14:29,736][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:14:29,736][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:14:29,736][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:14:37,496][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:14:37,717][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:15:03,561][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:15:03,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:15:03,646][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:15:05,127][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:15:06,464][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:15:06,466][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:15:06,467][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:15:06,468][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:15:06,474][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:15:06,475][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:15:06,475][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:15:06,475][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:15:06,476][train][INFO] - Logging hyperparameters!
[2022-09-21 12:15:06,479][train][INFO] - Starting training!
[2022-09-21 12:15:06,480][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:15:06,481][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:15:06,481][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:15:06,481][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:15:14,063][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:15:14,272][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:15:39,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:15:39,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:15:39,984][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:15:41,437][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:15:42,771][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:15:42,773][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:15:42,773][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:15:42,775][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:15:42,781][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:15:42,781][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:15:42,781][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:15:42,782][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:15:42,782][train][INFO] - Logging hyperparameters!
[2022-09-21 12:15:42,786][train][INFO] - Starting training!
[2022-09-21 12:15:42,786][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:15:42,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:15:42,787][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:15:42,788][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:15:50,507][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:15:50,727][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:16:50,938][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:16:50,939][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:16:51,022][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:16:52,502][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:16:53,893][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:16:53,895][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:16:53,895][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:16:53,897][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:16:53,903][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:16:53,903][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:16:53,903][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:16:53,904][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:16:53,904][train][INFO] - Logging hyperparameters!
[2022-09-21 12:16:53,907][train][INFO] - Starting training!
[2022-09-21 12:16:53,908][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:16:53,909][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:16:53,909][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:16:53,910][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:17:01,820][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:17:02,040][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:17:17,747][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:17:17,748][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:17:17,831][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:17:19,311][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:17:20,658][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:17:20,660][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:17:20,661][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:17:20,662][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:17:20,668][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:17:20,668][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:17:20,669][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:17:20,669][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:17:20,670][train][INFO] - Logging hyperparameters!
[2022-09-21 12:17:20,673][train][INFO] - Starting training!
[2022-09-21 12:17:20,674][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:17:20,675][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:17:20,675][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:17:20,675][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:17:28,403][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:17:28,622][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:17:57,005][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:17:57,005][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:17:57,089][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:17:58,576][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:17:59,929][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:17:59,931][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:17:59,932][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:17:59,933][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:17:59,939][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:17:59,939][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:17:59,939][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:17:59,940][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:17:59,940][train][INFO] - Logging hyperparameters!
[2022-09-21 12:17:59,944][train][INFO] - Starting training!
[2022-09-21 12:17:59,944][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:17:59,945][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:17:59,946][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:17:59,946][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:18:07,798][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:18:08,022][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:19:56,274][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:19:56,274][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:19:56,358][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:19:57,833][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:19:59,180][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:19:59,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:19:59,182][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:19:59,183][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:19:59,189][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:19:59,190][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:19:59,190][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:19:59,190][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:19:59,191][train][INFO] - Logging hyperparameters!
[2022-09-21 12:19:59,194][train][INFO] - Starting training!
[2022-09-21 12:19:59,195][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:19:59,196][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:19:59,196][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:19:59,196][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:20:06,871][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:20:07,089][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:20:38,303][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:20:38,304][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:20:38,387][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:20:39,852][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:20:41,177][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:20:41,179][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:20:41,179][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:20:41,181][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:20:41,187][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:20:41,187][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:20:41,187][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:20:41,188][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:20:41,188][train][INFO] - Logging hyperparameters!
[2022-09-21 12:20:41,191][train][INFO] - Starting training!
[2022-09-21 12:20:41,192][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:20:41,193][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:20:41,193][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:20:41,194][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:20:48,805][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:20:49,017][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:21:16,394][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:21:16,395][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:21:16,478][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:21:17,949][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:21:19,301][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:21:19,302][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:21:19,303][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:21:19,305][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:21:19,311][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:21:19,311][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:21:19,311][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:21:19,312][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:21:19,312][train][INFO] - Logging hyperparameters!
[2022-09-21 12:21:19,315][train][INFO] - Starting training!
[2022-09-21 12:21:19,316][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:21:19,317][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:21:19,318][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:21:19,318][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:21:27,127][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:21:27,352][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:22:32,690][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:22:32,690][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:22:32,775][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:22:34,261][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:22:35,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:22:35,616][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:22:35,617][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:22:35,618][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:22:35,624][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:22:35,625][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:22:35,625][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:22:35,625][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:22:35,626][train][INFO] - Logging hyperparameters!
[2022-09-21 12:22:35,629][train][INFO] - Starting training!
[2022-09-21 12:22:35,630][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:22:35,631][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:22:35,631][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:22:35,631][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:22:43,412][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:22:43,636][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:23:07,900][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:23:07,900][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:23:07,984][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:23:09,460][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:23:10,808][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:23:10,809][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:23:10,810][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:23:10,812][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:23:10,818][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:23:10,818][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:23:10,818][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:23:10,819][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:23:10,819][train][INFO] - Logging hyperparameters!
[2022-09-21 12:23:10,822][train][INFO] - Starting training!
[2022-09-21 12:23:10,823][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:23:10,824][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:23:10,824][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:23:10,824][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:23:18,466][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:23:18,684][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:23:53,543][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:23:53,543][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:23:53,627][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:23:55,079][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:23:56,393][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:23:56,395][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:23:56,396][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:23:56,397][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:23:56,403][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:23:56,403][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:23:56,404][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:23:56,404][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:23:56,404][train][INFO] - Logging hyperparameters!
[2022-09-21 12:23:56,408][train][INFO] - Starting training!
[2022-09-21 12:23:56,409][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:23:56,409][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:23:56,410][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:23:56,410][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:24:04,019][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:24:04,229][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:24:28,959][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:24:28,959][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:24:29,043][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:24:30,515][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:24:31,878][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:24:31,880][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:24:31,881][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:24:31,882][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:24:31,888][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:24:31,888][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:24:31,889][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:24:31,889][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:24:31,889][train][INFO] - Logging hyperparameters!
[2022-09-21 12:24:31,893][train][INFO] - Starting training!
[2022-09-21 12:24:31,893][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:24:31,894][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:24:31,895][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:24:31,895][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:24:39,615][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:24:39,834][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:25:45,431][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:25:45,431][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:25:45,513][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:25:46,973][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:25:48,339][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:25:48,341][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:25:48,341][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:25:48,343][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:25:48,349][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:25:48,349][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:25:48,349][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:25:48,350][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:25:48,350][train][INFO] - Logging hyperparameters!
[2022-09-21 12:25:48,353][train][INFO] - Starting training!
[2022-09-21 12:25:48,354][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:25:48,355][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:25:48,356][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:25:48,356][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:25:56,041][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:25:56,257][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:26:30,347][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:26:30,347][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:26:30,431][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:26:31,899][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:26:33,226][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:26:33,228][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:26:33,229][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:26:33,230][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:26:33,236][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:26:33,237][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:26:33,237][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:26:33,237][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:26:33,238][train][INFO] - Logging hyperparameters!
[2022-09-21 12:26:33,241][train][INFO] - Starting training!
[2022-09-21 12:26:33,242][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:26:33,243][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:26:33,243][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:26:33,243][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:26:40,965][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:26:41,183][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:27:15,771][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:27:15,771][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:27:15,855][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:27:17,343][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:27:18,860][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:27:18,862][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:27:18,863][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:27:18,864][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:27:18,870][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:27:18,871][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:27:18,871][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:27:18,871][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:27:18,872][train][INFO] - Logging hyperparameters!
[2022-09-21 12:27:18,875][train][INFO] - Starting training!
[2022-09-21 12:27:18,876][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:27:18,877][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:27:18,877][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:27:18,877][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:27:26,499][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:27:26,712][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:28:44,168][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:28:44,168][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:28:44,252][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:28:45,726][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:28:47,040][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:28:47,041][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:28:47,042][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:28:47,044][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:28:47,050][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:28:47,050][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:28:47,050][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:28:47,051][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:28:47,051][train][INFO] - Logging hyperparameters!
[2022-09-21 12:28:47,054][train][INFO] - Starting training!
[2022-09-21 12:28:47,055][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:28:47,056][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:28:47,056][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:28:47,056][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:28:54,693][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:28:54,909][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:29:20,935][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:29:20,935][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:29:21,019][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:29:22,498][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:29:23,853][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:29:23,855][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:29:23,856][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:29:23,857][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:29:23,863][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:29:23,864][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:29:23,864][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:29:23,864][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:29:23,865][train][INFO] - Logging hyperparameters!
[2022-09-21 12:29:23,868][train][INFO] - Starting training!
[2022-09-21 12:29:23,869][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:29:23,870][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:29:23,870][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:29:23,870][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:29:31,614][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:29:31,834][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:30:40,972][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:30:40,972][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:30:41,056][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:30:42,511][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:30:43,918][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:30:43,920][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:30:43,921][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:30:43,922][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:30:43,929][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:30:43,929][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:30:43,929][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:30:43,930][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:30:43,930][train][INFO] - Logging hyperparameters!
[2022-09-21 12:30:43,933][train][INFO] - Starting training!
[2022-09-21 12:30:43,934][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:30:43,935][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:30:43,935][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:30:43,936][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:30:51,607][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:30:51,827][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:31:31,139][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:31:31,139][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:31:31,222][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:31:32,681][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:31:33,997][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:31:33,999][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:31:34,000][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:31:34,001][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:31:34,007][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:31:34,008][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:31:34,008][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:31:34,008][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:31:34,009][train][INFO] - Logging hyperparameters!
[2022-09-21 12:31:34,012][train][INFO] - Starting training!
[2022-09-21 12:31:34,013][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:31:34,014][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:31:34,014][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:31:34,014][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:31:41,653][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:31:41,869][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:32:52,631][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:32:52,632][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:32:52,715][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:32:54,191][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:32:55,553][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:32:55,555][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:32:55,556][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:32:55,557][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:32:55,563][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:32:55,563][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:32:55,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:32:55,564][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:32:55,564][train][INFO] - Logging hyperparameters!
[2022-09-21 12:32:55,567][train][INFO] - Starting training!
[2022-09-21 12:32:55,568][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:32:55,569][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:32:55,569][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:32:55,570][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:33:03,276][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:33:03,494][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:35:07,652][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:35:07,653][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:35:07,741][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:35:09,217][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:35:10,539][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:35:10,541][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:35:10,542][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:35:10,543][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:35:10,549][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:35:10,549][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:35:10,549][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:35:10,550][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:35:10,550][train][INFO] - Logging hyperparameters!
[2022-09-21 12:35:10,554][train][INFO] - Starting training!
[2022-09-21 12:35:10,554][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:35:10,555][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:35:10,556][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:35:10,556][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:35:18,235][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:35:18,453][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:36:33,694][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:36:33,694][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:36:33,777][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:36:35,292][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:36:36,636][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:36:36,638][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:36:36,639][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:36:36,640][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:36:36,646][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:36:36,646][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:36:36,646][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:36:36,647][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:36:36,647][train][INFO] - Logging hyperparameters!
[2022-09-21 12:36:36,651][train][INFO] - Starting training!
[2022-09-21 12:36:36,651][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:36:36,652][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:36:36,653][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:36:36,653][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:36:44,354][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:36:44,571][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 66.3 K
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
45.8 M    Trainable params
222 K     Non-trainable params
46.1 M    Total params
184.212   Total estimated model params size (MB)
[2022-09-21 12:38:14,459][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:38:14,459][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:38:14,543][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:38:16,022][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:38:17,420][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:38:17,421][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:38:17,422][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:38:17,424][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:38:17,430][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:38:17,430][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:38:17,430][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:38:17,431][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:38:17,431][train][INFO] - Logging hyperparameters!
[2022-09-21 12:38:17,434][train][INFO] - Starting training!
[2022-09-21 12:38:17,435][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:38:17,436][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:38:17,436][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:38:17,437][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:38:25,111][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:38:25,334][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:50:37,503][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:50:37,503][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:50:37,588][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:50:39,083][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:50:40,446][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:50:40,447][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:50:40,448][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:50:40,450][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:50:40,456][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:50:40,456][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:50:40,456][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:50:40,457][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:50:40,457][train][INFO] - Logging hyperparameters!
[2022-09-21 12:50:40,460][train][INFO] - Starting training!
[2022-09-21 12:50:40,461][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:50:40,462][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:50:40,462][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:50:40,462][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:50:48,122][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:50:48,338][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:51:45,636][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:51:45,636][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:51:45,719][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:51:47,185][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:51:48,524][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:51:48,526][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:51:48,527][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:51:48,528][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:51:48,534][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:51:48,534][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:51:48,534][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:51:48,535][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:51:48,535][train][INFO] - Logging hyperparameters!
[2022-09-21 12:51:48,539][train][INFO] - Starting training!
[2022-09-21 12:51:48,539][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:51:48,541][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:51:48,541][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:51:48,541][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:51:56,252][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:51:56,474][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:53:19,432][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:53:19,432][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:53:19,516][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:53:20,994][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:53:22,336][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:53:22,337][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:53:22,338][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:53:22,340][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:53:22,345][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:53:22,346][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:53:22,346][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:53:22,347][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:53:22,347][train][INFO] - Logging hyperparameters!
[2022-09-21 12:53:22,350][train][INFO] - Starting training!
[2022-09-21 12:53:22,351][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:53:22,352][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:53:22,352][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:53:22,352][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:53:30,022][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:53:30,243][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:55:04,001][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:55:04,001][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:55:04,085][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:55:05,548][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:55:06,910][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:55:06,912][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:55:06,913][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:55:06,914][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:55:06,920][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:55:06,920][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:55:06,920][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:55:06,921][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:55:06,921][train][INFO] - Logging hyperparameters!
[2022-09-21 12:55:06,925][train][INFO] - Starting training!
[2022-09-21 12:55:06,925][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:55:06,926][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:55:06,927][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:55:06,927][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:55:14,663][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:55:14,889][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:56:00,213][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:56:00,214][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:56:00,298][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:56:01,777][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:56:03,117][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:56:03,118][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:56:03,119][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:56:03,121][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:56:03,127][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:56:03,127][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:56:03,127][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:56:03,128][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:56:03,128][train][INFO] - Logging hyperparameters!
[2022-09-21 12:56:03,131][train][INFO] - Starting training!
[2022-09-21 12:56:03,132][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:56:03,133][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:56:03,133][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:56:03,133][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:56:10,773][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:56:10,995][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:57:44,708][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:57:44,709][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:57:44,793][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:57:46,274][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:57:47,639][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:57:47,640][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:57:47,641][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:57:47,643][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:57:47,649][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:57:47,649][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:57:47,649][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:57:47,650][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:57:47,650][train][INFO] - Logging hyperparameters!
[2022-09-21 12:57:47,653][train][INFO] - Starting training!
[2022-09-21 12:57:47,654][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:57:47,655][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:57:47,655][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:57:47,655][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:57:55,403][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:57:55,626][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 12:58:39,613][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:58:39,614][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:58:39,697][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:58:41,152][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:58:56,757][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 12:58:56,757][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 12:58:56,841][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 12:58:58,296][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 12:58:59,647][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 12:58:59,649][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 12:58:59,649][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 12:58:59,651][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 12:58:59,657][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 12:58:59,657][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 12:58:59,657][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 12:58:59,658][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 12:58:59,658][train][INFO] - Logging hyperparameters!
[2022-09-21 12:58:59,661][train][INFO] - Starting training!
[2022-09-21 12:58:59,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 12:58:59,663][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 12:58:59,663][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 12:58:59,664][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 12:59:07,369][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 12:59:07,594][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 3.5 M 
-----------------------------------------------------------
46.8 M    Trainable params
222 K     Non-trainable params
47.0 M    Total params
187.891   Total estimated model params size (MB)
[2022-09-21 13:00:34,254][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 13:00:34,254][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 13:00:34,337][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 13:00:35,811][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 13:00:37,155][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 13:00:37,157][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 13:00:37,157][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 13:00:37,159][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 13:00:37,165][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 13:00:37,165][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 13:00:37,165][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 13:00:37,166][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 13:00:37,166][train][INFO] - Logging hyperparameters!
[2022-09-21 13:00:37,169][train][INFO] - Starting training!
[2022-09-21 13:00:37,170][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 13:00:37,171][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 13:00:37,171][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 13:00:37,172][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 13:00:44,877][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 13:00:45,099][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 1.8 M 
-----------------------------------------------------------
45.0 M    Trainable params
222 K     Non-trainable params
45.2 M    Total params
180.813   Total estimated model params size (MB)
[2022-09-21 13:01:30,687][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 13:01:30,687][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 13:01:30,771][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 13:01:32,263][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 13:01:33,600][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 13:01:33,602][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 13:01:33,603][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 13:01:33,604][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 13:01:33,610][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 13:01:33,610][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 13:01:33,611][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 13:01:33,611][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 13:01:33,612][train][INFO] - Logging hyperparameters!
[2022-09-21 13:01:33,615][train][INFO] - Starting training!
[2022-09-21 13:01:33,616][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 13:01:33,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 13:01:33,617][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 13:01:33,617][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 13:01:41,297][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 13:01:41,518][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 4.9 K 
12 | temporal_conv   | Sequential            | 1.8 M 
-----------------------------------------------------------
45.0 M    Trainable params
222 K     Non-trainable params
45.2 M    Total params
180.813   Total estimated model params size (MB)
[2022-09-21 13:02:33,777][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 13:02:33,777][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 13:02:33,862][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 13:02:35,334][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 13:02:36,674][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 13:02:36,676][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 13:02:36,677][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 13:02:36,678][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 13:02:36,684][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 13:02:36,684][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 13:02:36,685][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 13:02:36,685][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 13:02:36,685][train][INFO] - Logging hyperparameters!
[2022-09-21 13:02:36,689][train][INFO] - Starting training!
[2022-09-21 13:02:36,689][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 13:02:36,690][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 13:02:36,691][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 13:02:36,691][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 13:02:44,473][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 13:02:44,700][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 14:39:42,572][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 14:39:42,588][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 14:39:42,736][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 14:40:01,590][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 14:40:03,775][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 14:40:03,776][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 14:40:03,777][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 14:40:03,779][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 14:40:03,785][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 14:40:03,785][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 14:40:03,785][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 14:40:03,786][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 14:40:03,867][train][INFO] - Logging hyperparameters!
[2022-09-21 14:40:03,871][train][INFO] - Starting training!
[2022-09-21 14:40:03,871][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 14:40:03,873][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 14:40:03,873][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 14:40:03,873][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 14:40:13,211][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 14:40:13,433][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 14:53:30,743][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 14:53:30,743][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 14:53:30,826][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 14:53:32,280][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 14:54:39,346][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 14:54:39,346][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 14:54:39,429][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 14:54:40,881][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 14:54:42,258][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 14:54:42,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 14:54:42,260][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 14:54:42,262][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 14:54:42,268][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 14:54:42,268][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 14:54:42,268][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 14:54:42,269][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 14:54:42,269][train][INFO] - Logging hyperparameters!
[2022-09-21 14:54:42,272][train][INFO] - Starting training!
[2022-09-21 14:54:42,273][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 14:54:42,274][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 14:54:42,274][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 14:54:42,275][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 14:54:50,071][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 14:54:50,292][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 14:58:18,470][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 14:58:18,483][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 14:58:18,567][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 14:58:22,816][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 14:58:24,568][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 14:58:24,570][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 14:58:24,570][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 14:58:24,572][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 14:58:24,578][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 14:58:24,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 14:58:24,578][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 14:58:24,579][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 14:58:24,579][train][INFO] - Logging hyperparameters!
[2022-09-21 14:58:24,582][train][INFO] - Starting training!
[2022-09-21 14:58:24,583][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 14:58:24,584][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 14:58:24,584][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 14:58:24,585][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 14:58:32,384][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 14:58:32,608][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 14:59:11,915][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 14:59:11,916][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 14:59:11,999][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 14:59:15,156][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 14:59:16,500][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 14:59:16,501][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 14:59:16,502][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 14:59:16,504][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 14:59:16,510][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 14:59:16,510][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 14:59:16,510][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 14:59:16,511][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 14:59:16,511][train][INFO] - Logging hyperparameters!
[2022-09-21 14:59:16,514][train][INFO] - Starting training!
[2022-09-21 14:59:16,515][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 14:59:16,516][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 14:59:16,516][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 14:59:16,516][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 14:59:24,268][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 14:59:24,491][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:00:18,637][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:00:18,638][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:00:18,723][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:00:21,995][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:00:23,391][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:00:23,393][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:00:23,394][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:00:23,395][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:00:23,401][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:00:23,401][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:00:23,402][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:00:23,402][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:00:23,402][train][INFO] - Logging hyperparameters!
[2022-09-21 15:00:23,406][train][INFO] - Starting training!
[2022-09-21 15:00:23,407][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:00:23,407][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:00:23,408][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:00:23,408][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:00:31,220][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:00:31,442][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:03:58,388][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:03:58,388][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:03:58,473][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:04:02,098][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:04:43,726][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:04:43,743][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:04:43,841][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:04:46,883][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:04:49,112][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:04:49,114][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:04:49,115][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:04:49,116][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:04:49,122][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:04:49,122][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:04:49,123][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:04:49,123][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:04:49,123][train][INFO] - Logging hyperparameters!
[2022-09-21 15:04:49,127][train][INFO] - Starting training!
[2022-09-21 15:04:49,127][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:04:49,128][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:04:49,129][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:04:49,129][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:04:57,026][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:04:57,250][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:27:20,117][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:27:20,167][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:27:20,275][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:27:24,105][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:27:48,999][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:27:49,000][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:27:49,083][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:27:51,514][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:28:05,077][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:28:05,078][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:28:05,164][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:28:07,520][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:28:09,314][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:28:09,316][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:28:09,317][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:28:09,318][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:28:09,324][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:28:09,324][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:28:09,325][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:28:09,325][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:28:09,325][train][INFO] - Logging hyperparameters!
[2022-09-21 15:28:09,329][train][INFO] - Starting training!
[2022-09-21 15:28:09,329][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:28:09,330][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:28:09,331][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:28:09,331][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:28:17,155][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:28:17,377][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:28:53,956][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:28:53,957][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:28:54,041][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:28:57,134][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:28:58,621][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:28:58,622][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:28:58,623][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:28:58,625][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:28:58,631][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:28:58,631][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:28:58,632][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:28:58,632][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:28:58,632][train][INFO] - Logging hyperparameters!
[2022-09-21 15:28:58,636][train][INFO] - Starting training!
[2022-09-21 15:28:58,637][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:28:58,638][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:28:58,638][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:28:58,638][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:29:06,583][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:29:06,807][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:29:30,208][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:29:30,208][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:29:30,291][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:29:33,275][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:29:34,713][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:29:34,714][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:29:34,715][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:29:34,717][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:29:34,723][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:29:34,723][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:29:34,723][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:29:34,724][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:29:34,724][train][INFO] - Logging hyperparameters!
[2022-09-21 15:29:34,728][train][INFO] - Starting training!
[2022-09-21 15:29:34,729][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:29:34,730][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:29:34,730][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:29:34,731][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:29:43,043][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:29:43,266][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:30:23,638][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:30:23,656][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:30:23,768][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:30:26,798][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:30:28,840][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:30:28,842][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:30:28,843][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:30:28,844][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:30:28,850][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:30:28,851][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:30:28,851][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:30:28,851][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:30:28,852][train][INFO] - Logging hyperparameters!
[2022-09-21 15:30:28,855][train][INFO] - Starting training!
[2022-09-21 15:30:28,856][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:30:28,857][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:30:28,857][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:30:28,857][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:30:36,829][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:30:37,056][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:30:56,730][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:30:56,756][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:30:56,841][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:30:59,834][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:31:01,963][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:31:01,965][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:31:01,966][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:31:01,967][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:31:01,973][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:31:01,974][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:31:01,974][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:31:01,975][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:31:01,975][train][INFO] - Logging hyperparameters!
[2022-09-21 15:31:01,978][train][INFO] - Starting training!
[2022-09-21 15:31:01,979][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:31:01,980][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:31:01,980][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:31:01,980][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:31:09,836][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:31:10,059][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 15:53:51,205][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:53:51,216][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:53:51,306][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:53:54,858][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:54:11,739][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 15:54:11,739][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 15:54:11,823][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 15:54:13,590][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 15:54:15,316][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 15:54:15,317][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 15:54:15,318][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 15:54:15,320][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 15:54:15,326][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 15:54:15,326][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 15:54:15,326][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 15:54:15,327][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 15:54:15,327][train][INFO] - Logging hyperparameters!
[2022-09-21 15:54:15,330][train][INFO] - Starting training!
[2022-09-21 15:54:15,331][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 15:54:15,332][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 15:54:15,332][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 15:54:15,332][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 15:54:23,090][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 15:54:23,310][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 16:01:51,935][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 16:01:51,936][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 16:01:52,019][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 16:01:54,109][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 16:01:55,474][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 16:01:55,476][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 16:01:55,477][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 16:01:55,478][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 16:01:55,484][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 16:01:55,484][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 16:01:55,485][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 16:01:55,485][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 16:01:55,485][train][INFO] - Logging hyperparameters!
[2022-09-21 16:01:55,489][train][INFO] - Starting training!
[2022-09-21 16:01:55,490][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 16:01:55,490][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 16:01:55,491][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 16:01:55,491][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 16:02:03,255][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 16:02:03,477][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-21 16:03:56,965][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-21 16:03:56,966][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-21 16:03:57,051][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-21 16:03:58,513][train][INFO] - Instantiating model <models.model.Model>
[2022-09-21 16:03:59,910][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-21 16:03:59,911][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-21 16:03:59,912][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-21 16:03:59,940][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-21 16:03:59,946][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-21 16:03:59,946][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-21 16:03:59,946][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-21 16:03:59,947][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-21 16:03:59,947][train][INFO] - Logging hyperparameters!
[2022-09-21 16:03:59,950][train][INFO] - Starting training!
[2022-09-21 16:03:59,951][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-21 16:03:59,952][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-21 16:03:59,952][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-21 16:03:59,953][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-21 16:04:07,804][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-21 16:04:08,033][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | conv            | Conv2d                | 524 K 
3  | attention       | MultiheadAttention    | 1.1 M 
4  | pos_embed       | PositionalEncoding2D  | 0     
5  | upsample_layer  | Sequential            | 442 K 
6  | head_detect_sot | Conv2d                | 8     
7  | head_class_sot  | Conv2d                | 4     
8  | head_mask_sot   | Conv2d                | 4     
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
43.7 M    Trainable params
222 K     Non-trainable params
44.0 M    Total params
175.822   Total estimated model params size (MB)
[2022-09-22 23:33:37,009][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-22 23:33:37,020][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-22 23:33:37,104][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-22 23:33:38,765][train][INFO] - Instantiating model <models.model.Model>
[2022-09-22 23:33:40,553][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-22 23:33:40,554][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-22 23:33:40,555][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-22 23:33:40,557][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-22 23:33:40,563][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-22 23:33:40,563][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-22 23:33:40,563][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-22 23:33:40,564][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-22 23:33:40,564][train][INFO] - Logging hyperparameters!
[2022-09-22 23:33:40,568][train][INFO] - Starting training!
[2022-09-22 23:33:40,569][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-22 23:33:40,570][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-22 23:33:40,570][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-22 23:33:40,570][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-22 23:33:48,345][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-22 23:33:48,567][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-22 23:56:04,438][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-22 23:56:04,438][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-22 23:56:04,521][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-22 23:56:06,003][train][INFO] - Instantiating model <models.model.Model>
[2022-09-22 23:56:07,295][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-22 23:56:07,297][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-22 23:56:07,298][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-22 23:56:07,299][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-22 23:56:07,305][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-22 23:56:07,305][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-22 23:56:07,306][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-22 23:56:07,306][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-22 23:56:07,306][train][INFO] - Logging hyperparameters!
[2022-09-22 23:56:07,310][train][INFO] - Starting training!
[2022-09-22 23:56:07,311][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-22 23:56:07,312][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-22 23:56:07,312][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-22 23:56:07,312][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-22 23:56:15,125][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-22 23:56:15,346][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-22 23:57:07,546][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-22 23:57:07,546][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-22 23:57:07,630][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-22 23:57:09,105][train][INFO] - Instantiating model <models.model.Model>
[2022-09-22 23:57:10,407][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-22 23:57:10,409][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-22 23:57:10,409][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-22 23:57:10,411][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-22 23:57:10,417][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-22 23:57:10,417][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-22 23:57:10,417][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-22 23:57:10,418][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-22 23:57:10,418][train][INFO] - Logging hyperparameters!
[2022-09-22 23:57:10,421][train][INFO] - Starting training!
[2022-09-22 23:57:10,422][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-22 23:57:10,423][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-22 23:57:10,423][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-22 23:57:10,424][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-22 23:57:18,160][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-22 23:57:18,382][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-22 23:58:36,411][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-22 23:58:36,411][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-22 23:58:36,495][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-22 23:58:37,975][train][INFO] - Instantiating model <models.model.Model>
[2022-09-22 23:58:39,267][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-22 23:58:39,269][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-22 23:58:39,270][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-22 23:58:39,272][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-22 23:58:39,277][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-22 23:58:39,277][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-22 23:58:39,278][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-22 23:58:39,278][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-22 23:58:39,279][train][INFO] - Logging hyperparameters!
[2022-09-22 23:58:39,282][train][INFO] - Starting training!
[2022-09-22 23:58:39,283][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-22 23:58:39,283][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-22 23:58:39,284][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-22 23:58:39,284][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-22 23:58:46,967][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-22 23:58:47,188][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-22 23:59:41,876][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-22 23:59:41,877][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-22 23:59:41,961][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-22 23:59:43,434][train][INFO] - Instantiating model <models.model.Model>
[2022-09-22 23:59:44,736][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-22 23:59:44,738][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-22 23:59:44,739][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-22 23:59:44,741][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-22 23:59:44,747][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-22 23:59:44,747][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-22 23:59:44,747][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-22 23:59:44,748][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-22 23:59:44,748][train][INFO] - Logging hyperparameters!
[2022-09-22 23:59:44,751][train][INFO] - Starting training!
[2022-09-22 23:59:44,752][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-22 23:59:44,753][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-22 23:59:44,753][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-22 23:59:44,753][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-22 23:59:52,549][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-22 23:59:52,770][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 00:04:36,383][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 00:04:36,384][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 00:04:36,467][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 00:04:37,936][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 00:04:39,254][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 00:04:39,255][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 00:04:39,256][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 00:04:39,258][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 00:04:39,263][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 00:04:39,264][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 00:04:39,264][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 00:04:39,264][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 00:04:39,276][train][INFO] - Logging hyperparameters!
[2022-09-23 00:04:39,279][train][INFO] - Starting training!
[2022-09-23 00:04:39,280][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 00:04:39,281][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 00:04:39,281][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 00:04:39,282][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 00:04:47,034][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 00:04:47,256][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 00:05:17,394][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 00:05:17,394][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 00:05:17,478][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 00:05:18,952][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 00:05:20,239][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 00:05:20,240][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 00:05:20,241][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 00:05:20,243][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 00:05:20,249][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 00:05:20,249][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 00:05:20,249][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 00:05:20,250][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 00:05:20,250][train][INFO] - Logging hyperparameters!
[2022-09-23 00:05:20,253][train][INFO] - Starting training!
[2022-09-23 00:05:20,254][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 00:05:20,255][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 00:05:20,256][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 00:05:20,256][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 00:05:27,978][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 00:05:28,199][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 10:52:35,688][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 10:52:35,706][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 10:52:35,808][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 10:52:49,091][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 10:52:51,270][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 10:52:51,272][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 10:52:51,273][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 10:52:51,274][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 10:52:51,280][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 10:52:51,280][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 10:52:51,281][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 10:52:51,281][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 10:52:51,281][train][INFO] - Logging hyperparameters!
[2022-09-23 10:52:51,285][train][INFO] - Starting training!
[2022-09-23 10:52:51,286][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 10:52:51,287][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 10:52:51,287][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 10:52:51,287][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 10:52:59,632][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 10:52:59,848][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:12:53,119][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:12:53,120][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:12:53,204][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:12:56,620][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:12:57,986][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:12:57,987][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:12:57,988][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:12:57,990][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:12:57,996][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:12:57,996][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:12:57,996][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:12:57,997][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:12:57,997][train][INFO] - Logging hyperparameters!
[2022-09-23 11:12:58,000][train][INFO] - Starting training!
[2022-09-23 11:12:58,001][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:12:58,002][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:12:58,002][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:12:58,002][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:13:05,791][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:13:06,044][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:14:42,524][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:14:42,524][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:14:42,609][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:14:45,656][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:14:47,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:14:47,028][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:14:47,029][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:14:47,031][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:14:47,037][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:14:47,037][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:14:47,037][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:14:47,038][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:14:47,038][train][INFO] - Logging hyperparameters!
[2022-09-23 11:14:47,041][train][INFO] - Starting training!
[2022-09-23 11:14:47,042][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:14:47,044][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:14:47,044][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:14:47,044][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:14:54,859][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:14:55,085][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:50:35,021][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:50:35,037][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:50:35,123][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:50:39,239][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:50:40,920][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:50:40,921][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:50:40,922][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:50:40,924][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:50:40,930][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:50:40,930][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:50:40,931][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:50:40,932][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:50:40,932][train][INFO] - Logging hyperparameters!
[2022-09-23 11:50:40,935][train][INFO] - Starting training!
[2022-09-23 11:50:40,936][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:50:40,937][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:50:40,937][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:50:40,938][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:50:48,789][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:50:49,008][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:52:14,733][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:52:14,734][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:52:14,817][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:52:16,319][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:52:17,647][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:52:17,648][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:52:17,649][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:52:17,651][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:52:17,657][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:52:17,657][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:52:17,657][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:52:17,658][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:52:17,658][train][INFO] - Logging hyperparameters!
[2022-09-23 11:52:17,661][train][INFO] - Starting training!
[2022-09-23 11:52:17,662][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:52:17,663][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:52:17,663][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:52:17,664][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:52:25,380][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:52:25,600][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:53:40,152][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:53:40,153][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:53:40,237][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:53:41,793][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:53:43,117][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:53:43,119][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:53:43,120][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:53:43,122][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:53:43,127][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:53:43,127][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:53:43,128][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:53:43,128][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:53:43,129][train][INFO] - Logging hyperparameters!
[2022-09-23 11:53:43,132][train][INFO] - Starting training!
[2022-09-23 11:53:43,133][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:53:43,134][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:53:43,134][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:53:43,134][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:53:50,936][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:53:51,155][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:54:21,261][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:54:21,261][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:54:21,351][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:54:22,868][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:54:24,192][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:54:24,194][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:54:24,195][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:54:24,196][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:54:24,202][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:54:24,202][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:54:24,203][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:54:24,203][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:54:24,203][train][INFO] - Logging hyperparameters!
[2022-09-23 11:54:24,207][train][INFO] - Starting training!
[2022-09-23 11:54:24,207][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:54:24,208][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:54:24,209][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:54:24,209][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:54:31,932][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:54:32,151][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:55:30,051][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:55:30,089][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:55:30,244][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:55:31,741][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:55:33,048][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:55:33,049][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:55:33,050][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:55:33,052][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:55:33,058][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:55:33,058][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:55:33,058][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:55:33,059][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:55:33,059][train][INFO] - Logging hyperparameters!
[2022-09-23 11:55:33,062][train][INFO] - Starting training!
[2022-09-23 11:55:33,063][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:55:33,064][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:55:33,064][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:55:33,065][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:55:40,786][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:55:41,006][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:56:11,355][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:56:11,355][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:56:11,440][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:56:12,911][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:56:14,225][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:56:14,227][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:56:14,228][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:56:14,230][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:56:14,235][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:56:14,236][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:56:14,237][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:56:14,238][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:56:14,238][train][INFO] - Logging hyperparameters!
[2022-09-23 11:56:14,241][train][INFO] - Starting training!
[2022-09-23 11:56:14,242][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:56:14,243][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:56:14,243][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:56:14,243][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:56:22,059][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:56:22,279][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:56:43,977][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:56:43,977][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:56:44,061][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:56:45,718][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:56:47,033][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:56:47,035][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:56:47,036][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:56:47,037][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:56:47,043][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:56:47,043][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:56:47,044][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:56:47,044][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:56:47,044][train][INFO] - Logging hyperparameters!
[2022-09-23 11:56:47,048][train][INFO] - Starting training!
[2022-09-23 11:56:47,049][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:56:47,050][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:56:47,050][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:56:47,051][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:56:54,818][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:56:55,040][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:58:01,294][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:58:01,294][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:58:01,380][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:58:03,016][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:58:04,345][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:58:04,347][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:58:04,347][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:58:04,349][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:58:04,355][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:58:04,355][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:58:04,355][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:58:04,356][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:58:04,356][train][INFO] - Logging hyperparameters!
[2022-09-23 11:58:04,359][train][INFO] - Starting training!
[2022-09-23 11:58:04,360][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:58:04,361][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:58:04,361][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:58:04,362][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:58:12,131][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:58:12,350][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 11:59:16,968][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 11:59:16,969][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 11:59:17,053][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 11:59:18,514][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 11:59:19,821][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 11:59:19,822][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 11:59:19,823][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 11:59:19,825][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 11:59:19,830][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 11:59:19,831][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 11:59:19,831][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 11:59:19,832][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 11:59:19,832][train][INFO] - Logging hyperparameters!
[2022-09-23 11:59:19,835][train][INFO] - Starting training!
[2022-09-23 11:59:19,836][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 11:59:19,837][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 11:59:19,837][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 11:59:19,837][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 11:59:27,537][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 11:59:27,753][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 12:11:03,134][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 12:11:03,134][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 12:11:03,219][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 12:11:04,682][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 12:11:05,998][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 12:11:06,000][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 12:11:06,000][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 12:11:06,002][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 12:11:06,008][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 12:11:06,008][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 12:11:06,008][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 12:11:06,009][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 12:11:06,009][train][INFO] - Logging hyperparameters!
[2022-09-23 12:11:06,012][train][INFO] - Starting training!
[2022-09-23 12:11:06,013][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 12:11:06,014][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 12:11:06,015][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 12:11:06,015][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 12:11:13,765][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 12:11:13,983][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 22:31:36,639][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 22:31:36,653][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 22:31:36,829][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 22:31:51,164][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 22:31:53,219][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 22:31:53,220][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 22:31:53,221][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 22:31:53,223][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 22:31:53,229][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 22:31:53,229][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 22:31:53,229][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 22:31:53,230][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 22:31:53,230][train][INFO] - Logging hyperparameters!
[2022-09-23 22:31:53,234][train][INFO] - Starting training!
[2022-09-23 22:31:53,234][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 22:31:53,236][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 22:31:53,236][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 22:31:53,236][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 22:32:02,526][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 22:32:02,749][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 22:38:16,024][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 22:38:16,025][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 22:38:16,108][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 22:38:18,104][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 22:38:19,434][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 22:38:19,436][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 22:38:19,436][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 22:38:19,438][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 22:38:19,444][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 22:38:19,444][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 22:38:19,444][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 22:38:19,445][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 22:38:19,445][train][INFO] - Logging hyperparameters!
[2022-09-23 22:38:19,448][train][INFO] - Starting training!
[2022-09-23 22:38:19,458][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 22:38:19,459][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 22:38:19,459][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 22:38:19,459][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 22:38:27,196][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 22:38:27,415][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:11:21,080][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:11:21,081][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:11:21,165][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:11:23,145][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:11:24,554][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:11:24,555][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:11:24,556][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:11:24,558][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:11:24,564][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:11:24,564][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:11:24,564][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:11:24,565][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:11:24,565][train][INFO] - Logging hyperparameters!
[2022-09-23 23:11:24,568][train][INFO] - Starting training!
[2022-09-23 23:11:24,569][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:11:24,571][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:11:24,571][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:11:24,571][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:11:32,431][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:11:32,673][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:13:30,646][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:13:30,646][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:13:30,731][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:13:32,227][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:13:33,586][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:13:33,588][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:13:33,589][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:13:33,591][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:13:33,596][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:13:33,597][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:13:33,597][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:13:33,598][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:13:33,598][train][INFO] - Logging hyperparameters!
[2022-09-23 23:13:33,601][train][INFO] - Starting training!
[2022-09-23 23:13:33,602][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:13:33,603][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:13:33,603][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:13:33,603][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:13:41,401][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:13:41,632][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:15:53,034][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:15:53,034][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:15:53,118][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:15:54,604][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:15:55,952][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:15:55,954][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:15:55,955][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:15:55,957][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:15:55,962][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:15:55,962][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:15:55,963][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:15:55,963][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:15:55,964][train][INFO] - Logging hyperparameters!
[2022-09-23 23:15:55,967][train][INFO] - Starting training!
[2022-09-23 23:15:55,968][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:15:55,969][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:15:55,969][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:15:55,970][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:16:03,761][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:16:03,993][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:21:12,891][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:21:12,892][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:21:12,976][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:21:14,448][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:21:15,755][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:21:15,774][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:21:15,775][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:21:15,777][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:21:15,782][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:21:15,782][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:21:15,783][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:21:15,783][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:21:15,784][train][INFO] - Logging hyperparameters!
[2022-09-23 23:21:15,787][train][INFO] - Starting training!
[2022-09-23 23:21:15,788][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:21:15,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:21:15,789][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:21:15,789][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:21:23,598][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:21:23,821][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:24:01,975][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:24:01,975][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:24:02,058][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:24:03,522][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:24:04,842][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:24:04,844][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:24:04,845][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:24:04,847][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:24:04,852][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:24:04,853][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:24:04,853][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:24:04,853][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:24:04,854][train][INFO] - Logging hyperparameters!
[2022-09-23 23:24:04,857][train][INFO] - Starting training!
[2022-09-23 23:24:04,858][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:24:04,859][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:24:04,859][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:24:04,860][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:24:13,108][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:24:13,331][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:26:20,484][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:26:20,485][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:26:20,568][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:26:22,042][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:26:23,348][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:26:23,350][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:26:23,351][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:26:23,352][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:26:23,358][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:26:23,358][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:26:23,358][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:26:23,359][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:26:23,359][train][INFO] - Logging hyperparameters!
[2022-09-23 23:26:23,363][train][INFO] - Starting training!
[2022-09-23 23:26:23,363][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:26:23,364][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:26:23,365][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:26:23,365][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:26:31,118][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:26:31,340][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:28:30,426][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:28:30,427][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:28:30,510][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:28:31,980][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:28:33,389][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:28:33,391][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:28:33,392][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:28:33,394][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:28:33,400][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:28:33,400][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:28:33,400][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:28:33,401][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:28:33,401][train][INFO] - Logging hyperparameters!
[2022-09-23 23:28:33,404][train][INFO] - Starting training!
[2022-09-23 23:28:33,405][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:28:33,406][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:28:33,406][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:28:33,407][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:28:41,198][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:28:41,430][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:43:17,138][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:43:17,138][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:43:17,221][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:43:18,685][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:43:19,979][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:43:19,981][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:43:19,982][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:43:19,983][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:43:19,989][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:43:19,989][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:43:19,989][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:43:19,990][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:43:19,990][train][INFO] - Logging hyperparameters!
[2022-09-23 23:43:19,994][train][INFO] - Starting training!
[2022-09-23 23:43:19,994][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:43:19,995][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:43:19,996][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:43:19,996][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:43:27,719][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:43:27,939][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:43:56,178][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:43:56,179][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:43:56,262][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:43:57,737][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:43:59,032][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:43:59,034][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:43:59,035][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:43:59,036][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:43:59,042][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:43:59,042][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:43:59,043][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:43:59,043][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:43:59,043][train][INFO] - Logging hyperparameters!
[2022-09-23 23:43:59,047][train][INFO] - Starting training!
[2022-09-23 23:43:59,047][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:43:59,048][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:43:59,049][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:43:59,049][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:44:06,761][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:44:06,980][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:45:24,969][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:45:24,970][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:45:25,053][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:45:26,517][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:45:27,813][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:45:27,815][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:45:27,816][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:45:27,817][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:45:27,823][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:45:27,823][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:45:27,824][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:45:27,825][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:45:27,825][train][INFO] - Logging hyperparameters!
[2022-09-23 23:45:27,828][train][INFO] - Starting training!
[2022-09-23 23:45:27,829][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:45:27,830][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:45:27,830][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:45:27,830][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:45:35,492][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:45:35,715][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:46:08,966][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:46:08,966][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:46:09,049][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:46:10,512][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:46:11,848][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:46:11,849][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:46:11,850][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:46:11,852][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:46:11,858][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:46:11,858][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:46:11,858][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:46:11,859][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:46:11,859][train][INFO] - Logging hyperparameters!
[2022-09-23 23:46:11,862][train][INFO] - Starting training!
[2022-09-23 23:46:11,863][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:46:11,864][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:46:11,864][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:46:11,865][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:46:19,663][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:46:19,883][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:47:03,500][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:47:03,501][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:47:03,584][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:47:05,068][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:47:06,412][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:47:06,414][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:47:06,414][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:47:06,416][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:47:06,422][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:47:06,422][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:47:06,422][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:47:06,423][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:47:06,423][train][INFO] - Logging hyperparameters!
[2022-09-23 23:47:06,427][train][INFO] - Starting training!
[2022-09-23 23:47:06,428][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:47:06,429][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:47:06,429][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:47:06,429][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:47:14,196][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:47:14,427][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:49:36,723][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:49:36,724][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:49:36,808][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:49:38,278][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:49:39,602][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:49:39,603][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:49:39,604][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:49:39,606][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:49:39,611][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:49:39,612][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:49:39,612][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:49:39,612][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:49:39,613][train][INFO] - Logging hyperparameters!
[2022-09-23 23:49:39,616][train][INFO] - Starting training!
[2022-09-23 23:49:39,617][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:49:39,618][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:49:39,618][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:49:39,618][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:49:47,307][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:49:47,527][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:51:39,170][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:51:39,170][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:51:39,253][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:51:40,721][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:51:42,027][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:51:42,029][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:51:42,030][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:51:42,032][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:51:42,037][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:51:42,037][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:51:42,038][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:51:42,038][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:51:42,038][train][INFO] - Logging hyperparameters!
[2022-09-23 23:51:42,042][train][INFO] - Starting training!
[2022-09-23 23:51:42,043][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:51:42,043][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:51:42,044][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:51:42,044][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:51:49,784][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:51:50,004][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:53:47,850][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:53:47,850][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:53:47,934][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:53:49,404][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:53:50,701][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:53:50,702][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:53:50,703][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:53:50,705][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:53:50,711][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:53:50,711][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:53:50,711][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:53:50,712][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:53:50,712][train][INFO] - Logging hyperparameters!
[2022-09-23 23:53:50,716][train][INFO] - Starting training!
[2022-09-23 23:53:50,716][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:53:50,717][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:53:50,718][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:53:50,718][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:53:58,490][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:53:58,712][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:54:23,832][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:54:23,832][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:54:23,916][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:54:25,389][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:54:26,683][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:54:26,684][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:54:26,685][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:54:26,687][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:54:26,693][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:54:26,693][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:54:26,693][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:54:26,694][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:54:26,694][train][INFO] - Logging hyperparameters!
[2022-09-23 23:54:26,697][train][INFO] - Starting training!
[2022-09-23 23:54:26,698][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:54:26,699][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:54:26,699][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:54:26,699][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:54:34,421][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:54:34,641][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:55:35,150][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:55:35,150][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:55:35,232][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:55:36,705][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:55:38,010][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:55:38,011][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:55:38,012][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:55:38,014][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:55:38,020][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:55:38,020][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:55:38,020][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:55:38,021][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:55:38,021][train][INFO] - Logging hyperparameters!
[2022-09-23 23:55:38,024][train][INFO] - Starting training!
[2022-09-23 23:55:38,025][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:55:38,026][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:55:38,026][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:55:38,027][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:55:45,780][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:55:46,003][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:56:34,894][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:56:34,895][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:56:34,978][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:56:36,452][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:56:37,764][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:56:37,765][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:56:37,766][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:56:37,768][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:56:37,774][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:56:37,774][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:56:37,774][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:56:37,775][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:56:37,775][train][INFO] - Logging hyperparameters!
[2022-09-23 23:56:37,778][train][INFO] - Starting training!
[2022-09-23 23:56:37,779][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:56:37,780][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:56:37,780][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:56:37,781][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:56:45,581][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:56:45,806][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-23 23:57:08,083][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-23 23:57:08,083][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-23 23:57:08,167][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-23 23:57:09,630][train][INFO] - Instantiating model <models.model.Model>
[2022-09-23 23:57:10,985][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-23 23:57:10,986][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-23 23:57:10,987][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-23 23:57:10,989][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-23 23:57:10,994][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-23 23:57:10,995][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-23 23:57:10,995][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-23 23:57:10,995][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-23 23:57:10,996][train][INFO] - Logging hyperparameters!
[2022-09-23 23:57:10,999][train][INFO] - Starting training!
[2022-09-23 23:57:11,000][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-23 23:57:11,001][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-23 23:57:11,001][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-23 23:57:11,001][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-23 23:57:18,794][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-23 23:57:19,016][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:00:16,257][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:00:16,258][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:00:16,341][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:00:17,811][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:00:19,155][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:00:19,156][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:00:19,157][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:00:19,159][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:00:19,165][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:00:19,165][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:00:19,165][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:00:19,166][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:00:19,166][train][INFO] - Logging hyperparameters!
[2022-09-24 00:00:19,169][train][INFO] - Starting training!
[2022-09-24 00:00:19,170][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:00:19,171][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:00:19,171][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:00:19,172][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:00:26,888][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:00:27,111][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:00:43,254][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:00:43,254][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:00:43,338][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:00:44,800][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:00:46,120][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:00:46,122][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:00:46,123][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:00:46,125][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:00:46,130][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:00:46,131][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:00:46,131][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:00:46,131][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:00:46,132][train][INFO] - Logging hyperparameters!
[2022-09-24 00:00:46,135][train][INFO] - Starting training!
[2022-09-24 00:00:46,136][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:00:46,137][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:00:46,137][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:00:46,137][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:00:53,924][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:00:54,147][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:01:36,553][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:01:36,554][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:01:36,639][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:01:38,107][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:01:39,445][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:01:39,446][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:01:39,447][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:01:39,449][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:01:39,455][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:01:39,455][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:01:39,455][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:01:39,456][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:01:39,456][train][INFO] - Logging hyperparameters!
[2022-09-24 00:01:39,459][train][INFO] - Starting training!
[2022-09-24 00:01:39,460][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:01:39,461][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:01:39,461][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:01:39,461][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:01:47,250][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:01:47,474][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:02:21,631][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:02:21,632][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:02:21,716][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:02:23,190][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:02:24,514][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:02:24,516][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:02:24,517][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:02:24,519][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:02:24,524][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:02:24,525][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:02:24,525][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:02:24,526][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:02:24,526][train][INFO] - Logging hyperparameters!
[2022-09-24 00:02:24,529][train][INFO] - Starting training!
[2022-09-24 00:02:24,530][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:02:24,531][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:02:24,531][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:02:24,531][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:02:32,284][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:02:32,506][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:02:43,637][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:02:43,637][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:02:43,720][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:02:45,193][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:02:46,503][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:02:46,505][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:02:46,506][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:02:46,507][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:02:46,513][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:02:46,513][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:02:46,514][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:02:46,514][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:02:46,515][train][INFO] - Logging hyperparameters!
[2022-09-24 00:02:46,518][train][INFO] - Starting training!
[2022-09-24 00:02:46,519][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:02:46,520][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:02:46,520][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:02:46,520][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:02:54,272][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:02:54,494][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:03:54,632][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:03:54,632][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:03:54,716][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:03:56,179][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:03:57,518][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:03:57,520][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:03:57,521][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:03:57,522][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:03:57,528][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:03:57,528][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:03:57,529][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:03:57,529][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:03:57,529][train][INFO] - Logging hyperparameters!
[2022-09-24 00:03:57,533][train][INFO] - Starting training!
[2022-09-24 00:03:57,534][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:03:57,534][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:03:57,535][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:03:57,535][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:04:05,229][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:04:05,449][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:04:26,561][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:04:26,561][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:04:26,646][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:04:28,107][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:04:29,467][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:04:29,469][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:04:29,470][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:04:29,471][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:04:29,477][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:04:29,477][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:04:29,478][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:04:29,478][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:04:29,479][train][INFO] - Logging hyperparameters!
[2022-09-24 00:04:29,482][train][INFO] - Starting training!
[2022-09-24 00:04:29,483][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:04:29,484][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:04:29,484][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:04:29,484][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:04:37,303][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:04:37,526][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:05:02,918][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:05:02,918][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:05:03,002][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:05:04,468][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:05:05,795][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:05:05,797][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:05:05,798][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:05:05,799][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:05:05,805][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:05:05,805][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:05:05,805][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:05:05,806][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:05:05,806][train][INFO] - Logging hyperparameters!
[2022-09-24 00:05:05,810][train][INFO] - Starting training!
[2022-09-24 00:05:05,810][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:05:05,811][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:05:05,812][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:05:05,812][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:05:13,585][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:05:13,809][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:13:11,654][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:13:11,654][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:13:11,738][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:13:13,204][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:13:50,383][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:13:50,383][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:13:50,466][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:13:51,936][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:14:12,943][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:14:12,943][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:14:13,026][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:14:14,490][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:14:15,841][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:14:15,843][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:14:15,844][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:14:15,845][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:14:15,851][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:14:15,851][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:14:15,851][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:14:15,852][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:14:15,852][train][INFO] - Logging hyperparameters!
[2022-09-24 00:14:15,855][train][INFO] - Starting training!
[2022-09-24 00:14:15,856][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:14:15,857][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:14:15,858][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:14:15,858][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:14:23,614][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:14:23,834][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:15:01,170][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:15:01,171][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:15:01,253][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:15:02,719][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:15:04,053][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:15:04,055][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:15:04,056][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:15:04,058][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:15:04,063][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:15:04,063][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:15:04,064][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:15:04,064][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:15:04,065][train][INFO] - Logging hyperparameters!
[2022-09-24 00:15:04,068][train][INFO] - Starting training!
[2022-09-24 00:15:04,069][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:15:04,070][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:15:04,070][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:15:04,070][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:15:11,834][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:15:12,057][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:15:58,755][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:15:58,755][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:15:58,839][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:16:00,324][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:16:01,680][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:16:01,681][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:16:01,682][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:16:01,684][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:16:01,690][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:16:01,690][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:16:01,690][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:16:01,691][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:16:01,691][train][INFO] - Logging hyperparameters!
[2022-09-24 00:16:01,694][train][INFO] - Starting training!
[2022-09-24 00:16:01,695][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:16:01,696][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:16:01,696][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:16:01,696][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:16:09,452][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:16:09,688][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:17:22,739][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:17:22,740][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:17:22,823][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:17:24,301][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:17:25,631][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:17:25,633][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:17:25,634][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:17:25,635][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:17:25,641][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:17:25,641][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:17:25,642][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:17:25,642][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:17:25,642][train][INFO] - Logging hyperparameters!
[2022-09-24 00:17:25,646][train][INFO] - Starting training!
[2022-09-24 00:17:25,647][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:17:25,647][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:17:25,648][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:17:25,648][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:17:33,435][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:17:33,658][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-24 00:18:56,195][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-24 00:18:56,195][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-24 00:18:56,279][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-24 00:18:57,753][train][INFO] - Instantiating model <models.model.Model>
[2022-09-24 00:18:59,084][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-24 00:18:59,086][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-24 00:18:59,087][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-24 00:18:59,088][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-24 00:18:59,094][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-24 00:18:59,094][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-24 00:18:59,094][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-24 00:18:59,095][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-24 00:18:59,095][train][INFO] - Logging hyperparameters!
[2022-09-24 00:18:59,099][train][INFO] - Starting training!
[2022-09-24 00:18:59,099][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-24 00:18:59,100][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-24 00:18:59,101][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-24 00:18:59,101][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-24 00:19:06,863][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-24 00:19:07,085][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:22:46,473][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:22:46,488][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:22:46,651][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:23:06,895][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:23:09,079][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:23:09,080][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:23:09,081][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:23:09,083][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:23:09,089][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:23:09,089][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:23:09,089][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:23:09,090][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:23:09,090][train][INFO] - Logging hyperparameters!
[2022-09-25 17:23:09,093][train][INFO] - Starting training!
[2022-09-25 17:23:09,094][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:23:09,095][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:23:09,095][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:23:09,095][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:23:17,339][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:23:17,557][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:27:17,229][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:27:17,229][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:27:17,313][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:27:18,831][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:27:20,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:27:20,134][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:27:20,135][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:27:20,137][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:27:20,142][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:27:20,142][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:27:20,143][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:27:20,143][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:27:20,143][train][INFO] - Logging hyperparameters!
[2022-09-25 17:27:20,147][train][INFO] - Starting training!
[2022-09-25 17:27:20,148][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:27:20,148][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:27:20,149][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:27:20,149][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:27:27,822][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:27:28,040][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:28:36,676][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:28:36,676][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:28:36,760][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:28:38,217][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:28:52,351][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:28:52,351][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:28:52,434][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:28:53,889][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:29:25,764][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:29:25,765][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:29:25,848][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:29:27,308][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:29:28,749][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:29:28,750][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:29:28,751][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:29:28,753][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:29:28,758][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:29:28,759][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:29:28,759][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:29:28,760][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:29:28,760][train][INFO] - Logging hyperparameters!
[2022-09-25 17:29:28,763][train][INFO] - Starting training!
[2022-09-25 17:29:28,764][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:29:28,765][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:29:28,765][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:29:28,765][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:29:36,455][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:29:36,675][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:45:39,394][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:45:39,395][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:45:39,493][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:45:53,074][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:45:54,527][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:45:54,529][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:45:54,530][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:45:54,532][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:45:54,537][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:45:54,538][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:45:54,538][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:45:54,538][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:45:54,539][train][INFO] - Logging hyperparameters!
[2022-09-25 17:45:54,542][train][INFO] - Starting training!
[2022-09-25 17:45:54,543][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:45:54,544][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:45:54,544][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:45:54,545][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:46:02,332][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:46:02,553][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:46:28,602][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:46:28,603][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:46:28,687][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:46:30,146][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:46:31,481][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:46:31,483][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:46:31,484][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:46:31,486][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:46:31,491][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:46:31,492][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:46:31,492][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:46:31,492][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:46:31,493][train][INFO] - Logging hyperparameters!
[2022-09-25 17:46:31,496][train][INFO] - Starting training!
[2022-09-25 17:46:31,497][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:46:31,498][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:46:31,498][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:46:31,499][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:46:39,259][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:46:39,482][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:49:49,348][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:49:49,349][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:49:49,432][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:49:50,889][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:49:52,198][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:49:52,200][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:49:52,200][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:49:52,202][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:49:52,208][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:49:52,208][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:49:52,209][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:49:52,209][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:49:52,210][train][INFO] - Logging hyperparameters!
[2022-09-25 17:49:52,213][train][INFO] - Starting training!
[2022-09-25 17:49:52,214][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:49:52,215][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:49:52,215][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:49:52,215][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:49:59,909][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:50:00,130][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:50:28,199][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:50:28,199][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:50:28,283][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:50:29,739][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:50:31,049][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:50:31,050][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:50:31,051][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:50:31,053][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:50:31,059][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:50:31,059][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:50:31,059][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:50:31,060][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:50:31,060][train][INFO] - Logging hyperparameters!
[2022-09-25 17:50:31,063][train][INFO] - Starting training!
[2022-09-25 17:50:31,064][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:50:31,066][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:50:31,066][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:50:31,066][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:50:38,844][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:50:39,064][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:51:21,572][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:51:21,572][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:51:21,656][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:51:23,119][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:51:24,428][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:51:24,430][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:51:24,430][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:51:24,432][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:51:24,438][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:51:24,438][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:51:24,438][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:51:24,439][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:51:24,439][train][INFO] - Logging hyperparameters!
[2022-09-25 17:51:24,442][train][INFO] - Starting training!
[2022-09-25 17:51:24,443][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:51:24,444][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:51:24,444][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:51:24,445][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:51:32,146][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:51:32,367][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:53:06,686][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:53:06,686][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:53:06,769][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:53:08,225][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:53:09,566][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:53:09,568][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:53:09,568][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:53:09,570][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:53:09,576][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:53:09,576][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:53:09,576][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:53:09,577][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:53:09,577][train][INFO] - Logging hyperparameters!
[2022-09-25 17:53:09,581][train][INFO] - Starting training!
[2022-09-25 17:53:09,581][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:53:09,582][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:53:09,583][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:53:09,583][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:53:17,294][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:53:17,514][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:59:02,855][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:59:02,856][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:59:02,939][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:59:04,397][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:59:05,699][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:59:05,700][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:59:05,701][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:59:05,703][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:59:05,708][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:59:05,709][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:59:05,709][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:59:05,709][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:59:05,710][train][INFO] - Logging hyperparameters!
[2022-09-25 17:59:05,713][train][INFO] - Starting training!
[2022-09-25 17:59:05,714][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:59:05,715][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:59:05,715][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:59:05,715][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:59:13,421][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:59:13,640][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 17:59:38,061][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 17:59:38,061][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 17:59:38,145][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 17:59:39,607][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 17:59:40,924][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 17:59:40,925][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 17:59:40,926][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 17:59:40,928][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 17:59:40,934][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 17:59:40,934][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 17:59:40,934][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 17:59:40,935][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 17:59:40,935][train][INFO] - Logging hyperparameters!
[2022-09-25 17:59:40,938][train][INFO] - Starting training!
[2022-09-25 17:59:40,939][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 17:59:40,940][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 17:59:40,940][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 17:59:40,941][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 17:59:48,619][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 17:59:48,839][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:00:34,492][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:00:34,492][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:00:34,577][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:00:36,043][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:01:06,004][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:01:06,005][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:01:06,088][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:01:07,542][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:01:08,861][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:01:08,862][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:01:08,863][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:01:08,865][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:01:08,870][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:01:08,871][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:01:08,871][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:01:08,872][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:01:08,873][train][INFO] - Logging hyperparameters!
[2022-09-25 18:01:08,876][train][INFO] - Starting training!
[2022-09-25 18:01:08,877][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:01:08,878][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:01:08,878][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:01:08,878][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:01:16,593][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:01:16,814][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:02:24,704][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:02:24,704][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:02:24,788][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:02:26,248][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:02:27,580][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:02:27,581][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:02:27,582][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:02:27,584][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:02:27,590][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:02:27,590][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:02:27,590][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:02:27,591][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:02:27,591][train][INFO] - Logging hyperparameters!
[2022-09-25 18:02:27,594][train][INFO] - Starting training!
[2022-09-25 18:02:27,595][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:02:27,596][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:02:27,596][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:02:27,596][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:02:35,335][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:02:35,559][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:02:44,612][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:02:44,612][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:02:44,696][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:02:46,158][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:02:47,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:02:47,482][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:02:47,482][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:02:47,484][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:02:47,490][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:02:47,490][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:02:47,490][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:02:47,491][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:02:47,491][train][INFO] - Logging hyperparameters!
[2022-09-25 18:02:47,494][train][INFO] - Starting training!
[2022-09-25 18:02:47,495][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:02:47,496][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:02:47,496][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:02:47,497][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:02:55,231][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:02:55,455][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:03:18,480][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:03:18,481][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:03:18,564][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:03:20,028][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:03:21,343][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:03:21,345][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:03:21,346][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:03:21,347][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:03:21,353][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:03:21,353][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:03:21,354][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:03:21,354][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:03:21,354][train][INFO] - Logging hyperparameters!
[2022-09-25 18:03:21,358][train][INFO] - Starting training!
[2022-09-25 18:03:21,359][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:03:21,360][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:03:21,360][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:03:21,360][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:03:29,110][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:03:29,331][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:05:53,901][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:05:53,901][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:05:53,984][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:05:55,472][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:05:56,918][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:05:56,920][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:05:56,921][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:05:56,923][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:05:56,928][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:05:56,929][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:05:56,929][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:05:56,929][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:05:56,930][train][INFO] - Logging hyperparameters!
[2022-09-25 18:05:56,933][train][INFO] - Starting training!
[2022-09-25 18:05:56,934][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:05:56,935][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:05:56,935][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:05:56,935][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:06:04,671][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:06:04,890][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:10:39,695][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:10:39,696][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:10:39,779][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:10:41,236][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:10:42,557][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:10:42,558][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:10:42,559][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:10:42,561][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:10:42,567][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:10:42,567][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:10:42,567][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:10:42,568][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:10:42,568][train][INFO] - Logging hyperparameters!
[2022-09-25 18:10:42,571][train][INFO] - Starting training!
[2022-09-25 18:10:42,572][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:10:42,573][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:10:42,573][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:10:42,574][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:10:50,300][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:10:50,521][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:11:46,305][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:11:46,305][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:11:46,391][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:11:47,847][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:11:49,167][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:11:49,169][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:11:49,170][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:11:49,171][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:11:49,177][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:11:49,177][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:11:49,178][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:11:49,178][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:11:49,178][train][INFO] - Logging hyperparameters!
[2022-09-25 18:11:49,182][train][INFO] - Starting training!
[2022-09-25 18:11:49,182][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:11:49,183][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:11:49,184][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:11:49,184][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:11:56,948][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:11:57,167][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:12:14,124][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:12:14,125][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:12:14,208][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:12:15,674][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:12:17,005][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:12:17,007][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:12:17,008][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:12:17,010][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:12:17,015][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:12:17,015][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:12:17,016][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:12:17,016][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:12:17,017][train][INFO] - Logging hyperparameters!
[2022-09-25 18:12:17,020][train][INFO] - Starting training!
[2022-09-25 18:12:17,021][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:12:17,022][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:12:17,022][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:12:17,022][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:12:24,745][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:12:24,975][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:13:03,530][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:13:03,531][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:13:03,615][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:13:05,073][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:13:06,398][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:13:06,400][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:13:06,401][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:13:06,403][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:13:06,408][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:13:06,408][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:13:06,409][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:13:06,409][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:13:06,410][train][INFO] - Logging hyperparameters!
[2022-09-25 18:13:06,413][train][INFO] - Starting training!
[2022-09-25 18:13:06,414][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:13:06,415][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:13:06,415][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:13:06,415][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:13:14,122][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:13:14,343][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:14:37,414][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:14:37,414][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:14:37,498][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:14:38,959][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:14:40,261][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:14:40,263][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:14:40,263][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:14:40,265][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:14:40,271][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:14:40,271][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:14:40,271][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:14:40,272][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:14:40,272][train][INFO] - Logging hyperparameters!
[2022-09-25 18:14:40,275][train][INFO] - Starting training!
[2022-09-25 18:14:40,276][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:14:40,277][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:14:40,277][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:14:40,278][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:14:47,967][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:14:48,193][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:15:11,091][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:15:11,092][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:15:11,175][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:15:12,635][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:15:13,967][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:15:13,968][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:15:13,969][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:15:13,971][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:15:13,976][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:15:13,977][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:15:13,977][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:15:13,978][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:15:13,978][train][INFO] - Logging hyperparameters!
[2022-09-25 18:15:13,981][train][INFO] - Starting training!
[2022-09-25 18:15:13,982][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:15:13,983][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:15:13,983][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:15:13,984][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:15:21,778][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:15:21,999][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:15:45,105][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:15:45,105][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:15:45,189][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:15:46,644][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:15:47,983][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:15:47,985][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:15:47,985][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:15:47,987][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:15:47,993][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:15:47,993][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:15:47,993][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:15:47,994][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:15:47,994][train][INFO] - Logging hyperparameters!
[2022-09-25 18:15:47,997][train][INFO] - Starting training!
[2022-09-25 18:15:47,998][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:15:47,999][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:15:48,000][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:15:48,000][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:15:55,740][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:15:55,963][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:16:48,183][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:16:48,183][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:16:48,267][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:16:49,739][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:16:51,080][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:16:51,082][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:16:51,082][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:16:51,084][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:16:51,090][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:16:51,090][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:16:51,090][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:16:51,091][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:16:51,091][train][INFO] - Logging hyperparameters!
[2022-09-25 18:16:51,094][train][INFO] - Starting training!
[2022-09-25 18:16:51,095][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:16:51,096][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:16:51,096][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:16:51,097][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:16:58,884][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:16:59,109][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:17:21,753][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:17:21,753][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:17:21,837][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:17:23,313][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:17:24,686][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:17:24,688][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:17:24,689][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:17:24,690][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:17:24,696][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:17:24,696][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:17:24,697][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:17:24,697][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:17:24,698][train][INFO] - Logging hyperparameters!
[2022-09-25 18:17:24,701][train][INFO] - Starting training!
[2022-09-25 18:17:24,702][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:17:24,703][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:17:24,703][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:17:24,704][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:17:32,529][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:17:32,751][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:19:12,214][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:19:12,214][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:19:12,298][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:19:13,755][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:19:15,066][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:19:15,068][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:19:15,069][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:19:15,071][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:19:15,076][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:19:15,076][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:19:15,077][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:19:15,077][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:19:15,078][train][INFO] - Logging hyperparameters!
[2022-09-25 18:19:15,081][train][INFO] - Starting training!
[2022-09-25 18:19:15,082][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:19:15,083][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:19:15,083][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:19:15,083][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:19:22,769][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:19:22,989][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:20:20,814][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:20:20,814][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:20:20,898][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:20:22,371][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:20:23,732][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:20:23,733][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:20:23,734][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:20:23,736][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:20:23,742][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:20:23,742][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:20:23,742][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:20:23,743][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:20:23,743][train][INFO] - Logging hyperparameters!
[2022-09-25 18:20:23,746][train][INFO] - Starting training!
[2022-09-25 18:20:23,747][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:20:23,748][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:20:23,748][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:20:23,749][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:20:31,451][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:20:31,673][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:28:38,181][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:28:38,183][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:28:38,315][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:28:57,107][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:28:59,064][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:28:59,065][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:28:59,066][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:28:59,068][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:28:59,073][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:28:59,074][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:28:59,074][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:28:59,075][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:28:59,075][train][INFO] - Logging hyperparameters!
[2022-09-25 18:28:59,078][train][INFO] - Starting training!
[2022-09-25 18:28:59,079][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:28:59,081][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:28:59,081][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:28:59,081][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:29:08,137][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:29:08,361][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:29:41,824][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:29:41,825][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:29:41,909][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:29:45,075][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:29:46,454][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:29:46,455][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:29:46,456][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:29:46,458][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:29:46,464][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:29:46,464][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:29:46,464][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:29:46,465][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:29:46,465][train][INFO] - Logging hyperparameters!
[2022-09-25 18:29:46,468][train][INFO] - Starting training!
[2022-09-25 18:29:46,469][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:29:46,470][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:29:46,470][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:29:46,470][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:29:54,170][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:29:54,391][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:30:53,155][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:30:53,156][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:30:53,239][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:30:55,190][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:30:56,559][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:30:56,560][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:30:56,561][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:30:56,563][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:30:56,568][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:30:56,569][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:30:56,569][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:30:56,570][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:30:56,570][train][INFO] - Logging hyperparameters!
[2022-09-25 18:30:56,573][train][INFO] - Starting training!
[2022-09-25 18:30:56,574][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:30:56,575][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:30:56,575][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:30:56,575][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:31:04,321][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:31:04,536][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:31:22,340][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:31:22,340][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:31:22,423][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:31:23,919][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:31:25,232][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:31:25,233][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:31:25,234][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:31:25,236][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:31:25,242][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:31:25,242][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:31:25,242][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:31:25,243][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:31:25,243][train][INFO] - Logging hyperparameters!
[2022-09-25 18:31:25,246][train][INFO] - Starting training!
[2022-09-25 18:31:25,247][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:31:25,248][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:31:25,248][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:31:25,249][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:31:32,913][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:31:33,126][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:32:08,540][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:32:08,541][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:32:08,624][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:32:10,109][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:32:11,437][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:32:11,439][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:32:11,439][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:32:11,441][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:32:11,447][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:32:11,447][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:32:11,447][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:32:11,448][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:32:11,448][train][INFO] - Logging hyperparameters!
[2022-09-25 18:32:11,451][train][INFO] - Starting training!
[2022-09-25 18:32:11,452][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:32:11,454][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:32:11,454][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:32:11,454][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:32:19,178][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:32:19,394][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:32:53,722][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:32:53,722][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:32:53,806][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:32:55,408][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:32:56,724][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:32:56,725][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:32:56,726][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:32:56,728][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:32:56,733][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:32:56,734][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:32:56,734][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:32:56,735][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:32:56,735][train][INFO] - Logging hyperparameters!
[2022-09-25 18:32:56,738][train][INFO] - Starting training!
[2022-09-25 18:32:56,739][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:32:56,740][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:32:56,740][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:32:56,740][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:33:04,506][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:33:04,727][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:38:34,348][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:38:34,349][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:38:34,432][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:38:36,275][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:38:55,728][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:38:55,729][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:38:55,812][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:38:57,300][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:38:58,647][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:38:58,649][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:38:58,650][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:38:58,651][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:38:58,657][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:38:58,657][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:38:58,658][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:38:58,658][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:38:58,658][train][INFO] - Logging hyperparameters!
[2022-09-25 18:38:58,662][train][INFO] - Starting training!
[2022-09-25 18:38:58,663][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:38:58,663][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:38:58,664][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:38:58,664][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:39:06,356][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:39:06,571][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:39:29,664][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:39:29,664][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:39:29,748][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:39:31,357][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:39:32,708][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:39:32,710][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:39:32,711][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:39:32,713][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:39:32,718][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:39:32,719][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:39:32,719][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:39:32,719][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:39:32,720][train][INFO] - Logging hyperparameters!
[2022-09-25 18:39:32,723][train][INFO] - Starting training!
[2022-09-25 18:39:32,724][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:39:32,725][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:39:32,725][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:39:32,725][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:39:40,391][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:39:40,604][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:40:06,912][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:40:06,913][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:40:06,996][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:40:08,544][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:40:25,812][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:40:25,813][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:40:25,896][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:40:27,442][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:40:28,789][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:40:28,790][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:40:28,791][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:40:28,793][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:40:28,799][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:40:28,799][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:40:28,799][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:40:28,800][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:40:28,800][train][INFO] - Logging hyperparameters!
[2022-09-25 18:40:28,803][train][INFO] - Starting training!
[2022-09-25 18:40:28,804][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:40:28,805][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:40:28,805][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:40:28,805][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:40:36,572][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:40:36,793][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:41:20,182][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:41:20,182][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:41:20,266][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:41:22,009][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:41:23,375][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:41:23,376][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:41:23,377][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:41:23,379][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:41:23,385][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:41:23,385][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:41:23,385][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:41:23,386][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:41:23,386][train][INFO] - Logging hyperparameters!
[2022-09-25 18:41:23,389][train][INFO] - Starting training!
[2022-09-25 18:41:23,390][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:41:23,391][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:41:23,391][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:41:23,392][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:41:31,164][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:41:31,385][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:45:00,906][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:45:00,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:45:00,989][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:45:02,567][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:45:03,917][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:45:03,919][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:45:03,919][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:45:03,921][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:45:03,927][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:45:03,927][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:45:03,927][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:45:03,928][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:45:03,928][train][INFO] - Logging hyperparameters!
[2022-09-25 18:45:03,931][train][INFO] - Starting training!
[2022-09-25 18:45:03,932][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:45:03,934][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:45:03,934][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:45:03,934][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:45:11,614][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:45:11,835][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:45:36,720][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:45:36,721][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:45:36,804][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:45:38,503][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:45:39,839][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:45:39,841][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:45:39,841][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:45:39,843][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:45:39,849][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:45:39,849][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:45:39,849][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:45:39,850][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:45:39,850][train][INFO] - Logging hyperparameters!
[2022-09-25 18:45:39,853][train][INFO] - Starting training!
[2022-09-25 18:45:39,854][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:45:39,855][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:45:39,855][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:45:39,856][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:45:47,549][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:45:47,765][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:46:33,620][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:46:33,621][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:46:33,705][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:46:35,198][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:46:36,565][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:46:36,566][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:46:36,567][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:46:36,569][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:46:36,574][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:46:36,575][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:46:36,575][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:46:36,576][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:46:36,576][train][INFO] - Logging hyperparameters!
[2022-09-25 18:46:36,579][train][INFO] - Starting training!
[2022-09-25 18:46:36,580][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:46:36,581][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:46:36,581][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:46:36,581][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:46:44,315][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:46:44,534][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:47:29,364][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:47:29,364][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:47:29,448][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:47:30,950][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:47:32,360][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:47:32,362][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:47:32,363][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:47:32,364][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:47:32,370][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:47:32,370][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:47:32,370][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:47:32,371][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:47:32,371][train][INFO] - Logging hyperparameters!
[2022-09-25 18:47:32,375][train][INFO] - Starting training!
[2022-09-25 18:47:32,375][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:47:32,376][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:47:32,377][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:47:32,377][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:47:40,136][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:47:40,371][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:50:36,641][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:50:36,642][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:50:36,725][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:50:38,449][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:50:39,812][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:50:39,813][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:50:39,814][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:50:39,816][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:50:39,822][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:50:39,822][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:50:39,822][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:50:39,823][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:50:39,823][train][INFO] - Logging hyperparameters!
[2022-09-25 18:50:39,826][train][INFO] - Starting training!
[2022-09-25 18:50:39,827][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:50:39,828][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:50:39,828][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:50:39,828][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:50:47,584][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:50:47,800][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:53:50,838][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:53:50,852][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:53:50,935][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:53:52,399][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:53:53,731][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:53:53,732][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:53:53,733][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:53:53,735][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:53:53,741][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:53:53,741][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:53:53,741][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:53:53,742][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:53:53,742][train][INFO] - Logging hyperparameters!
[2022-09-25 18:53:53,745][train][INFO] - Starting training!
[2022-09-25 18:53:53,746][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:53:53,747][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:53:53,747][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:53:53,747][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:54:01,427][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:54:01,641][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:54:21,762][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:54:21,762][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:54:21,845][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:54:23,301][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:54:24,631][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:54:24,633][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:54:24,633][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:54:24,635][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:54:24,641][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:54:24,641][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:54:24,642][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:54:24,643][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:54:24,643][train][INFO] - Logging hyperparameters!
[2022-09-25 18:54:24,646][train][INFO] - Starting training!
[2022-09-25 18:54:24,647][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:54:24,648][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:54:24,648][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:54:24,648][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:54:32,369][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:54:32,582][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:55:22,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:55:22,569][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:55:22,653][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:55:24,144][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:55:25,521][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:55:25,522][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:55:25,523][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:55:25,525][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:55:25,531][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:55:25,531][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:55:25,531][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:55:25,532][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:55:25,532][train][INFO] - Logging hyperparameters!
[2022-09-25 18:55:25,535][train][INFO] - Starting training!
[2022-09-25 18:55:25,536][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:55:25,537][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:55:25,537][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:55:25,563][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:55:33,328][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:55:33,545][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:55:56,554][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:55:56,555][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:55:56,639][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:55:59,867][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:56:01,364][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:56:01,365][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:56:01,366][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:56:01,368][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:56:01,373][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:56:01,374][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:56:01,374][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:56:01,375][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:56:01,375][train][INFO] - Logging hyperparameters!
[2022-09-25 18:56:01,378][train][INFO] - Starting training!
[2022-09-25 18:56:01,379][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:56:01,380][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:56:01,380][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:56:01,380][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:56:09,157][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:56:09,374][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:57:14,574][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:57:14,575][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:57:14,659][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:57:16,261][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:57:17,623][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:57:17,625][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:57:17,626][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:57:17,627][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:57:17,633][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:57:17,633][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:57:17,634][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:57:17,634][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:57:17,634][train][INFO] - Logging hyperparameters!
[2022-09-25 18:57:17,638][train][INFO] - Starting training!
[2022-09-25 18:57:17,639][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:57:17,640][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:57:17,640][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:57:17,640][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:57:25,408][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:57:25,625][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:58:24,876][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:58:24,877][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:58:24,960][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:58:26,516][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:58:27,864][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:58:27,866][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:58:27,867][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:58:27,869][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:58:27,874][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:58:27,874][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:58:27,875][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:58:27,875][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:58:27,875][train][INFO] - Logging hyperparameters!
[2022-09-25 18:58:27,879][train][INFO] - Starting training!
[2022-09-25 18:58:27,880][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:58:27,881][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:58:27,881][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:58:27,881][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:58:35,532][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:58:35,746][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:59:16,243][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:59:16,244][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:59:16,327][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:59:17,787][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:59:19,121][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:59:19,123][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:59:19,124][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:59:19,126][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:59:19,132][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:59:19,132][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:59:19,133][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:59:19,133][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:59:19,133][train][INFO] - Logging hyperparameters!
[2022-09-25 18:59:19,137][train][INFO] - Starting training!
[2022-09-25 18:59:19,138][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:59:19,138][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:59:19,139][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:59:19,139][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 18:59:26,856][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 18:59:27,072][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 18:59:53,214][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 18:59:53,215][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 18:59:53,299][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 18:59:54,775][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 18:59:56,135][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 18:59:56,136][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 18:59:56,137][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 18:59:56,139][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 18:59:56,145][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 18:59:56,145][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 18:59:56,145][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 18:59:56,146][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 18:59:56,146][train][INFO] - Logging hyperparameters!
[2022-09-25 18:59:56,149][train][INFO] - Starting training!
[2022-09-25 18:59:56,150][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 18:59:56,179][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 18:59:56,179][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 18:59:56,180][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:00:03,947][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:00:04,168][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:00:33,026][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:00:33,026][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:00:33,110][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:00:34,572][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:00:35,896][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:00:35,897][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:00:35,898][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:00:35,900][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:00:35,906][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:00:35,906][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:00:35,906][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:00:35,907][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:00:35,907][train][INFO] - Logging hyperparameters!
[2022-09-25 19:00:35,910][train][INFO] - Starting training!
[2022-09-25 19:00:35,911][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:00:35,912][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:00:35,912][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:00:35,912][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:00:43,582][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:00:43,796][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:00:59,738][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:00:59,738][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:00:59,822][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:01:01,379][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:01:02,894][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:01:02,896][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:01:02,897][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:01:02,898][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:01:02,904][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:01:02,904][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:01:02,905][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:01:02,905][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:01:02,905][train][INFO] - Logging hyperparameters!
[2022-09-25 19:01:02,909][train][INFO] - Starting training!
[2022-09-25 19:01:02,910][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:01:02,911][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:01:02,911][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:01:02,911][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:01:10,657][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:01:10,872][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:01:31,556][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:01:31,557][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:01:31,640][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:01:34,703][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:01:36,059][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:01:36,060][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:01:36,061][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:01:36,063][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:01:36,069][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:01:36,069][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:01:36,069][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:01:36,070][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:01:36,070][train][INFO] - Logging hyperparameters!
[2022-09-25 19:01:36,073][train][INFO] - Starting training!
[2022-09-25 19:01:36,074][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:01:36,075][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:01:36,075][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:01:36,075][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:01:43,829][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:01:44,044][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:08:55,601][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:08:55,601][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:08:55,686][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:08:58,392][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:08:59,729][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:08:59,731][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:08:59,745][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:08:59,747][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:08:59,752][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:08:59,753][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:08:59,753][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:08:59,753][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:08:59,754][train][INFO] - Logging hyperparameters!
[2022-09-25 19:08:59,757][train][INFO] - Starting training!
[2022-09-25 19:08:59,758][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:08:59,759][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:08:59,759][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:08:59,759][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:09:07,579][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:09:07,796][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:09:29,199][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:09:29,199][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:09:29,287][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:09:32,070][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:09:33,417][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:09:33,418][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:09:33,419][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:09:33,421][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:09:33,427][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:09:33,427][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:09:33,427][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:09:33,428][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:09:33,428][train][INFO] - Logging hyperparameters!
[2022-09-25 19:09:33,458][train][INFO] - Starting training!
[2022-09-25 19:09:33,459][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:09:33,460][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:09:33,460][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:09:33,460][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:09:41,197][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:09:41,413][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:10:56,610][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:10:56,610][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:10:56,694][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:10:58,413][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:10:59,837][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:10:59,839][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:10:59,839][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:10:59,841][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:10:59,847][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:10:59,847][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:10:59,847][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:10:59,848][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:10:59,848][train][INFO] - Logging hyperparameters!
[2022-09-25 19:10:59,851][train][INFO] - Starting training!
[2022-09-25 19:10:59,852][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:10:59,853][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:10:59,853][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:10:59,854][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:11:07,684][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:11:07,906][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:12:34,577][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:12:34,578][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:12:34,663][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:12:37,437][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:12:38,897][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:12:38,899][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:12:38,900][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:12:38,901][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:12:38,907][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:12:38,907][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:12:38,908][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:12:38,908][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:12:38,908][train][INFO] - Logging hyperparameters!
[2022-09-25 19:12:38,912][train][INFO] - Starting training!
[2022-09-25 19:12:38,913][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:12:38,914][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:12:38,914][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:12:38,914][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:12:46,609][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:12:46,824][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:13:17,395][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:13:17,395][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:13:17,479][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:13:19,019][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:13:20,359][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:13:20,360][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:13:20,361][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:13:20,363][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:13:20,368][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:13:20,369][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:13:20,369][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:13:20,370][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:13:20,370][train][INFO] - Logging hyperparameters!
[2022-09-25 19:13:20,373][train][INFO] - Starting training!
[2022-09-25 19:13:20,374][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:13:20,375][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:13:20,375][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:13:20,375][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:13:28,065][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:13:28,280][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:14:39,615][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:14:39,615][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:14:39,699][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:14:41,228][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:14:42,583][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:14:42,585][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:14:42,586][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:14:42,588][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:14:42,593][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:14:42,593][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:14:42,594][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:14:42,594][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:14:42,595][train][INFO] - Logging hyperparameters!
[2022-09-25 19:14:42,598][train][INFO] - Starting training!
[2022-09-25 19:14:42,599][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:14:42,600][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:14:42,600][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:14:42,600][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:14:50,297][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:14:50,517][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:16:33,862][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:16:33,862][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:16:33,946][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:16:35,606][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:16:36,944][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:16:36,946][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:16:36,946][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:16:36,948][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:16:36,954][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:16:36,954][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:16:36,954][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:16:36,957][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:16:36,958][train][INFO] - Logging hyperparameters!
[2022-09-25 19:16:36,961][train][INFO] - Starting training!
[2022-09-25 19:16:36,962][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:16:36,963][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:16:36,963][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:16:36,963][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:16:44,619][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:16:44,835][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:17:11,410][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:17:11,410][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:17:11,494][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:17:12,979][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:17:14,348][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:17:14,350][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:17:14,351][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:17:14,352][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:17:14,358][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:17:14,358][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:17:14,358][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:17:14,359][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:17:14,359][train][INFO] - Logging hyperparameters!
[2022-09-25 19:17:14,362][train][INFO] - Starting training!
[2022-09-25 19:17:14,363][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:17:14,364][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:17:14,365][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:17:14,365][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:17:22,052][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:17:22,268][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:19:02,679][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:19:02,679][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:19:02,762][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:19:04,269][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:19:05,644][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:19:05,645][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:19:05,646][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:19:05,648][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:19:05,653][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:19:05,654][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:19:05,654][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:19:05,654][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:19:05,655][train][INFO] - Logging hyperparameters!
[2022-09-25 19:19:05,658][train][INFO] - Starting training!
[2022-09-25 19:19:05,659][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:19:05,660][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:19:05,660][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:19:05,660][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:19:13,375][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:19:13,592][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:19:54,230][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:19:54,230][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:19:54,314][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:19:55,835][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:19:57,203][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:19:57,204][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:19:57,205][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:19:57,207][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:19:57,213][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:19:57,213][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:19:57,213][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:19:57,214][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:19:57,214][train][INFO] - Logging hyperparameters!
[2022-09-25 19:19:57,217][train][INFO] - Starting training!
[2022-09-25 19:19:57,218][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:19:57,219][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:19:57,219][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:19:57,219][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:20:04,917][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:20:05,138][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:20:33,047][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:20:33,047][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:20:33,131][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:20:34,606][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:20:35,978][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:20:35,979][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:20:35,980][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:20:35,982][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:20:35,988][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:20:35,988][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:20:35,988][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:20:35,989][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:20:35,989][train][INFO] - Logging hyperparameters!
[2022-09-25 19:20:35,992][train][INFO] - Starting training!
[2022-09-25 19:20:35,993][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:20:35,994][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:20:35,994][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:20:35,995][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:20:43,645][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:20:43,860][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:21:20,193][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:21:20,193][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:21:20,277][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:21:21,758][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:21:23,098][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:21:23,100][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:21:23,101][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:21:23,102][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:21:23,108][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:21:23,108][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:21:23,111][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:21:23,112][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:21:23,112][train][INFO] - Logging hyperparameters!
[2022-09-25 19:21:23,115][train][INFO] - Starting training!
[2022-09-25 19:21:23,116][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:21:23,117][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:21:23,117][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:21:23,117][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:21:30,826][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:21:31,046][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:22:16,129][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:22:16,129][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:22:16,213][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:22:17,696][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:22:19,036][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:22:19,038][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:22:19,039][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:22:19,040][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:22:19,046][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:22:19,046][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:22:19,047][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:22:19,047][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:22:19,047][train][INFO] - Logging hyperparameters!
[2022-09-25 19:22:19,051][train][INFO] - Starting training!
[2022-09-25 19:22:19,051][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:22:19,052][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:22:19,053][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:22:19,053][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:22:26,666][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:22:26,881][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:23:06,190][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:23:06,190][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:23:06,276][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:23:07,756][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:23:09,098][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:23:09,100][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:23:09,101][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:23:09,102][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:23:09,108][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:23:09,108][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:23:09,108][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:23:09,109][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:23:09,109][train][INFO] - Logging hyperparameters!
[2022-09-25 19:23:09,112][train][INFO] - Starting training!
[2022-09-25 19:23:09,113][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:23:09,114][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:23:09,114][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:23:09,115][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:23:16,799][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:23:17,014][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:23:35,711][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:23:35,711][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:23:35,795][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:23:37,281][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:23:38,613][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:23:38,614][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:23:38,615][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:23:38,617][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:23:38,622][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:23:38,623][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:23:38,623][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:23:38,624][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:23:38,624][train][INFO] - Logging hyperparameters!
[2022-09-25 19:23:38,627][train][INFO] - Starting training!
[2022-09-25 19:23:38,628][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:23:38,629][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:23:38,629][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:23:38,629][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:23:46,247][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:23:46,460][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 19:24:38,297][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 19:24:38,297][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 19:24:38,381][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 19:24:39,844][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 19:24:41,247][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 19:24:41,249][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 19:24:41,250][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 19:24:41,252][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 19:24:41,257][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 19:24:41,257][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 19:24:41,258][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 19:24:41,258][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 19:24:41,259][train][INFO] - Logging hyperparameters!
[2022-09-25 19:24:41,262][train][INFO] - Starting training!
[2022-09-25 19:24:41,263][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 19:24:41,264][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 19:24:41,264][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 19:24:41,264][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 19:24:48,932][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 19:24:49,146][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:44:49,681][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:44:49,701][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:44:49,811][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:45:06,030][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:45:08,043][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:45:08,044][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:45:08,045][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:45:08,047][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:45:08,127][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:45:08,127][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:45:08,127][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:45:08,128][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:45:08,128][train][INFO] - Logging hyperparameters!
[2022-09-25 22:45:08,131][train][INFO] - Starting training!
[2022-09-25 22:45:08,132][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:45:08,133][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:45:08,134][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:45:08,134][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:45:17,053][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:45:17,271][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:46:41,583][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:46:41,583][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:46:41,667][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:46:43,615][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:46:44,923][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:46:44,924][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:46:44,925][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:46:44,927][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:46:44,932][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:46:44,933][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:46:44,933][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:46:44,934][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:46:44,934][train][INFO] - Logging hyperparameters!
[2022-09-25 22:46:44,937][train][INFO] - Starting training!
[2022-09-25 22:46:44,938][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:46:44,939][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:46:44,939][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:46:44,939][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:46:52,671][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:46:52,888][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:47:29,585][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:47:29,585][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:47:29,669][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:47:31,131][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:47:32,450][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:47:32,452][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:47:32,453][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:47:32,455][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:47:32,460][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:47:32,461][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:47:32,461][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:47:32,461][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:47:32,462][train][INFO] - Logging hyperparameters!
[2022-09-25 22:47:32,465][train][INFO] - Starting training!
[2022-09-25 22:47:32,466][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:47:32,467][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:47:32,467][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:47:32,467][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:47:40,219][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:47:40,437][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:48:41,615][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:48:41,616][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:48:41,699][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:48:43,206][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:48:44,518][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:48:44,520][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:48:44,521][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:48:44,523][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:48:44,528][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:48:44,529][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:48:44,529][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:48:44,530][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:48:44,530][train][INFO] - Logging hyperparameters!
[2022-09-25 22:48:44,533][train][INFO] - Starting training!
[2022-09-25 22:48:44,534][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:48:44,535][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:48:44,535][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:48:44,535][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:48:52,228][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:48:52,444][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:50:10,741][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:50:10,741][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:50:10,824][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:50:12,843][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:50:14,196][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:50:14,198][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:50:14,198][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:50:14,200][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:50:14,206][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:50:14,206][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:50:14,206][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:50:14,207][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:50:14,207][train][INFO] - Logging hyperparameters!
[2022-09-25 22:50:14,211][train][INFO] - Starting training!
[2022-09-25 22:50:14,212][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:50:14,213][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:50:14,213][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:50:14,213][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:50:21,999][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:50:22,227][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:51:01,463][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:51:01,463][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:51:01,547][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:51:03,141][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:51:04,465][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:51:04,467][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:51:04,467][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:51:04,469][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:51:04,475][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:51:04,475][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:51:04,475][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:51:04,476][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:51:04,476][train][INFO] - Logging hyperparameters!
[2022-09-25 22:51:04,479][train][INFO] - Starting training!
[2022-09-25 22:51:04,480][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:51:04,481][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:51:04,482][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:51:04,482][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:51:12,170][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:51:12,387][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 22:53:03,139][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 22:53:03,139][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 22:53:03,223][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 22:53:04,829][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 22:53:06,155][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 22:53:06,157][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 22:53:06,158][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 22:53:06,159][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 22:53:06,165][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 22:53:06,165][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 22:53:06,166][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 22:53:06,166][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 22:53:06,167][train][INFO] - Logging hyperparameters!
[2022-09-25 22:53:06,170][train][INFO] - Starting training!
[2022-09-25 22:53:06,171][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 22:53:06,172][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 22:53:06,172][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 22:53:06,172][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 22:53:13,840][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 22:53:14,055][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 23:05:11,103][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:05:11,104][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:05:11,187][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:05:12,853][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:05:14,158][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:05:14,160][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:05:14,161][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:05:14,163][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:05:14,168][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:05:14,168][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:05:14,169][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:05:14,169][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:05:14,170][train][INFO] - Logging hyperparameters!
[2022-09-25 23:05:14,173][train][INFO] - Starting training!
[2022-09-25 23:05:14,174][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:05:14,175][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:05:14,175][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:05:14,175][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:05:21,813][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:05:22,029][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 23:06:53,170][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:06:53,170][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:06:53,254][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:06:54,834][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:06:56,153][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:06:56,154][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:06:56,155][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:06:56,157][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:06:56,163][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:06:56,163][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:06:56,163][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:06:56,164][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:06:56,164][train][INFO] - Logging hyperparameters!
[2022-09-25 23:06:56,167][train][INFO] - Starting training!
[2022-09-25 23:06:56,168][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:06:56,169][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:06:56,169][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:06:56,169][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:07:03,785][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:07:04,003][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | transformer     | DeformableTransformer | 17.0 M
9  | matcher         | HungarianMatcher      | 0     
10 | spatial_conv    | Sequential            | 2.3 K 
11 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.3 M    Trainable params
222 K     Non-trainable params
42.5 M    Total params
170.145   Total estimated model params size (MB)
[2022-09-25 23:09:00,912][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:09:00,913][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:09:00,997][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:09:02,528][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:09:03,853][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:09:03,854][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:09:03,855][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:09:03,857][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:09:03,862][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:09:03,863][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:09:03,863][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:09:03,864][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:09:03,864][train][INFO] - Logging hyperparameters!
[2022-09-25 23:09:03,867][train][INFO] - Starting training!
[2022-09-25 23:09:03,868][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:09:03,869][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:09:03,869][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:09:03,869][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:09:11,558][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:09:11,781][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.454   Total estimated model params size (MB)
[2022-09-25 23:09:31,509][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:09:31,510][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:09:31,594][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:09:33,096][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:09:34,439][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:09:34,441][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:09:34,442][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:09:34,444][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:09:34,449][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:09:34,450][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:09:34,450][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:09:34,451][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:09:34,451][train][INFO] - Logging hyperparameters!
[2022-09-25 23:09:34,454][train][INFO] - Starting training!
[2022-09-25 23:09:34,455][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:09:34,456][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:09:34,456][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:09:34,457][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:09:42,416][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:09:42,644][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.454   Total estimated model params size (MB)
[2022-09-25 23:13:16,227][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:13:16,228][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:13:16,311][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:13:17,787][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:13:19,142][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:13:19,143][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:13:19,144][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:13:19,146][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:13:19,152][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:13:19,152][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:13:19,152][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:13:19,153][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:13:19,153][train][INFO] - Logging hyperparameters!
[2022-09-25 23:13:19,157][train][INFO] - Starting training!
[2022-09-25 23:13:19,157][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:13:19,158][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:13:19,159][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:13:19,159][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:13:26,819][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:13:27,035][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.454   Total estimated model params size (MB)
[2022-09-25 23:13:55,623][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:13:55,624][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:13:55,707][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:13:57,176][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:13:58,481][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:13:58,483][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:13:58,484][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:13:58,485][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:13:58,491][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:13:58,491][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:13:58,492][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:13:58,492][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:13:58,492][train][INFO] - Logging hyperparameters!
[2022-09-25 23:13:58,496][train][INFO] - Starting training!
[2022-09-25 23:13:58,497][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:13:58,498][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:13:58,498][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:13:58,498][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:14:06,181][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:14:06,400][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:14:54,237][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:14:54,237][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:14:54,321][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:14:55,811][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:14:57,148][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:14:57,150][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:14:57,150][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:14:57,152][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:14:57,158][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:14:57,158][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:14:57,158][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:14:57,159][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:14:57,159][train][INFO] - Logging hyperparameters!
[2022-09-25 23:14:57,162][train][INFO] - Starting training!
[2022-09-25 23:14:57,163][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:14:57,164][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:14:57,164][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:14:57,165][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:15:04,877][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:15:05,097][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:16:29,719][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:16:29,720][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:16:29,803][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:16:31,281][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:16:32,616][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:16:32,618][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:16:32,619][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:16:32,620][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:16:32,626][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:16:32,626][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:16:32,626][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:16:32,627][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:16:32,627][train][INFO] - Logging hyperparameters!
[2022-09-25 23:16:32,631][train][INFO] - Starting training!
[2022-09-25 23:16:32,632][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:16:32,633][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:16:32,633][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:16:32,633][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:16:40,376][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:16:40,607][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:17:02,092][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:17:02,093][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:17:02,177][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:17:03,636][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:17:04,995][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:17:04,997][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:17:04,998][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:17:04,999][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:17:05,005][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:17:05,005][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:17:05,006][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:17:05,006][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:17:05,006][train][INFO] - Logging hyperparameters!
[2022-09-25 23:17:05,010][train][INFO] - Starting training!
[2022-09-25 23:17:05,011][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:17:05,012][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:17:05,012][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:17:05,012][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:17:12,741][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:17:12,958][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:17:42,475][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:17:42,476][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:17:42,560][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:17:44,049][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:17:45,868][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:17:45,870][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:17:45,871][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:17:45,873][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:17:45,879][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:17:45,879][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:17:45,879][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:17:45,880][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:17:45,880][train][INFO] - Logging hyperparameters!
[2022-09-25 23:17:45,884][train][INFO] - Starting training!
[2022-09-25 23:17:45,884][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:17:45,885][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:17:45,886][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:17:45,886][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:17:53,513][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:17:53,730][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:20:44,931][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:20:44,931][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:20:45,014][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:20:46,519][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:20:47,817][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:20:47,818][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:20:47,819][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:20:47,821][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:20:47,827][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:20:47,827][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:20:47,827][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:20:47,828][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:20:47,828][train][INFO] - Logging hyperparameters!
[2022-09-25 23:20:47,832][train][INFO] - Starting training!
[2022-09-25 23:20:47,832][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:20:47,833][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:20:47,834][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:20:47,834][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:20:55,750][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:20:55,978][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:27:05,273][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:27:05,274][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:27:05,357][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:27:06,837][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:27:08,176][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:27:08,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:27:08,179][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:27:08,180][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:27:08,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:27:08,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:27:08,186][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:27:08,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:27:08,187][train][INFO] - Logging hyperparameters!
[2022-09-25 23:27:08,191][train][INFO] - Starting training!
[2022-09-25 23:27:08,191][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:27:08,192][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:27:08,193][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:27:08,193][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:27:15,900][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:27:16,118][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:31:33,525][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:31:33,525][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:31:33,609][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:31:35,114][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:31:36,520][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:31:36,521][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:31:36,522][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:31:36,524][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:31:36,530][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:31:36,530][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:31:36,531][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:31:36,531][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:31:36,532][train][INFO] - Logging hyperparameters!
[2022-09-25 23:31:36,535][train][INFO] - Starting training!
[2022-09-25 23:31:36,536][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:31:36,537][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:31:36,537][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:31:36,537][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:31:44,547][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:31:44,765][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:33:01,088][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:33:01,088][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:33:01,172][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:33:02,638][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:33:04,044][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:33:04,045][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:33:04,046][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:33:04,048][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:33:04,054][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:33:04,054][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:33:04,054][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:33:04,055][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:33:04,055][train][INFO] - Logging hyperparameters!
[2022-09-25 23:33:04,058][train][INFO] - Starting training!
[2022-09-25 23:33:04,059][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:33:04,060][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:33:04,060][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:33:04,061][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:33:11,733][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:33:11,950][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:33:31,006][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:33:31,007][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:33:31,091][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:33:32,569][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:33:33,880][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:33:33,881][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:33:33,882][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:33:33,884][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:33:33,890][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:33:33,890][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:33:33,890][pytorch_lightning.trainer.connectors.accelerator_connector][INFO] - Using native 16bit precision.
[2022-09-25 23:33:33,891][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:33:33,891][train][INFO] - Logging hyperparameters!
[2022-09-25 23:33:33,894][train][INFO] - Starting training!
[2022-09-25 23:33:33,895][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:33:33,896][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:33:33,896][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:33:33,896][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:33:41,674][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:33:41,892][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:38:10,758][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:38:10,758][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:38:10,843][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:38:12,299][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:38:13,580][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:38:13,582][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:38:13,582][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:38:13,584][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:38:13,590][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:38:13,590][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:38:13,591][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:38:13,591][train][INFO] - Logging hyperparameters!
[2022-09-25 23:38:13,594][train][INFO] - Starting training!
[2022-09-25 23:38:13,595][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:38:13,596][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:38:13,596][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:38:13,597][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:38:21,228][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:38:21,448][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:39:08,347][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:39:08,347][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:39:08,430][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:39:09,894][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:39:11,204][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:39:11,206][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:39:11,207][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:39:11,209][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:39:11,215][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:39:11,215][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:39:11,215][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:39:11,216][train][INFO] - Logging hyperparameters!
[2022-09-25 23:39:11,219][train][INFO] - Starting training!
[2022-09-25 23:39:11,220][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:39:11,221][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:39:11,221][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:39:11,221][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:39:18,897][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:39:19,115][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 17.0 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
42.4 M    Trainable params
222 K     Non-trainable params
42.6 M    Total params
170.458   Total estimated model params size (MB)
[2022-09-25 23:40:34,768][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:40:34,768][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:40:34,852][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:40:36,319][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:40:38,082][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:40:38,083][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:40:38,084][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:40:38,086][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:40:38,091][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:40:38,092][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:40:38,092][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:40:38,093][train][INFO] - Logging hyperparameters!
[2022-09-25 23:40:38,096][train][INFO] - Starting training!
[2022-09-25 23:40:38,097][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:40:38,098][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:40:38,098][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:40:38,098][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:40:45,818][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:40:46,061][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 40.2 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
65.6 M    Trainable params
222 K     Non-trainable params
65.8 M    Total params
263.380   Total estimated model params size (MB)
[2022-09-25 23:41:36,863][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:41:36,863][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:41:36,947][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:41:38,472][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:41:40,076][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:41:40,078][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:41:40,079][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:41:40,081][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:41:40,086][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:41:40,086][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:41:40,087][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:41:40,087][train][INFO] - Logging hyperparameters!
[2022-09-25 23:41:40,090][train][INFO] - Starting training!
[2022-09-25 23:41:40,091][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:41:40,092][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:41:40,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:41:40,093][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:41:47,860][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:41:47,987][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 20.1 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
45.5 M    Trainable params
222 K     Non-trainable params
45.7 M    Total params
182.982   Total estimated model params size (MB)
[2022-09-25 23:56:00,397][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:56:00,399][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:56:00,515][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:56:18,523][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:56:20,670][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:56:20,671][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:56:20,672][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:56:20,674][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:56:20,680][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:56:20,680][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:56:20,681][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:56:20,681][train][INFO] - Logging hyperparameters!
[2022-09-25 23:56:20,683][train][INFO] - Starting training!
[2022-09-25 23:56:20,684][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:56:20,685][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:56:20,685][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:56:20,686][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:56:30,109][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:56:30,233][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.2 M    Trainable params
222 K     Non-trainable params
39.4 M    Total params
157.792   Total estimated model params size (MB)
[2022-09-25 23:57:42,123][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:57:42,123][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:57:42,206][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:57:45,999][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:57:47,545][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:57:47,547][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:57:47,548][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:57:47,550][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:57:47,555][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:57:47,555][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:57:47,556][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:57:47,557][train][INFO] - Logging hyperparameters!
[2022-09-25 23:57:47,559][train][INFO] - Starting training!
[2022-09-25 23:57:47,560][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:57:47,561][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:57:47,561][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:57:47,561][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:57:55,463][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:57:55,586][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.2 M    Trainable params
222 K     Non-trainable params
39.4 M    Total params
157.792   Total estimated model params size (MB)
[2022-09-25 23:58:15,077][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:58:15,078][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:58:15,161][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:58:16,623][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:58:18,119][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:58:18,120][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:58:18,121][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:58:18,123][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:58:18,129][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:58:18,129][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:58:18,130][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:58:18,130][train][INFO] - Logging hyperparameters!
[2022-09-25 23:58:18,132][train][INFO] - Starting training!
[2022-09-25 23:58:18,133][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:58:18,134][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:58:18,134][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:58:18,134][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:58:26,026][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:58:26,107][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 14.1 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.6 M    Trainable params
222 K     Non-trainable params
39.8 M    Total params
159.110   Total estimated model params size (MB)
[2022-09-25 23:59:04,328][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-25 23:59:04,329][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-25 23:59:04,414][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-25 23:59:08,207][train][INFO] - Instantiating model <models.model.Model>
[2022-09-25 23:59:09,744][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-25 23:59:09,746][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-25 23:59:09,747][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-25 23:59:09,748][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-25 23:59:09,754][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-25 23:59:09,754][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-25 23:59:09,755][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-25 23:59:09,755][train][INFO] - Logging hyperparameters!
[2022-09-25 23:59:09,757][train][INFO] - Starting training!
[2022-09-25 23:59:09,758][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-25 23:59:09,759][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-25 23:59:09,759][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-25 23:59:09,760][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-25 23:59:17,660][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-25 23:59:17,784][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 77.1 K
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.2 M    Trainable params
222 K     Non-trainable params
39.4 M    Total params
157.792   Total estimated model params size (MB)
[2022-09-26 00:04:22,989][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:04:22,990][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:04:23,073][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:04:24,917][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:04:26,424][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:04:26,426][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:04:26,427][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:04:26,428][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:04:26,434][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:04:26,434][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:04:26,435][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:04:26,435][train][INFO] - Logging hyperparameters!
[2022-09-26 00:04:26,438][train][INFO] - Starting training!
[2022-09-26 00:04:26,438][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:04:26,439][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:04:26,440][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:04:26,440][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:04:34,274][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:04:34,397][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 154 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.100   Total estimated model params size (MB)
[2022-09-26 00:05:21,379][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:05:21,379][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:05:21,463][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:05:23,011][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:05:24,536][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:05:24,537][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:05:24,538][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:05:24,540][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:05:24,545][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:05:24,545][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:05:24,546][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:05:24,546][train][INFO] - Logging hyperparameters!
[2022-09-26 00:05:24,549][train][INFO] - Starting training!
[2022-09-26 00:05:24,550][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:05:24,551][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:05:24,551][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:05:24,551][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:05:32,390][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:05:32,513][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 154 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.100   Total estimated model params size (MB)
[2022-09-26 00:06:01,414][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:06:01,414][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:06:01,498][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:06:03,155][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:06:04,687][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:06:04,688][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:06:04,689][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:06:04,691][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:06:04,697][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:06:04,697][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:06:04,698][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:06:04,698][train][INFO] - Logging hyperparameters!
[2022-09-26 00:06:04,701][train][INFO] - Starting training!
[2022-09-26 00:06:04,701][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:06:04,702][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:06:04,703][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:06:04,703][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:06:12,630][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:06:12,755][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 154 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.100   Total estimated model params size (MB)
[2022-09-26 00:07:01,999][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:07:02,000][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:07:02,083][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:07:03,607][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:07:05,131][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:07:05,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:07:05,133][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:07:05,135][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:07:05,141][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:07:05,141][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:07:05,142][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:07:05,142][train][INFO] - Logging hyperparameters!
[2022-09-26 00:07:05,144][train][INFO] - Starting training!
[2022-09-26 00:07:05,145][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:07:05,146][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:07:05,146][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:07:05,146][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:07:12,926][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:07:13,064][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Linear                | 154 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.100   Total estimated model params size (MB)
[2022-09-26 00:09:23,346][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:09:23,346][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:09:23,429][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:09:24,959][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:09:26,478][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:09:26,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:09:26,480][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:09:26,482][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:09:26,488][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:09:26,488][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:09:26,489][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:09:26,489][train][INFO] - Logging hyperparameters!
[2022-09-26 00:09:26,491][train][INFO] - Starting training!
[2022-09-26 00:09:26,492][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:09:26,493][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:09:26,493][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:09:26,494][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:09:34,289][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:09:34,412][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.098   Total estimated model params size (MB)
[2022-09-26 00:10:00,184][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 00:10:00,184][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 00:10:00,268][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 00:10:01,739][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 00:10:03,331][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 00:10:03,332][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 00:10:03,333][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 00:10:03,335][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 00:10:03,340][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 00:10:03,341][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 00:10:03,341][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 00:10:03,342][train][INFO] - Logging hyperparameters!
[2022-09-26 00:10:03,344][train][INFO] - Starting training!
[2022-09-26 00:10:03,345][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 00:10:03,346][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 00:10:03,346][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 00:10:03,346][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 00:10:11,255][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 00:10:11,380][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.098   Total estimated model params size (MB)
[2022-09-26 10:35:46,570][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:35:46,590][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:35:46,705][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:36:02,291][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:36:04,710][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:36:04,711][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:36:04,713][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:36:04,714][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:36:04,720][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:36:04,720][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:36:04,721][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:36:04,721][train][INFO] - Logging hyperparameters!
[2022-09-26 10:36:04,724][train][INFO] - Starting training!
[2022-09-26 10:36:04,724][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:36:04,726][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:36:04,726][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:36:04,726][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:36:13,391][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:36:13,513][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.098   Total estimated model params size (MB)
[2022-09-26 10:36:49,775][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:36:49,775][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:36:49,859][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:36:51,324][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:36:52,824][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:36:52,825][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:36:52,826][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:36:52,828][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:36:52,834][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:36:52,834][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:36:52,835][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:36:52,835][train][INFO] - Logging hyperparameters!
[2022-09-26 10:36:52,837][train][INFO] - Starting training!
[2022-09-26 10:36:52,838][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:36:52,840][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:36:52,840][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:36:52,841][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:37:00,672][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:37:00,793][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 13.8 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.3 M    Trainable params
222 K     Non-trainable params
39.5 M    Total params
158.098   Total estimated model params size (MB)
[2022-09-26 10:38:35,079][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:38:35,080][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:38:35,164][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:38:36,625][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:38:38,130][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:38:38,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:38:38,133][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:38:38,134][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:38:38,140][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:38:38,140][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:38:38,141][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:38:38,141][train][INFO] - Logging hyperparameters!
[2022-09-26 10:38:38,143][train][INFO] - Starting training!
[2022-09-26 10:38:38,144][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:38:38,145][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:38:38,145][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:38:38,146][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:38:45,998][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:38:46,080][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 14.1 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.6 M    Trainable params
222 K     Non-trainable params
39.9 M    Total params
159.416   Total estimated model params size (MB)
[2022-09-26 10:39:10,804][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:39:10,804][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:39:10,889][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:39:12,357][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:39:13,890][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:39:13,891][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:39:13,892][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:39:13,894][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:39:13,899][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:39:13,900][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:39:13,900][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:39:13,901][train][INFO] - Logging hyperparameters!
[2022-09-26 10:39:13,903][train][INFO] - Starting training!
[2022-09-26 10:39:13,904][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:39:13,905][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:39:13,905][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:39:13,905][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:39:21,727][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:39:21,810][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 14.1 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.6 M    Trainable params
222 K     Non-trainable params
39.9 M    Total params
159.416   Total estimated model params size (MB)
[2022-09-26 10:39:48,832][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:39:48,832][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:39:48,916][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:39:50,378][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:39:51,901][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:39:51,903][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:39:51,904][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:39:51,905][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:39:51,911][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:39:51,911][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:39:51,912][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:39:51,912][train][INFO] - Logging hyperparameters!
[2022-09-26 10:39:51,914][train][INFO] - Starting training!
[2022-09-26 10:39:51,915][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:39:51,916][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:39:51,916][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:39:51,917][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:39:59,766][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:39:59,848][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | class_embed     | Linear                | 23.4 K
3  | bbox_embed      | MLP                   | 132 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 14.1 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.6 M    Trainable params
222 K     Non-trainable params
39.9 M    Total params
159.416   Total estimated model params size (MB)
[2022-09-26 10:44:04,233][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:44:04,233][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:44:04,331][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:44:05,810][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:44:39,150][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:44:39,150][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:44:39,234][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:44:40,702][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:44:42,218][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:44:42,219][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:44:42,220][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:44:42,222][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:44:42,227][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:44:42,228][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:44:42,228][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:44:42,228][train][INFO] - Logging hyperparameters!
[2022-09-26 10:44:42,231][train][INFO] - Starting training!
[2022-09-26 10:44:42,232][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:44:42,233][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:44:42,233][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:44:42,233][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:44:49,910][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:44:50,030][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | upsample_layer  | Sequential            | 442 K 
3  | head_detect_sot | Conv2d                | 8     
4  | head_class_sot  | Conv2d                | 4     
5  | head_mask_sot   | Conv2d                | 4     
6  | class_embed     | ModuleList            | 70.2 K
7  | bbox_embed      | ModuleList            | 397 K 
8  | query_embed     | Embedding             | 153 K 
9  | transformer     | DeformableTransformer | 14.6 M
10 | matcher         | HungarianMatcher      | 0     
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.2 M    Total params
160.664   Total estimated model params size (MB)
[2022-09-26 10:47:44,107][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:47:44,108][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:47:44,192][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:47:45,673][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:47:47,232][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:47:47,233][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:47:47,234][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:47:47,236][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:47:47,242][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:47:47,242][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:47:47,242][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:47:47,243][train][INFO] - Logging hyperparameters!
[2022-09-26 10:47:47,246][train][INFO] - Starting training!
[2022-09-26 10:47:47,247][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:47:47,248][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:47:47,248][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:47:47,248][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:47:54,973][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:47:55,095][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | transformer     | DeformableTransformer | 14.3 M
3  | matcher         | HungarianMatcher      | 0     
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | class_embed     | ModuleList            | 70.2 K
9  | bbox_embed      | ModuleList            | 397 K 
10 | query_embed     | Embedding             | 153 K 
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 10:52:05,920][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 10:52:05,920][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 10:52:06,004][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 10:52:07,465][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 10:52:08,997][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 10:52:08,998][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 10:52:08,999][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 10:52:09,001][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 10:52:09,006][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 10:52:09,007][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 10:52:09,007][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 10:52:09,007][train][INFO] - Logging hyperparameters!
[2022-09-26 10:52:09,010][train][INFO] - Starting training!
[2022-09-26 10:52:09,011][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 10:52:09,012][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 10:52:09,012][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 10:52:09,012][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 10:52:16,767][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 10:52:16,892][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | transformer     | DeformableTransformer | 14.3 M
3  | matcher         | HungarianMatcher      | 0     
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | class_embed     | ModuleList            | 70.2 K
9  | bbox_embed      | ModuleList            | 397 K 
10 | query_embed     | Embedding             | 153 K 
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:06:12,078][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:06:12,079][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:06:12,162][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:06:15,131][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:06:16,876][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:06:16,877][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:06:16,878][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:06:16,880][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:06:16,885][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:06:16,886][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:06:16,886][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:06:16,886][train][INFO] - Logging hyperparameters!
[2022-09-26 11:06:16,889][train][INFO] - Starting training!
[2022-09-26 11:06:16,890][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:06:16,891][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:06:16,891][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:06:16,891][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:06:24,596][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:06:24,717][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | input_proj      | ModuleList            | 986 K 
1  | backbone        | Joiner                | 23.5 M
2  | transformer     | DeformableTransformer | 14.3 M
3  | matcher         | HungarianMatcher      | 0     
4  | upsample_layer  | Sequential            | 442 K 
5  | head_detect_sot | Conv2d                | 8     
6  | head_class_sot  | Conv2d                | 4     
7  | head_mask_sot   | Conv2d                | 4     
8  | class_embed     | ModuleList            | 70.2 K
9  | bbox_embed      | ModuleList            | 397 K 
10 | query_embed     | Embedding             | 153 K 
11 | spatial_conv    | Sequential            | 2.3 K 
12 | temporal_conv   | Sequential            | 524 K 
-----------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:37:05,436][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:37:05,436][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:37:05,519][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:37:08,112][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:37:09,893][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:37:09,895][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:37:09,896][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:37:09,897][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:37:09,903][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:37:09,903][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:37:09,904][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:37:09,904][train][INFO] - Logging hyperparameters!
[2022-09-26 11:37:09,906][train][INFO] - Starting training!
[2022-09-26 11:37:09,907][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:37:09,908][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:37:09,909][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:37:09,909][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:37:17,576][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:37:17,696][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.8 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:37:55,545][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:37:55,546][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:37:55,630][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:37:57,090][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:37:58,594][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:37:58,595][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:37:58,596][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:37:58,598][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:37:58,604][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:37:58,604][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:37:58,605][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:37:58,605][train][INFO] - Logging hyperparameters!
[2022-09-26 11:37:58,608][train][INFO] - Starting training!
[2022-09-26 11:37:58,608][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:37:58,609][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:37:58,610][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:37:58,610][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:38:06,286][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:38:06,407][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.8 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:38:37,313][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:38:37,313][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:38:37,395][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:38:38,857][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:38:40,400][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:38:40,401][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:38:40,402][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:38:40,404][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:38:40,409][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:38:40,409][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:38:40,410][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:38:40,410][train][INFO] - Logging hyperparameters!
[2022-09-26 11:38:40,413][train][INFO] - Starting training!
[2022-09-26 11:38:40,414][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:38:40,415][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:38:40,415][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:38:40,415][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:38:48,065][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:38:48,186][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.8 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:40:14,642][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:40:14,642][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:40:14,725][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:40:16,187][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:40:17,689][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:40:17,691][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:40:17,692][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:40:17,693][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:40:17,699][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:40:17,699][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:40:17,700][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:40:17,700][train][INFO] - Logging hyperparameters!
[2022-09-26 11:40:17,703][train][INFO] - Starting training!
[2022-09-26 11:40:17,704][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:40:17,705][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:40:17,705][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:40:17,705][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:40:25,403][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:40:25,524][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.8 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:40:59,790][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:40:59,791][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:40:59,874][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:41:01,336][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:41:02,824][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:41:02,826][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:41:02,827][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:41:02,829][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:41:02,834][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:41:02,834][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:41:02,835][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:41:02,835][train][INFO] - Logging hyperparameters!
[2022-09-26 11:41:02,838][train][INFO] - Starting training!
[2022-09-26 11:41:02,839][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:41:02,840][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:41:02,840][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:41:02,840][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:41:10,488][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:41:10,608][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.8 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
40.1 M    Trainable params
222 K     Non-trainable params
40.3 M    Total params
161.288   Total estimated model params size (MB)
[2022-09-26 11:47:04,269][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:47:04,269][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:47:04,353][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:47:05,806][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:47:07,506][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:47:07,508][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:47:07,509][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:47:07,510][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:47:07,518][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:47:07,518][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:47:07,519][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:47:07,519][train][INFO] - Logging hyperparameters!
[2022-09-26 11:47:07,522][train][INFO] - Starting training!
[2022-09-26 11:47:07,523][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:47:07,524][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:47:07,524][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:47:07,524][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:47:15,194][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:47:15,315][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 153 K 
---------------------------------------------------------
39.8 M    Trainable params
222 K     Non-trainable params
40.0 M    Total params
159.970   Total estimated model params size (MB)
[2022-09-26 11:54:59,341][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 11:54:59,342][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 11:54:59,426][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 11:55:00,889][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 11:55:02,368][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 11:55:02,369][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 11:55:02,370][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 11:55:02,372][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 11:55:02,377][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 11:55:02,377][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 11:55:02,378][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 11:55:02,378][train][INFO] - Logging hyperparameters!
[2022-09-26 11:55:02,381][train][INFO] - Starting training!
[2022-09-26 11:55:02,382][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 11:55:02,383][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 11:55:02,383][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 11:55:02,383][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 11:55:10,053][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 11:55:10,173][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:01:09,770][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:01:09,771][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:01:09,854][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:01:11,318][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:01:12,814][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:01:12,816][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:01:12,817][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:01:12,818][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:01:12,824][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:01:12,824][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:01:12,825][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:01:12,825][train][INFO] - Logging hyperparameters!
[2022-09-26 12:01:12,828][train][INFO] - Starting training!
[2022-09-26 12:01:12,829][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:01:12,829][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:01:12,830][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:01:12,830][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:01:20,546][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:01:20,667][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:02:35,715][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:02:35,715][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:02:35,799][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:02:37,267][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:02:38,770][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:02:38,771][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:02:38,772][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:02:38,774][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:02:38,779][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:02:38,780][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:02:38,780][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:02:38,780][train][INFO] - Logging hyperparameters!
[2022-09-26 12:02:38,783][train][INFO] - Starting training!
[2022-09-26 12:02:38,784][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:02:38,785][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:02:38,785][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:02:38,785][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:02:46,499][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:02:46,619][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:03:47,492][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:03:47,492][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:03:47,575][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:03:49,040][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:03:50,514][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:03:50,515][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:03:50,516][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:03:50,518][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:03:50,539][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:03:50,540][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:03:50,540][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:03:50,540][train][INFO] - Logging hyperparameters!
[2022-09-26 12:03:50,543][train][INFO] - Starting training!
[2022-09-26 12:03:50,544][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:03:50,545][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:03:50,545][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:03:50,545][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:03:58,173][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:03:58,292][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:05:01,461][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:05:01,461][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:05:01,545][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:05:03,004][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:05:04,492][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:05:04,494][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:05:04,495][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:05:04,496][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:05:04,502][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:05:04,502][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:05:04,503][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:05:04,503][train][INFO] - Logging hyperparameters!
[2022-09-26 12:05:04,506][train][INFO] - Starting training!
[2022-09-26 12:05:04,507][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:05:04,508][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:05:04,508][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:05:04,508][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:05:12,205][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:05:12,325][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:05:27,570][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:05:27,570][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:05:27,654][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:05:29,115][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:05:30,599][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:05:30,601][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:05:30,602][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:05:30,603][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:05:30,609][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:05:30,609][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:05:30,610][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:05:30,610][train][INFO] - Logging hyperparameters!
[2022-09-26 12:05:30,613][train][INFO] - Starting training!
[2022-09-26 12:05:30,613][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:05:30,614][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:05:30,615][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:05:30,615][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:05:38,313][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:05:38,433][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:06:30,640][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:06:30,640][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:06:30,724][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:06:32,185][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:06:33,785][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:06:33,787][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:06:33,788][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:06:33,789][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:06:33,795][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:06:33,795][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:06:33,796][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:06:33,796][train][INFO] - Logging hyperparameters!
[2022-09-26 12:06:33,799][train][INFO] - Starting training!
[2022-09-26 12:06:33,799][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:06:33,800][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:06:33,801][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:06:33,801][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:06:41,453][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:06:41,572][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:07:02,317][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:07:02,317][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:07:02,401][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:07:03,863][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:07:05,344][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:07:05,346][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:07:05,346][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:07:05,348][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:07:05,354][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:07:05,354][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:07:05,355][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:07:05,355][train][INFO] - Logging hyperparameters!
[2022-09-26 12:07:05,358][train][INFO] - Starting training!
[2022-09-26 12:07:05,358][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:07:05,359][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:07:05,360][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:07:05,360][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:07:13,021][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:07:13,141][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:16:15,685][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:16:15,685][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:16:15,769][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:16:17,232][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:16:18,723][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:16:18,725][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:16:18,725][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:16:18,727][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:16:18,733][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:16:18,733][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:16:18,734][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:16:18,734][train][INFO] - Logging hyperparameters!
[2022-09-26 12:16:18,736][train][INFO] - Starting training!
[2022-09-26 12:16:18,737][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:16:18,738][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:16:18,738][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:16:18,739][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:16:26,392][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:16:26,512][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:16:54,421][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:16:54,422][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:16:54,505][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:16:55,968][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:16:57,457][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:16:57,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:16:57,460][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:16:57,461][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:16:57,467][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:16:57,467][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:16:57,468][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:16:57,468][train][INFO] - Logging hyperparameters!
[2022-09-26 12:16:57,471][train][INFO] - Starting training!
[2022-09-26 12:16:57,472][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:16:57,473][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:16:57,473][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:16:57,473][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:17:05,171][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:17:05,294][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:29:30,037][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:29:30,037][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:29:30,121][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:29:31,591][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:29:33,077][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:29:33,078][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:29:33,079][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:29:33,081][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:29:33,086][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:29:33,086][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:29:33,087][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:29:33,087][train][INFO] - Logging hyperparameters!
[2022-09-26 12:29:33,090][train][INFO] - Starting training!
[2022-09-26 12:29:33,091][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:29:33,092][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:29:33,092][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:29:33,092][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:29:40,824][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:29:40,944][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:30:08,108][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:30:08,109][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:30:08,193][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:30:09,657][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:30:11,129][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:30:11,130][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:30:11,131][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:30:11,133][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:30:11,138][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:30:11,139][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:30:11,140][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:30:11,140][train][INFO] - Logging hyperparameters!
[2022-09-26 12:30:11,142][train][INFO] - Starting training!
[2022-09-26 12:30:11,143][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:30:11,144][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:30:11,144][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:30:11,145][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:30:18,768][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:30:18,888][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:30:35,652][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:30:35,652][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:30:35,737][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:30:37,196][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:30:38,681][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:30:38,682][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:30:38,683][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:30:38,685][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:30:38,691][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:30:38,691][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:30:38,691][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:30:38,692][train][INFO] - Logging hyperparameters!
[2022-09-26 12:30:38,694][train][INFO] - Starting training!
[2022-09-26 12:30:38,695][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:30:38,696][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:30:38,696][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:30:38,696][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:30:46,345][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:30:46,465][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:31:05,639][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:31:05,639][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:31:05,722][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:31:07,231][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:31:09,039][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:31:09,041][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:31:09,042][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:31:09,043][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:31:09,049][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:31:09,049][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:31:09,050][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:31:09,050][train][INFO] - Logging hyperparameters!
[2022-09-26 12:31:09,052][train][INFO] - Starting training!
[2022-09-26 12:31:09,053][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:31:09,054][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:31:09,054][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:31:09,055][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:31:16,746][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:31:16,865][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:31:36,481][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:31:36,482][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:31:36,565][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:31:38,025][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:31:39,506][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:31:39,508][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:31:39,509][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:31:39,510][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:31:39,516][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:31:39,516][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:31:39,517][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:31:39,517][train][INFO] - Logging hyperparameters!
[2022-09-26 12:31:39,520][train][INFO] - Starting training!
[2022-09-26 12:31:39,521][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:31:39,522][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:31:39,522][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:31:39,522][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:31:47,205][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:31:47,325][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:32:24,414][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:32:24,415][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:32:24,498][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:32:25,972][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:32:27,458][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:32:27,459][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:32:27,460][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:32:27,462][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:32:27,467][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:32:27,467][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:32:27,469][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:32:27,469][train][INFO] - Logging hyperparameters!
[2022-09-26 12:32:27,472][train][INFO] - Starting training!
[2022-09-26 12:32:27,473][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:32:27,474][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:32:27,474][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:32:27,474][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:32:35,179][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:32:35,300][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 12:33:09,486][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 12:33:09,487][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 12:33:09,603][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 12:33:11,065][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 12:33:12,711][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 12:33:12,712][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 12:33:12,713][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 12:33:12,715][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 12:33:12,721][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 12:33:12,721][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 12:33:12,721][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 12:33:12,722][train][INFO] - Logging hyperparameters!
[2022-09-26 12:33:12,724][train][INFO] - Starting training!
[2022-09-26 12:33:12,725][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 12:33:12,726][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 12:33:12,726][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 12:33:12,726][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 12:33:20,381][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 12:33:20,501][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:11:44,248][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:11:44,262][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:11:44,403][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:12:01,431][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:12:03,767][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:12:03,769][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:12:03,770][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:12:03,771][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:12:03,777][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:12:03,777][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:12:03,778][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:12:03,778][train][INFO] - Logging hyperparameters!
[2022-09-26 15:12:03,781][train][INFO] - Starting training!
[2022-09-26 15:12:03,782][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:12:03,783][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:12:03,783][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:12:03,783][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:12:12,587][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:12:12,713][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:15:56,975][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:15:56,991][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:15:57,075][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:15:59,941][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:16:01,490][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:16:01,491][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:16:01,492][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:16:01,494][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:16:01,499][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:16:01,499][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:16:01,500][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:16:01,500][train][INFO] - Logging hyperparameters!
[2022-09-26 15:16:01,503][train][INFO] - Starting training!
[2022-09-26 15:16:01,504][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:16:01,505][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:16:01,505][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:16:01,505][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:16:09,276][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:16:09,397][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:16:49,436][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:16:49,436][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:16:49,520][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:16:51,298][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:16:52,813][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:16:52,814][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:16:52,815][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:16:52,817][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:16:52,823][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:16:52,823][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:16:52,823][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:16:52,824][train][INFO] - Logging hyperparameters!
[2022-09-26 15:16:52,827][train][INFO] - Starting training!
[2022-09-26 15:16:52,828][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:16:52,829][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:16:52,829][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:16:52,829][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:17:00,469][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:17:00,589][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:18:43,760][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:18:43,760][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:18:43,844][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:18:45,489][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:18:47,006][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:18:47,007][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:18:47,008][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:18:47,010][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:18:47,016][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:18:47,016][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:18:47,016][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:18:47,017][train][INFO] - Logging hyperparameters!
[2022-09-26 15:18:47,019][train][INFO] - Starting training!
[2022-09-26 15:18:47,020][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:18:47,021][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:18:47,021][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:18:47,021][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:18:54,805][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:18:54,929][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:19:42,569][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:19:42,569][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:19:42,653][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:19:44,133][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:19:45,653][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:19:45,654][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:19:45,655][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:19:45,657][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:19:45,662][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:19:45,663][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:19:45,663][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:19:45,663][train][INFO] - Logging hyperparameters!
[2022-09-26 15:19:45,666][train][INFO] - Starting training!
[2022-09-26 15:19:45,667][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:19:45,668][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:19:45,668][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:19:45,668][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:19:53,377][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:19:53,499][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:20:11,886][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:20:11,887][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:20:11,972][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:20:13,443][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:20:14,954][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:20:14,956][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:20:14,956][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:20:14,958][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:20:14,964][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:20:14,964][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:20:14,965][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:20:14,965][train][INFO] - Logging hyperparameters!
[2022-09-26 15:20:14,967][train][INFO] - Starting training!
[2022-09-26 15:20:14,968][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:20:14,969][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:20:14,969][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:20:14,970][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:20:22,687][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:20:22,810][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:20:33,788][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:20:33,788][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:20:33,872][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:20:35,359][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:20:36,885][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:20:36,887][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:20:36,887][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:20:36,889][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:20:36,895][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:20:36,895][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:20:36,895][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:20:36,896][train][INFO] - Logging hyperparameters!
[2022-09-26 15:20:36,899][train][INFO] - Starting training!
[2022-09-26 15:20:36,900][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:20:36,901][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:20:36,901][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:20:36,901][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:20:44,614][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:20:44,736][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:23:19,775][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:23:19,776][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:23:19,860][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:23:21,312][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:23:22,809][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:23:22,810][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:23:22,811][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:23:22,813][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:23:22,818][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:23:22,819][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:23:22,819][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:23:22,819][train][INFO] - Logging hyperparameters!
[2022-09-26 15:23:22,822][train][INFO] - Starting training!
[2022-09-26 15:23:22,823][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:23:22,824][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:23:22,824][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:23:22,824][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:23:30,418][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:23:30,557][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:23:54,590][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:23:54,590][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:23:54,674][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:23:56,150][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:23:57,667][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:23:57,668][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:23:57,669][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:23:57,671][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:23:57,677][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:23:57,677][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:23:57,677][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:23:57,678][train][INFO] - Logging hyperparameters!
[2022-09-26 15:23:57,680][train][INFO] - Starting training!
[2022-09-26 15:23:57,681][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:23:57,682][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:23:57,683][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:23:57,683][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:24:05,396][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:24:05,518][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:24:20,519][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:24:20,519][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:24:20,605][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:24:22,090][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:24:23,600][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:24:23,601][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:24:23,602][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:24:23,604][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:24:23,609][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:24:23,610][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:24:23,610][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:24:23,610][train][INFO] - Logging hyperparameters!
[2022-09-26 15:24:23,613][train][INFO] - Starting training!
[2022-09-26 15:24:23,614][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:24:23,615][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:24:23,615][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:24:23,615][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:24:31,283][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:24:31,404][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:24:54,612][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:24:54,612][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:24:54,697][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:24:56,198][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:24:57,729][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:24:57,730][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:24:57,731][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:24:57,733][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:24:57,739][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:24:57,739][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:24:57,739][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:24:57,740][train][INFO] - Logging hyperparameters!
[2022-09-26 15:24:57,743][train][INFO] - Starting training!
[2022-09-26 15:24:57,744][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:24:57,745][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:24:57,745][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:24:57,745][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:25:05,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:25:05,632][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:29:43,949][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:29:43,950][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:29:44,033][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:29:46,135][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:29:47,815][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:29:47,817][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:29:47,817][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:29:47,819][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:29:47,825][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:29:47,825][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:29:47,825][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:29:47,826][train][INFO] - Logging hyperparameters!
[2022-09-26 15:29:47,828][train][INFO] - Starting training!
[2022-09-26 15:29:47,829][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:29:47,830][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:29:47,830][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:29:47,830][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:29:55,520][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:29:55,638][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:55:32,022][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:55:32,022][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:55:32,106][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:55:37,498][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:55:39,095][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:55:39,097][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:55:39,097][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:55:39,099][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:55:39,105][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:55:39,105][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:55:39,105][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:55:39,106][train][INFO] - Logging hyperparameters!
[2022-09-26 15:55:39,108][train][INFO] - Starting training!
[2022-09-26 15:55:39,109][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:55:39,110][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:55:39,110][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:55:39,110][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:55:47,663][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:55:47,786][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:56:37,072][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:56:37,072][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:56:37,156][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:56:38,928][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:56:40,504][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:56:40,505][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:56:40,506][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:56:40,508][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:56:40,513][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:56:40,514][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:56:40,514][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:56:40,514][train][INFO] - Logging hyperparameters!
[2022-09-26 15:56:40,517][train][INFO] - Starting training!
[2022-09-26 15:56:40,518][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:56:40,519][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:56:40,519][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:56:40,519][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:56:48,318][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:56:48,440][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 15:57:10,210][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 15:57:10,211][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 15:57:10,295][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 15:57:11,784][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 15:57:13,281][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 15:57:13,282][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 15:57:13,283][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 15:57:13,285][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 15:57:13,290][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 15:57:13,291][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 15:57:13,291][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 15:57:13,291][train][INFO] - Logging hyperparameters!
[2022-09-26 15:57:13,295][train][INFO] - Starting training!
[2022-09-26 15:57:13,296][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 15:57:13,297][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 15:57:13,297][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 15:57:13,297][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 15:57:21,077][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 15:57:21,201][pytorch_lightning.core.lightning][INFO] - 
  | Name           | Type                  | Params
---------------------------------------------------------
0 | backbone       | Joiner                | 23.5 M
1 | transformer    | DeformableTransformer | 14.4 M
2 | matcher        | HungarianMatcher      | 0     
3 | input_proj     | ModuleList            | 986 K 
4 | upsample_layer | Sequential            | 442 K 
5 | spatial_conv   | Sequential            | 2.3 K 
6 | temporal_conv  | Sequential            | 524 K 
7 | class_embed    | ModuleList            | 70.2 K
8 | bbox_embed     | ModuleList            | 397 K 
9 | query_embed    | Embedding             | 307 K 
---------------------------------------------------------
39.9 M    Trainable params
222 K     Non-trainable params
40.1 M    Total params
160.584   Total estimated model params size (MB)
[2022-09-26 16:24:54,481][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:24:54,481][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:24:54,567][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:24:56,063][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:25:15,135][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:25:15,136][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:25:15,220][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:25:16,694][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:25:18,202][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:25:18,204][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:25:18,205][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:25:18,206][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:25:18,212][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:25:18,212][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:25:18,213][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:25:18,213][train][INFO] - Logging hyperparameters!
[2022-09-26 16:25:18,216][train][INFO] - Starting training!
[2022-09-26 16:25:18,217][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:25:18,218][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:25:18,218][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:25:18,218][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:25:25,909][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:25:26,031][pytorch_lightning.core.lightning][INFO] - 
   | Name           | Type                  | Params
----------------------------------------------------------
0  | backbone       | Joiner                | 23.5 M
1  | transformer    | DeformableTransformer | 14.4 M
2  | matcher        | HungarianMatcher      | 0     
3  | input_proj     | ModuleList            | 986 K 
4  | upsample_layer | Sequential            | 442 K 
5  | spatial_conv   | Sequential            | 2.3 K 
6  | temporal_conv  | Sequential            | 524 K 
7  | class_embed    | ModuleList            | 70.2 K
8  | bbox_embed     | ModuleList            | 397 K 
9  | query_embed    | Embedding             | 307 K 
10 | roi_projector  | Sequential            | 819 K 
----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:27:49,139][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:27:49,139][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:27:49,223][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:27:50,706][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:27:52,222][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:27:52,224][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:27:52,225][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:27:52,226][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:27:52,232][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:27:52,232][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:27:52,233][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:27:52,233][train][INFO] - Logging hyperparameters!
[2022-09-26 16:27:52,236][train][INFO] - Starting training!
[2022-09-26 16:27:52,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:27:52,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:27:52,238][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:27:52,238][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:27:59,946][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:28:00,070][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:28:46,204][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:28:46,205][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:28:46,289][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:28:47,749][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:28:49,248][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:28:49,249][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:28:49,250][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:28:49,252][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:28:49,258][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:28:49,258][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:28:49,258][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:28:49,259][train][INFO] - Logging hyperparameters!
[2022-09-26 16:28:49,261][train][INFO] - Starting training!
[2022-09-26 16:28:49,262][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:28:49,263][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:28:49,263][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:28:49,263][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:28:56,898][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:28:57,019][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:29:14,043][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:29:14,044][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:29:14,127][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:29:15,617][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:29:17,123][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:29:17,125][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:29:17,125][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:29:17,146][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:29:17,152][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:29:17,152][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:29:17,153][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:29:17,153][train][INFO] - Logging hyperparameters!
[2022-09-26 16:29:17,155][train][INFO] - Starting training!
[2022-09-26 16:29:17,156][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:29:17,157][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:29:17,157][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:29:17,158][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:29:24,786][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:29:24,909][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:30:06,619][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:30:06,619][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:30:06,704][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:30:08,208][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:30:09,742][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:30:09,743][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:30:09,744][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:30:09,746][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:30:09,752][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:30:09,752][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:30:09,753][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:30:09,753][train][INFO] - Logging hyperparameters!
[2022-09-26 16:30:09,755][train][INFO] - Starting training!
[2022-09-26 16:30:09,756][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:30:09,757][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:30:09,758][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:30:09,758][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:30:17,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:30:17,634][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 16:30:58,162][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:30:58,162][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:30:58,247][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:30:59,714][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:31:01,217][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:31:01,218][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:31:01,219][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:31:01,221][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:31:01,226][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:31:01,226][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:31:01,227][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:31:01,227][train][INFO] - Logging hyperparameters!
[2022-09-26 16:31:01,230][train][INFO] - Starting training!
[2022-09-26 16:31:01,231][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:31:01,232][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:31:01,232][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:31:01,232][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:31:08,966][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:31:09,091][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 16:31:59,463][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:31:59,464][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:31:59,547][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:32:01,002][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:32:02,493][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:32:02,495][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:32:02,496][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:32:02,497][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:32:02,503][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:32:02,503][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:32:02,504][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:32:02,504][train][INFO] - Logging hyperparameters!
[2022-09-26 16:32:02,507][train][INFO] - Starting training!
[2022-09-26 16:32:02,507][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:32:02,508][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:32:02,509][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:32:02,509][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:32:10,138][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:32:10,260][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:32:47,094][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:32:47,095][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:32:47,179][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:32:48,647][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:32:50,163][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:32:50,164][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:32:50,165][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:32:50,167][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:32:50,172][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:32:50,173][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:32:50,173][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:32:50,173][train][INFO] - Logging hyperparameters!
[2022-09-26 16:32:50,176][train][INFO] - Starting training!
[2022-09-26 16:32:50,177][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:32:50,178][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:32:50,178][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:32:50,178][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:32:57,985][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:32:58,109][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:33:41,069][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:33:41,069][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:33:41,154][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:33:42,620][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:33:44,130][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:33:44,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:33:44,132][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:33:44,134][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:33:44,140][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:33:44,140][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:33:44,141][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:33:44,141][train][INFO] - Logging hyperparameters!
[2022-09-26 16:33:44,144][train][INFO] - Starting training!
[2022-09-26 16:33:44,145][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:33:44,146][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:33:44,146][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:33:44,146][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:33:51,818][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:33:51,939][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:35:03,482][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:35:03,483][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:35:03,566][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:35:05,053][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:35:06,568][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:35:06,569][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:35:06,570][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:35:06,572][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:35:06,577][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:35:06,578][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:35:06,578][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:35:06,578][train][INFO] - Logging hyperparameters!
[2022-09-26 16:35:06,581][train][INFO] - Starting training!
[2022-09-26 16:35:06,582][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:35:06,583][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:35:06,583][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:35:06,583][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:35:14,382][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:35:14,508][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 803 K 
-----------------------------------------------------------
40.7 M    Trainable params
222 K     Non-trainable params
41.0 M    Total params
163.863   Total estimated model params size (MB)
[2022-09-26 16:35:44,225][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 16:35:44,225][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 16:35:44,309][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 16:35:45,769][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 16:35:47,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 16:35:47,260][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 16:35:47,261][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 16:35:47,263][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 16:35:47,268][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 16:35:47,269][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 16:35:47,269][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 16:35:47,269][train][INFO] - Logging hyperparameters!
[2022-09-26 16:35:47,272][train][INFO] - Starting training!
[2022-09-26 16:35:47,273][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 16:35:47,274][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 16:35:47,274][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 16:35:47,274][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 16:35:54,970][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 16:35:55,094][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:25:07,584][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:25:07,600][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:25:07,685][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:25:11,152][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:25:13,014][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:25:13,016][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:25:13,017][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:25:13,018][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:25:13,024][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:25:13,024][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:25:13,025][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:25:13,025][train][INFO] - Logging hyperparameters!
[2022-09-26 17:25:13,028][train][INFO] - Starting training!
[2022-09-26 17:25:13,029][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:25:13,030][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:25:13,030][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:25:13,030][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:25:20,797][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:25:20,924][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:32:58,030][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:32:58,031][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:32:58,115][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:33:00,999][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:33:24,015][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:33:24,016][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:33:24,099][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:33:25,872][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:33:49,713][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:33:49,714][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:33:49,797][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:33:51,505][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:33:53,132][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:33:53,133][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:33:53,134][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:33:53,136][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:33:53,142][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:33:53,142][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:33:53,143][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:33:53,143][train][INFO] - Logging hyperparameters!
[2022-09-26 17:33:53,145][train][INFO] - Starting training!
[2022-09-26 17:33:53,146][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:33:53,148][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:33:53,148][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:33:53,148][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:34:01,054][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:34:01,188][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:34:26,602][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:34:26,602][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:34:26,687][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:34:28,373][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:34:29,935][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:34:29,936][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:34:29,937][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:34:29,939][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:34:29,964][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:34:29,964][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:34:29,965][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:34:29,965][train][INFO] - Logging hyperparameters!
[2022-09-26 17:34:29,968][train][INFO] - Starting training!
[2022-09-26 17:34:29,969][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:34:29,970][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:34:29,970][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:34:29,970][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:34:37,799][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:34:37,932][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:36:15,792][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:36:15,792][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:36:15,880][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:36:17,386][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:36:18,898][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:36:18,899][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:36:18,900][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:36:18,902][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:36:18,907][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:36:18,908][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:36:18,908][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:36:18,908][train][INFO] - Logging hyperparameters!
[2022-09-26 17:36:18,911][train][INFO] - Starting training!
[2022-09-26 17:36:18,912][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:36:18,913][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:36:18,913][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:36:18,936][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:36:26,777][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:36:26,904][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:38:11,266][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:38:11,267][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:38:11,351][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:38:12,830][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:38:14,364][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:38:14,366][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:38:14,367][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:38:14,368][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:38:14,374][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:38:14,374][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:38:14,375][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:38:14,375][train][INFO] - Logging hyperparameters!
[2022-09-26 17:38:14,378][train][INFO] - Starting training!
[2022-09-26 17:38:14,379][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:38:14,379][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:38:14,380][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:38:14,380][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:38:22,181][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:38:22,309][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:40:08,094][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:40:08,094][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:40:08,179][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:40:09,703][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:40:11,387][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:40:11,388][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:40:11,389][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:40:11,391][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:40:11,396][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:40:11,397][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:40:11,397][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:40:11,397][train][INFO] - Logging hyperparameters!
[2022-09-26 17:40:11,400][train][INFO] - Starting training!
[2022-09-26 17:40:11,401][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:40:11,402][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:40:11,402][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:40:11,402][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:40:19,215][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:40:19,340][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:48:00,923][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:48:00,923][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:48:01,010][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:48:02,562][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:48:04,059][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:48:04,061][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:48:04,062][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:48:04,063][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:48:04,069][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:48:04,069][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:48:04,070][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:48:04,070][train][INFO] - Logging hyperparameters!
[2022-09-26 17:48:04,073][train][INFO] - Starting training!
[2022-09-26 17:48:04,074][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:48:04,074][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:48:04,075][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:48:04,075][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:48:11,911][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:48:12,035][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-26 17:48:30,562][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-26 17:48:30,563][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-26 17:48:30,646][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-26 17:48:32,123][train][INFO] - Instantiating model <models.model.Model>
[2022-09-26 17:48:33,627][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-26 17:48:33,628][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-26 17:48:33,629][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-26 17:48:33,631][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-26 17:48:33,636][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-26 17:48:33,636][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-26 17:48:33,637][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-26 17:48:33,638][train][INFO] - Logging hyperparameters!
[2022-09-26 17:48:33,640][train][INFO] - Starting training!
[2022-09-26 17:48:33,641][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-26 17:48:33,642][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-26 17:48:33,642][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-26 17:48:33,642][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-26 17:48:41,374][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-26 17:48:41,500][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 10:48:53,751][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 10:48:53,771][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 10:48:53,910][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 10:49:05,438][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 10:49:07,909][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 10:49:07,910][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 10:49:07,911][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 10:49:07,913][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 10:49:07,919][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 10:49:07,919][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 10:49:07,920][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 10:49:07,920][train][INFO] - Logging hyperparameters!
[2022-09-27 10:49:07,923][train][INFO] - Starting training!
[2022-09-27 10:49:07,924][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 10:49:07,925][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 10:49:07,926][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 10:49:07,926][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 10:49:17,101][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 10:49:17,414][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 14:48:41,429][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 14:48:41,440][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 14:48:41,536][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 14:48:56,570][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 15:29:39,622][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 15:29:39,622][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 15:29:39,706][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 15:29:43,224][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 15:30:23,821][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 15:30:23,821][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 15:30:23,904][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 15:30:25,742][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 15:31:11,885][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 15:31:11,885][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 15:31:11,969][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 15:31:13,926][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 15:31:50,666][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 15:31:50,666][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 15:31:50,752][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 15:31:52,314][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 15:31:54,662][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 15:31:54,663][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 15:31:54,664][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 15:31:54,666][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 15:31:54,671][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 15:31:54,672][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 15:31:54,672][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 15:31:54,672][train][INFO] - Logging hyperparameters!
[2022-09-27 15:31:54,675][train][INFO] - Starting training!
[2022-09-27 15:31:54,676][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 15:31:54,677][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 15:31:54,678][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 15:31:54,678][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 15:32:03,141][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 15:32:03,276][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 16:50:05,888][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 16:50:05,905][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 16:50:06,008][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 16:50:09,662][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 16:50:11,548][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 16:50:11,550][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 16:50:11,551][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 16:50:11,552][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 16:50:11,558][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 16:50:11,558][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 16:50:11,559][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 16:50:11,559][train][INFO] - Logging hyperparameters!
[2022-09-27 16:50:11,562][train][INFO] - Starting training!
[2022-09-27 16:50:11,562][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 16:50:11,563][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 16:50:11,564][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 16:50:11,564][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 16:50:19,356][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 16:50:19,479][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:07:44,373][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:07:44,391][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:07:44,476][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:07:47,521][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:08:28,014][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:08:28,014][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:08:28,099][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:08:29,678][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:09:08,285][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:09:08,286][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:09:08,373][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:09:09,898][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:09:21,116][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:09:21,116][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:09:21,201][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:09:22,661][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:09:49,473][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:09:49,473][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:09:49,558][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:09:51,209][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:13:28,017][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:13:28,018][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:13:28,104][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:13:29,917][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:13:31,742][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:13:31,743][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:13:31,744][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:13:31,746][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:13:31,751][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:13:31,752][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:13:31,752][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:13:31,752][train][INFO] - Logging hyperparameters!
[2022-09-27 17:13:31,755][train][INFO] - Starting training!
[2022-09-27 17:13:31,756][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:13:31,757][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:13:31,780][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:13:31,780][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:13:39,502][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:13:39,634][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:36:22,864][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:36:22,881][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:36:22,967][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:36:26,711][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:36:28,602][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:36:28,603][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:36:28,604][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:36:28,606][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:36:28,611][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:36:28,611][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:36:28,612][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:36:28,612][train][INFO] - Logging hyperparameters!
[2022-09-27 17:36:28,615][train][INFO] - Starting training!
[2022-09-27 17:36:28,616][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:36:28,617][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:36:28,617][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:36:28,617][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:36:36,407][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:36:36,529][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:37:07,781][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:37:07,781][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:37:07,866][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:37:10,996][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:37:12,548][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:37:12,549][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:37:12,550][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:37:12,552][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:37:12,557][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:37:12,558][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:37:12,558][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:37:12,558][train][INFO] - Logging hyperparameters!
[2022-09-27 17:37:12,561][train][INFO] - Starting training!
[2022-09-27 17:37:12,562][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:37:12,563][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:37:12,563][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:37:12,564][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:37:20,330][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:37:20,453][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:40:29,949][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:40:29,963][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:40:30,048][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:40:33,627][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:41:29,000][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:41:29,001][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:41:29,087][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:41:32,529][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:41:34,259][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:41:34,260][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:41:34,261][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:41:34,263][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:41:34,269][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:41:34,269][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:41:34,269][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:41:34,270][train][INFO] - Logging hyperparameters!
[2022-09-27 17:41:34,272][train][INFO] - Starting training!
[2022-09-27 17:41:34,273][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:41:34,274][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:41:34,275][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:41:34,275][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:41:42,110][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:41:42,236][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:41:58,211][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:41:58,211][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:41:58,298][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:42:01,663][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:42:03,207][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:42:03,209][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:42:03,209][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:42:03,211][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:42:03,217][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:42:03,217][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:42:03,218][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:42:03,218][train][INFO] - Logging hyperparameters!
[2022-09-27 17:42:03,221][train][INFO] - Starting training!
[2022-09-27 17:42:03,222][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:42:03,223][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:42:03,223][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:42:03,223][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:42:10,935][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:42:11,059][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:42:54,939][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:42:54,940][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:42:55,025][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:42:58,449][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:42:59,967][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:42:59,968][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:42:59,969][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:42:59,971][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:42:59,976][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:42:59,977][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:42:59,977][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:42:59,977][train][INFO] - Logging hyperparameters!
[2022-09-27 17:42:59,980][train][INFO] - Starting training!
[2022-09-27 17:42:59,981][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:42:59,982][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:42:59,983][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:42:59,983][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:43:07,731][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:43:07,854][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:46:58,053][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:46:58,054][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:46:58,139][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:47:01,449][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:47:16,415][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:47:16,416][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:47:16,504][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:47:18,590][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:47:20,451][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:47:20,452][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:47:20,453][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:47:20,455][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:47:20,460][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:47:20,461][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:47:20,461][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:47:20,462][train][INFO] - Logging hyperparameters!
[2022-09-27 17:47:20,464][train][INFO] - Starting training!
[2022-09-27 17:47:20,465][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:47:20,466][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:47:20,466][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:47:20,466][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:47:28,283][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:47:28,405][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:48:00,553][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:48:00,554][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:48:00,639][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:48:03,644][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:48:05,223][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:48:05,224][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:48:05,225][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:48:05,227][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:48:05,234][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:48:05,234][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:48:05,235][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:48:05,235][train][INFO] - Logging hyperparameters!
[2022-09-27 17:48:05,238][train][INFO] - Starting training!
[2022-09-27 17:48:05,239][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:48:05,240][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:48:05,240][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:48:05,240][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:48:13,013][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:48:13,137][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 17:51:45,481][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 17:51:45,498][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 17:51:45,584][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 17:51:48,741][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 17:51:50,340][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 17:51:50,341][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 17:51:50,342][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 17:51:50,344][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 17:51:50,349][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 17:51:50,350][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 17:51:50,350][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 17:51:50,350][train][INFO] - Logging hyperparameters!
[2022-09-27 17:51:50,353][train][INFO] - Starting training!
[2022-09-27 17:51:50,354][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 17:51:50,355][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 17:51:50,355][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 17:51:50,355][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 17:51:58,024][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 17:51:58,147][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
-----------------------------------------------------------
41.5 M    Trainable params
222 K     Non-trainable params
41.8 M    Total params
167.075   Total estimated model params size (MB)
[2022-09-27 18:02:27,256][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:02:27,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:02:27,345][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:02:28,958][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:02:52,882][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:02:52,882][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:02:52,971][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:02:54,515][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:03:31,902][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:03:31,902][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:03:31,992][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:03:33,492][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:04:56,206][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:04:56,206][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:04:56,295][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:04:57,842][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:05:29,262][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:05:29,263][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:05:29,352][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:05:30,905][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:05:32,442][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:05:32,443][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:05:32,444][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:05:32,446][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:05:32,452][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:05:32,452][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:05:32,452][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:05:32,453][train][INFO] - Logging hyperparameters!
[2022-09-27 18:05:32,455][train][INFO] - Starting training!
[2022-09-27 18:05:32,456][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:05:32,457][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:05:32,457][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:05:32,458][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:05:40,022][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:05:40,150][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:06:14,104][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:06:14,105][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:06:14,194][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:06:15,736][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:06:17,272][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:06:17,273][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:06:17,274][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:06:17,276][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:06:17,282][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:06:17,282][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:06:17,282][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:06:17,283][train][INFO] - Logging hyperparameters!
[2022-09-27 18:06:17,285][train][INFO] - Starting training!
[2022-09-27 18:06:17,286][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:06:17,287][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:06:17,288][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:06:17,288][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:06:24,841][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:06:24,974][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:07:04,106][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:07:04,107][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:07:04,196][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:07:05,760][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:07:07,294][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:07:07,295][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:07:07,296][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:07:07,298][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:07:07,303][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:07:07,332][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:07:07,333][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:07:07,333][train][INFO] - Logging hyperparameters!
[2022-09-27 18:07:07,336][train][INFO] - Starting training!
[2022-09-27 18:07:07,337][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:07:07,338][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:07:07,338][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:07:07,338][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:07:14,887][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:07:15,015][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:07:51,063][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:07:51,063][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:07:51,154][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:07:52,718][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:07:54,222][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:07:54,224][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:07:54,224][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:07:54,226][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:07:54,232][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:07:54,232][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:07:54,233][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:07:54,233][train][INFO] - Logging hyperparameters!
[2022-09-27 18:07:54,236][train][INFO] - Starting training!
[2022-09-27 18:07:54,237][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:07:54,238][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:07:54,238][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:07:54,238][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:08:01,820][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:08:01,947][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:11:52,658][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:11:52,658][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:11:52,748][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:11:54,307][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:11:55,833][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:11:55,835][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:11:55,836][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:11:55,837][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:11:55,843][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:11:55,843][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:11:55,844][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:11:55,844][train][INFO] - Logging hyperparameters!
[2022-09-27 18:11:55,847][train][INFO] - Starting training!
[2022-09-27 18:11:55,848][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:11:55,849][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:11:55,849][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:11:55,849][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:12:03,388][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:12:03,513][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:12:36,383][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:12:36,383][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:12:36,473][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:12:38,012][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:12:39,541][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:12:39,542][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:12:39,543][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:12:39,545][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:12:39,550][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:12:39,551][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:12:39,551][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:12:39,551][train][INFO] - Logging hyperparameters!
[2022-09-27 18:12:39,554][train][INFO] - Starting training!
[2022-09-27 18:12:39,555][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:12:39,556][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:12:39,556][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:12:39,556][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:12:47,049][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:12:47,179][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:13:27,322][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:13:27,323][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:13:27,412][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:13:28,953][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:13:30,479][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:13:30,480][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:13:30,481][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:13:30,483][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:13:30,488][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:13:30,489][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:13:30,489][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:13:30,490][train][INFO] - Logging hyperparameters!
[2022-09-27 18:13:30,492][train][INFO] - Starting training!
[2022-09-27 18:13:30,493][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:13:30,494][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:13:30,494][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:13:30,495][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:13:37,952][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:13:38,077][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 18:15:17,905][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 18:15:17,906][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 18:15:17,995][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 18:15:19,538][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 18:15:21,055][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 18:15:21,057][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 18:15:21,058][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 18:15:21,059][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 18:15:21,065][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 18:15:21,065][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 18:15:21,066][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 18:15:21,068][train][INFO] - Logging hyperparameters!
[2022-09-27 18:15:21,071][train][INFO] - Starting training!
[2022-09-27 18:15:21,072][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 18:15:21,073][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 18:15:21,073][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 18:15:21,073][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 18:15:28,595][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 18:15:28,720][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 21:34:39,298][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 21:34:39,314][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 21:34:39,496][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 21:34:59,577][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 21:35:02,035][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 21:35:02,036][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 21:35:02,037][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 21:35:02,039][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 21:35:02,045][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 21:35:02,045][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 21:35:02,046][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 21:35:02,046][train][INFO] - Logging hyperparameters!
[2022-09-27 21:35:02,049][train][INFO] - Starting training!
[2022-09-27 21:35:02,049][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 21:35:02,051][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 21:35:02,051][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 21:35:02,051][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 21:35:10,728][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 21:35:10,859][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 22:00:24,021][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 22:00:24,042][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 22:00:24,132][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 22:00:31,487][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 22:00:33,176][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 22:00:33,178][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 22:00:33,179][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 22:00:33,180][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 22:00:33,186][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 22:00:33,186][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 22:00:33,187][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 22:00:33,187][train][INFO] - Logging hyperparameters!
[2022-09-27 22:00:33,190][train][INFO] - Starting training!
[2022-09-27 22:00:33,191][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 22:00:33,192][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 22:00:33,192][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 22:00:33,192][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 22:00:40,858][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 22:00:40,983][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 22:55:24,398][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 22:55:24,398][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 22:55:24,487][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 22:55:31,960][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 22:55:33,765][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 22:55:33,766][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 22:55:33,767][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 22:55:33,769][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 22:55:33,775][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 22:55:33,775][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 22:55:33,775][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 22:55:33,776][train][INFO] - Logging hyperparameters!
[2022-09-27 22:55:33,778][train][INFO] - Starting training!
[2022-09-27 22:55:33,779][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 22:55:33,780][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 22:55:33,780][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 22:55:33,781][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 22:55:41,356][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 22:55:41,482][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 22:57:37,407][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 22:57:37,407][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 22:57:37,497][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 22:57:42,628][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 22:57:44,181][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 22:57:44,183][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 22:57:44,183][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 22:57:44,185][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 22:57:44,191][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 22:57:44,191][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 22:57:44,192][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 22:57:44,192][train][INFO] - Logging hyperparameters!
[2022-09-27 22:57:44,195][train][INFO] - Starting training!
[2022-09-27 22:57:44,195][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 22:57:44,196][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 22:57:44,197][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 22:57:44,197][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 22:57:51,805][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 22:57:51,932][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 22:58:21,905][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 22:58:21,905][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 22:58:21,994][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 22:58:23,568][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 22:58:25,166][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 22:58:25,167][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 22:58:25,168][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 22:58:25,170][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 22:58:25,176][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 22:58:25,176][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 22:58:25,176][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 22:58:25,177][train][INFO] - Logging hyperparameters!
[2022-09-27 22:58:25,258][train][INFO] - Starting training!
[2022-09-27 22:58:25,259][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 22:58:25,260][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 22:58:25,261][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 22:58:25,261][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 22:58:32,894][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 22:58:33,026][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:07:26,956][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:07:26,956][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:07:27,046][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:07:28,620][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:07:30,188][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:07:30,189][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:07:30,190][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:07:30,192][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:07:30,197][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:07:30,198][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:07:30,198][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:07:30,199][train][INFO] - Logging hyperparameters!
[2022-09-27 23:07:30,201][train][INFO] - Starting training!
[2022-09-27 23:07:30,202][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:07:30,203][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:07:30,203][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:07:30,204][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:07:37,758][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:07:37,887][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:08:08,599][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:08:08,599][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:08:08,689][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:08:10,257][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:08:11,820][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:08:11,822][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:08:11,823][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:08:11,824][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:08:11,830][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:08:11,830][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:08:11,831][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:08:11,831][train][INFO] - Logging hyperparameters!
[2022-09-27 23:08:11,834][train][INFO] - Starting training!
[2022-09-27 23:08:11,835][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:08:11,836][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:08:11,836][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:08:11,836][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:08:19,362][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:08:19,488][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:08:42,737][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:08:42,737][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:08:42,828][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:08:44,388][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:08:45,963][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:08:45,965][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:08:45,966][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:08:45,967][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:08:45,973][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:08:45,973][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:08:45,974][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:08:45,974][train][INFO] - Logging hyperparameters!
[2022-09-27 23:08:45,977][train][INFO] - Starting training!
[2022-09-27 23:08:45,978][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:08:45,979][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:08:45,979][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:08:45,979][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:08:53,541][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:08:53,669][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:09:52,627][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:09:52,627][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:09:52,716][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:09:54,279][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:09:55,872][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:09:55,873][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:09:55,874][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:09:55,876][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:09:55,881][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:09:55,882][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:09:55,882][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:09:55,882][train][INFO] - Logging hyperparameters!
[2022-09-27 23:09:55,885][train][INFO] - Starting training!
[2022-09-27 23:09:55,886][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:09:55,887][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:09:55,887][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:09:55,888][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:10:03,430][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:10:03,555][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:10:44,793][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:10:44,793][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:10:44,882][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:10:46,459][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:10:48,015][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:10:48,016][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:10:48,017][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:10:48,019][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:10:48,025][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:10:48,025][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:10:48,025][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:10:48,026][train][INFO] - Logging hyperparameters!
[2022-09-27 23:10:48,028][train][INFO] - Starting training!
[2022-09-27 23:10:48,029][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:10:48,030][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:10:48,030][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:10:48,031][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:10:55,570][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:10:55,698][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:12:33,048][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:12:33,048][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:12:33,138][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:12:34,714][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:12:36,300][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:12:36,301][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:12:36,302][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:12:36,304][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:12:36,310][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:12:36,310][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:12:36,310][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:12:36,311][train][INFO] - Logging hyperparameters!
[2022-09-27 23:12:36,313][train][INFO] - Starting training!
[2022-09-27 23:12:36,314][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:12:36,315][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:12:36,316][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:12:36,316][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:12:43,887][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:12:44,016][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:15:09,156][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:15:09,157][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:15:09,246][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:15:10,808][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:15:12,342][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:15:12,343][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:15:12,344][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:15:12,345][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:15:12,351][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:15:12,351][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:15:12,352][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:15:12,352][train][INFO] - Logging hyperparameters!
[2022-09-27 23:15:12,355][train][INFO] - Starting training!
[2022-09-27 23:15:12,356][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:15:12,357][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:15:12,357][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:15:12,357][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:15:19,879][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:15:20,006][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | input_proj      | ModuleList            | 986 K 
4  | upsample_layer  | Sequential            | 442 K 
5  | spatial_conv    | Sequential            | 2.3 K 
6  | temporal_conv   | Sequential            | 524 K 
7  | class_embed     | ModuleList            | 70.2 K
8  | bbox_embed      | ModuleList            | 397 K 
9  | query_embed     | Embedding             | 307 K 
10 | roi_projector   | Sequential            | 16.4 K
11 | roi_projector_2 | Sequential            | 1.6 M 
12 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:33:56,862][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:33:56,862][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:33:56,955][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:34:02,049][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:34:19,942][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:34:19,942][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:34:20,035][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:34:21,510][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:36:53,369][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:36:53,369][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:36:53,467][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:36:55,040][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:37:32,244][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:37:32,245][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:39:44,075][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:39:44,075][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:39:44,166][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:39:45,766][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:40:06,951][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:40:06,951][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:40:07,044][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:40:08,605][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:40:10,189][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:40:10,191][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:40:10,192][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:40:10,193][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:40:10,199][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:40:10,199][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:40:10,200][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:40:10,200][train][INFO] - Logging hyperparameters!
[2022-09-27 23:40:10,203][train][INFO] - Starting training!
[2022-09-27 23:40:10,204][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:40:10,205][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:40:10,206][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:40:10,206][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:40:17,805][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:40:17,931][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:40:43,921][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:40:43,922][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:40:44,012][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:40:45,575][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:40:47,100][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:40:47,102][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:40:47,102][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:40:47,104][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:40:47,110][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:40:47,110][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:40:47,111][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:40:47,111][train][INFO] - Logging hyperparameters!
[2022-09-27 23:40:47,114][train][INFO] - Starting training!
[2022-09-27 23:40:47,114][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:40:47,115][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:40:47,116][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:40:47,116][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:40:54,646][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:40:54,775][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:45:12,756][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:45:12,756][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:45:12,847][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:45:14,414][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:45:15,973][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:45:15,975][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:45:15,976][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:45:15,977][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:45:15,983][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:45:15,983][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:45:15,984][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:45:15,984][train][INFO] - Logging hyperparameters!
[2022-09-27 23:45:15,987][train][INFO] - Starting training!
[2022-09-27 23:45:15,988][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:45:15,989][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:45:15,989][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:45:15,989][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:45:23,648][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:45:23,776][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:45:39,775][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:45:39,775][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:45:39,865][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:45:41,430][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:45:42,985][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:45:42,986][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:45:42,987][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:45:42,989][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:45:42,995][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:45:42,995][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:45:42,995][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:45:42,996][train][INFO] - Logging hyperparameters!
[2022-09-27 23:45:42,998][train][INFO] - Starting training!
[2022-09-27 23:45:42,999][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:45:43,000][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:45:43,001][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:45:43,001][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:45:50,547][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:45:50,676][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:57:16,302][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:57:16,302][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:57:16,391][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:57:18,872][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:57:20,415][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:57:20,417][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:57:20,418][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:57:20,419][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:57:20,425][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:57:20,425][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:57:20,426][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:57:20,426][train][INFO] - Logging hyperparameters!
[2022-09-27 23:57:20,429][train][INFO] - Starting training!
[2022-09-27 23:57:20,430][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:57:20,431][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:57:20,431][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:57:20,431][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:57:27,980][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:57:28,109][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:58:32,882][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:58:32,882][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:58:32,972][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:58:34,539][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:58:36,093][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:58:36,094][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:58:36,095][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:58:36,097][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:58:36,102][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:58:36,102][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:58:36,103][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:58:36,103][train][INFO] - Logging hyperparameters!
[2022-09-27 23:58:36,106][train][INFO] - Starting training!
[2022-09-27 23:58:36,107][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:58:36,108][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:58:36,108][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:58:36,108][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:58:43,688][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:58:43,815][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-27 23:59:19,233][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-27 23:59:19,234][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-27 23:59:19,323][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-27 23:59:20,893][train][INFO] - Instantiating model <models.model.Model>
[2022-09-27 23:59:22,452][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-27 23:59:22,454][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-27 23:59:22,455][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-27 23:59:22,456][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-27 23:59:22,462][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-27 23:59:22,462][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-27 23:59:22,463][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-27 23:59:22,463][train][INFO] - Logging hyperparameters!
[2022-09-27 23:59:22,466][train][INFO] - Starting training!
[2022-09-27 23:59:22,467][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-27 23:59:22,468][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-27 23:59:22,468][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-27 23:59:22,468][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-27 23:59:30,040][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-27 23:59:30,168][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-28 00:35:18,257][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-28 00:35:18,257][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-28 00:35:18,347][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-28 00:35:20,491][train][INFO] - Instantiating model <models.model.Model>
[2022-09-28 00:35:22,043][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-28 00:35:22,044][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-28 00:35:22,045][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-28 00:35:22,047][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-28 00:35:22,053][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-28 00:35:22,053][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-28 00:35:22,053][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-28 00:35:22,054][train][INFO] - Logging hyperparameters!
[2022-09-28 00:35:22,056][train][INFO] - Starting training!
[2022-09-28 00:35:22,057][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-28 00:35:22,058][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-28 00:35:22,058][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-28 00:35:22,059][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-28 00:35:29,664][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-28 00:35:29,796][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
[2022-09-28 00:36:00,116][util][INFO] - Disabling python warnings! <config.ignore_warnings=True>
[2022-09-28 00:36:00,117][util][INFO] - Printing config tree with Rich! <config.print_config=True>
[2022-09-28 00:36:00,206][train][INFO] - Instantiating datamodule <dataloader.MOT2020Module>
[2022-09-28 00:36:01,783][train][INFO] - Instantiating model <models.model.Model>
[2022-09-28 00:36:03,379][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2022-09-28 00:36:03,381][train][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2022-09-28 00:36:03,382][train][INFO] - Instantiating logger <pytorch_lightning.loggers.tensorboard.TensorBoardLogger>
[2022-09-28 00:36:03,383][train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2022-09-28 00:36:03,389][pytorch_lightning.utilities.distributed][INFO] - GPU available: True, used: True
[2022-09-28 00:36:03,389][pytorch_lightning.utilities.distributed][INFO] - TPU available: False, using: 0 TPU cores
[2022-09-28 00:36:03,390][pytorch_lightning.utilities.distributed][INFO] - Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).
[2022-09-28 00:36:03,390][train][INFO] - Logging hyperparameters!
[2022-09-28 00:36:03,393][train][INFO] - Starting training!
[2022-09-28 00:36:03,394][pytorch_lightning.plugins.training_type.ddp][INFO] - initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1
[2022-09-28 00:36:03,395][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0
[2022-09-28 00:36:03,395][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for 1 nodes.
[2022-09-28 00:36:03,395][pytorch_lightning.utilities.distributed][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 1 processes
----------------------------------------------------------------------------------------------------

[2022-09-28 00:36:10,999][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2022-09-28 00:36:11,137][pytorch_lightning.core.lightning][INFO] - 
   | Name            | Type                  | Params
-----------------------------------------------------------
0  | backbone        | Joiner                | 23.5 M
1  | transformer     | DeformableTransformer | 14.4 M
2  | matcher         | HungarianMatcher      | 0     
3  | criterion_train | ClipMatcher           | 0     
4  | criterion_test  | ClipMatcher           | 0     
5  | input_proj      | ModuleList            | 986 K 
6  | upsample_layer  | Sequential            | 442 K 
7  | spatial_conv    | Sequential            | 2.3 K 
8  | temporal_conv   | Sequential            | 524 K 
9  | class_embed     | ModuleList            | 70.2 K
10 | bbox_embed      | ModuleList            | 397 K 
11 | query_embed     | Embedding             | 307 K 
12 | roi_projector   | Sequential            | 16.4 K
13 | roi_projector_2 | Sequential            | 1.6 M 
14 | memory_bank     | MemoryBank            | 461 K 
-----------------------------------------------------------
42.0 M    Trainable params
222 K     Non-trainable params
42.2 M    Total params
168.922   Total estimated model params size (MB)
